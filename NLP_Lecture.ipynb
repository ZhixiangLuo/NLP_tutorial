{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction to Natural Language Processing (NLP)\n",
    "#### *from classical methods to modern LLMs*\n",
    "\n",
    "**Part I — Lecture (1.5 hours)**\n",
    "\n",
    "---\n",
    "\n",
    "### Speaker: Jeffrey Luo, Ph.D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## About the Lecturer\n",
    "\n",
    "**Ph.D. in Physics**, UNC Chapel Hill  \n",
    "**B.S. in Mathematics and Physics**, Tsinghua University\n",
    "\n",
    "---\n",
    "### Experience\n",
    "- **Workday**: LLM-based agentic systems for chat, search, and automation\n",
    "- **Gradient AI**: Deep learning models for health insurance underwriting\n",
    "- **Wolters Kluwer**: ML models for infectious disease prediction \n",
    "- **T2 Biosystems**: NMR systems for sepsis detection\n",
    "- **Schlumberger**: NMR study of porous materials\n",
    "---\n",
    "### Hobby Project\n",
    "**OrcaEcho.ai**: AI assistant for presentation creation and editing  \n",
    "[Google Slides Add-on](https://workspace.google.com/marketplace/app/orcarina/404235414546) | [Website](https://orcaecho.ai)\n",
    "\n",
    "---\n",
    "[LinkedIn](https://www.linkedin.com/in/zhixiang-jeffrey-luo-70850124/) | [Google Scholar](https://scholar.google.com/citations?user=gzyW_GUAAAAJ&hl=en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part I: Lecture Overview\n",
    "\n",
    "1. **What is NLP and Why It Matters**\n",
    "2. **Rule-Based NLP (1950s–1980s)**\n",
    "3. **Statistical NLP (1990s–2010s)**\n",
    "4. **Word Embeddings and Deep Learning**\n",
    "5. **Transformers and the LLM Revolution**\n",
    "6. **Modern LLM Applications**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. What is NLP and Why It Matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.1 What is NLP?\n",
    "\n",
    "**Natural Language Processing (NLP)** is a field of AI focused on enabling computers to understand, interpret, generate, and interact with human language.\n",
    "\n",
    "### Some common things powered by NLP:\n",
    "\n",
    "- **Translation** (e.g., English → Chinese)\n",
    "- **Chatbots & assistants**\n",
    "- **Speech-to-text / text-to-speech**\n",
    "- **Summarization**\n",
    "- **Sentiment analysis** (detecting positive/negative tone)\n",
    "- **Search and question-answering**\n",
    "\n",
    "NLP blends **linguistics**, **computer science**, and **machine learning** so that machines can handle language in a useful way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.2 Evolution of NLP Approaches\n",
    "\n",
    "### Before Computers: Foundation (Ancient Times - 1950s)\n",
    "\n",
    "**Linguistics**: Analysis of grammar, syntax, semantics, sentence structure, and meaning \n",
    "**Mathematical Logic & Formal Models**: Scholars created systems to describe language in precise, machine-like terms: Formal grammars, symbolic logic etc.\n",
    "\n",
    "---\n",
    "\n",
    "### After Computers: NLP Timeline\n",
    "\n",
    "**1954**: First machine translation experiment (Georgetown-IBM)\n",
    "\n",
    "**1960s–90s**: Rule-based language systems  \n",
    "→ Translation, parsing algorithms, computational linguistics\n",
    "\n",
    "**2000s–2010s**: Statistical and neural models  \n",
    "→ Probabilistic approaches, early neural networks\n",
    "\n",
    "**2018+**: Transformers and modern LLMs  \n",
    "→ BERT, GPT, and the transformer revolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.3 Major NLP Applications\n",
    "\n",
    "| **Application Category**                   | **Examples & Notes**                                      |\n",
    "|--------------------------------------------|------------------------------------------------------------|\n",
    "| **Machine Translation (1950s–)**           | Georgetown-IBM demo; Google Translate, DeepL              |\n",
    "| **Search & Information Retrieval (1960s–)**| SMART system; AltaVista; Google Search                    |\n",
    "| **Spell/Grammar & Text Classification (1980s–)** | WordPerfect, spam filters; Grammarly                |\n",
    "| **Speech Recognition (1990s–)**            | Dragon Dictate; Siri, Alexa, Google Assistant             |\n",
    "| **Named Entity & Information Extraction (1990s–)** | MUC NER systems; news/event extraction          |\n",
    "| **Chatbots & QA Systems (2000s–)**         | TREC QA, IBM Watson; ChatGPT, Claude                      |\n",
    "| **Text Generation & LLMs (2018–)**         | GPT series, Claude; rewriting, summarization              |\n",
    "| **AI Agents & Tool Use (2023–)**           | LLMs executing actions via APIs/tools                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Rule-Based NLP (1950s–1980s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What Is Rule-Based NLP? (1950s–1980s)\n",
    "\n",
    "Early NLP relied on **manually written rules** from linguists\n",
    "\n",
    "- **Symbolic, deterministic systems**\n",
    "- **No machine learning or statistical models**\n",
    "- Worked only in **narrow, controlled domains**\n",
    "\n",
    "**Example rule:** \"If a sentence starts with 'Who', classify it as a question.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Language Basics from School\n",
    "\n",
    "**Sentence Structure:**\n",
    "- Sentences have parts: subject, verb, object\n",
    "- Words group into phrases: noun phrase (NP), verb phrase (VP)\n",
    "\n",
    "**Parts of Speech (POS):**\n",
    "- Noun, verb, adjective, adverb\n",
    "- Prepositions, conjunctions\n",
    "\n",
    "You already identify these intuitively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Language Basics from School\n",
    "\n",
    "**Morphology:**\n",
    "- Plural forms: cat → cats\n",
    "- Verb tenses: walk → walked\n",
    "- Irregulars: go → went, mouse → mice\n",
    "\n",
    "**Common Patterns:**\n",
    "- \"Who…?\" → question\n",
    "- \"I feel X\" → emotional statement\n",
    "- \"If…then…\" → condition\n",
    "\n",
    "These everyday language concepts form the foundation of early NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Rule-Based NLP Extended Linguistic Concepts\n",
    "\n",
    "**1. What NLP Inherited from Linguistics**\n",
    "\n",
    "- Sentence structure → NP, VP, subject, object\n",
    "- Parts of speech → nouns, verbs, adjectives, etc.\n",
    "- Morphology concepts → plurals, tenses, irregulars\n",
    "- Lexicon (linguistic sense) → words + meanings\n",
    "\n",
    "*(These are the same concepts students learn in school grammar.)*\n",
    "\n",
    "**2. What NLP Extended into Machine-Readable Form**\n",
    "\n",
    "- **Formal grammars:** Phrase structure rules rewritten as explicit production rules (e.g., S → NP VP, NP → Det N)\n",
    "- **Computational lexicon:** Structured entries storing POS, forms, syntactic frames, semantic features\n",
    "\n",
    "*(Linguistic ideas made precise and explicit for computer use.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Rule-Based NLP Extended Linguistic Concepts\n",
    "\n",
    "**3. What NLP Created Anew for Computation**\n",
    "\n",
    "- **Morphological rule engines:** Algorithms for generating/analyzing word forms, handling exceptions\n",
    "- **Parsing algorithms:** Automatic construction of parse trees from grammar rules\n",
    "\n",
    "*(These are engineering mechanisms that did not exist in pure linguistics.)*\n",
    "\n",
    "**4. Pattern Rules (New for Early NLP Applications)**\n",
    "\n",
    "- IF text matches pattern → THEN respond or extract\n",
    "- *Example: \"I feel X\" → \"Why do you feel X?\"*\n",
    "- Enabled early chatbots (ELIZA) and information extraction\n",
    "- Not derived from linguistics; purely application-driven\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classic Systems (ELIZA, SHRDLU, MT)\n",
    "\n",
    "**ELIZA (1966) — Shallow rules**\n",
    "\n",
    "- Pattern matching; no understanding\n",
    "- *Example: \"I feel sad.\" → \"Why do you feel sad?\"*\n",
    "\n",
    "**SHRDLU (1970s) — Deep rules in a tiny world**\n",
    "\n",
    "- Real parsing + reasoning, but only in the \"blocks world\"\n",
    "- *Example: \"Put the red block on the green cube.\"*\n",
    "\n",
    "**Rule-Based MT (SYSTRAN) — Large-scale rules**\n",
    "\n",
    "- Thousands of grammar rules; used in real translation systems\n",
    "- Expensive to maintain; brittle outside covered patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Strengths, Weaknesses & Why It Ended\n",
    "\n",
    "**Strengths:**\n",
    "- Interpretable, linguistically grounded\n",
    "- Reliable in narrow domains\n",
    "\n",
    "**Weaknesses:**\n",
    "- Brittle, unscalable\n",
    "- Huge rule sets, fails on new inputs\n",
    "- Could not handle ambiguity or real-world language variation\n",
    "\n",
    "**Shift in 1990s:** Rise of digital text + computing power enabled statistical NLP  \n",
    "Data-driven models outperformed handcrafted rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Statistical NLP (1990s–2010s)\n",
    "\n",
    "From hand-written rules → data-driven models.\n",
    "\n",
    "- Learn patterns from corpora instead of experts\n",
    "- Use probabilities to handle ambiguity\n",
    "- Built the foundation for modern NLP applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What Statistical NLP Built On\n",
    "\n",
    "- Parts of speech → still needed for tagging\n",
    "- Phrase structure → still used for parsing\n",
    "- Lexicon concepts → still needed for word categories\n",
    "- Morphology → still informs word forms\n",
    "- Linguistic features → used as input signals (suffixes, capitalization)\n",
    "\n",
    "*(Kept linguistic structure, but learned patterns instead of hand-writing them.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data-Driven Learning\n",
    "\n",
    "1. **Learn from corpora, not experts**\n",
    "2. **Probabilities replace deterministic rules**\n",
    "3. **NLP becomes an empirical science**\n",
    "\n",
    "*Example:* \"What word comes next?\" → choose the most probable one from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Corpora Fueling Statistical NLP\n",
    "\n",
    "**POS Tagging:** Penn Treebank WSJ (PTB-POS), Brown Corpus\n",
    "\n",
    "**Parsing:** PTB phrase-structure trees, PropBank / FrameNet (predicate–argument)\n",
    "\n",
    "**NER:** CoNLL-2003 (EN/DE), MUC, ACE\n",
    "\n",
    "**Machine Translation:** IBM Canadian Hansard (EN–FR), EuroParl, early WMT corpora\n",
    "\n",
    "**Language Modeling:** PTB LM split, Gigaword, Google N-grams\n",
    "\n",
    "**Text Classification:** Reuters-21578, 20 Newsgroups, TREC QA\n",
    "\n",
    "*(Data availability enabled empirical training and benchmarking.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Penn Treebank Became the Backbone\n",
    "\n",
    "**High-quality annotation:** POS tags, phrase-structure trees, consistent WSJ text\n",
    "\n",
    "**Used across tasks:** POS tagging, parsing, LM, syntactic features for NER/QA\n",
    "\n",
    "**Right size for 1990s–2000s compute:** large enough for statistics, small enough to train\n",
    "\n",
    "**Standard benchmarks:** shared splits enabled reproducible research\n",
    "\n",
    "**Reliable source (LDC @ Penn):** clear licensing, consistent versioning\n",
    "\n",
    "→ Became the de-facto shared dataset for statistical NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Penn Treebank Example (Data Backbone)\n",
    "\n",
    "A PTB item pairs a sentence with its phrase-structure tree and POS tags.\n",
    "\n",
    "```\n",
    "(S\n",
    "  (NP-SBJ (DT The) (NN company))\n",
    "  (VP (VBD said)\n",
    "      (SBAR\n",
    "        (S (NP-SBJ (PRP it))\n",
    "           (VP (MD would)\n",
    "               (VP (VB cut)\n",
    "                   (NP (NNS costs))\n",
    "                   (PP (IN by)\n",
    "                       (NP (CD 10) (NN percent)))))))\n",
    "  (. .))\n",
    "```\n",
    "\n",
    "Raw sentence: *The company said it would cut costs by 10 percent.*\n",
    "\n",
    "- POS tags: DT, NN, VBD, PRP …\n",
    "- Phrase labels: NP, VP, PP, SBAR, S\n",
    "- Bracketed tree = hallmark PTB format\n",
    "\n",
    "*(Statistical models learned probabilities from thousands of such trees.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basic Math: Probability & Estimation\n",
    "**1. Probability as Likelihood (Intuition)**\n",
    "\n",
    "Language is unpredictable → we model how likely each word or label is\n",
    "P(A|B) = \"probability of A given B\"\n",
    "\n",
    "**Example:**\n",
    "\n",
    "P(\"morning\" | \"good\") >> P(\"hippopotamus\" | \"good\")\n",
    "\n",
    "**2. Count-Based Estimation**\n",
    "\n",
    "Probability ≈ how often something appears in a corpus\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If \"San Francisco\" appears 10,000 times and \"San Jose\" appears 5,000 times, the model learns \"San Francisco\" is more common.\n",
    "\n",
    "**3. Local Dependencies (Markov Idea)**\n",
    "\n",
    "The next word depends mostly on a few previous words.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "P(\"you\" | \"thank\") is high  \n",
    "P(\"saucepan\" | \"thank\") is near zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Basic Math: Features & Sequence Decisions\n",
    "\n",
    "**4. Features as Linguistic Clues**\n",
    "\n",
    "Models use signals from the text to make decisions.\n",
    "\n",
    "**Examples of features:**\n",
    "\n",
    "- word suffix \"-ed\" → likely past tense\n",
    "- capitalized → maybe a name\n",
    "- previous word = \"Mr.\" → next likely PER\n",
    "- contains digits → maybe a date/number\n",
    "\n",
    "**These features feed:**\n",
    "\n",
    "- Naive Bayes\n",
    "- MaxEnt classifiers\n",
    "- CRFs\n",
    "\n",
    "**5. Joint Decision for Sequences**\n",
    "\n",
    "For tasks like POS tagging or NER, labels influence each other.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "\"New York City\" → all three tokens should be labeled as LOCATION\n",
    "\n",
    "*(Sequence models like HMMs and CRFs enforce consistency.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Probabilistic Models\n",
    "\n",
    "- **n-grams** for language modeling\n",
    "- **Hidden Markov Models (HMMs)** for POS tagging & speech recognition\n",
    "- **Maximum Entropy models** for classification\n",
    "- **Conditional Random Fields (CRFs)** for sequence labeling (NER, segmentation)\n",
    "\n",
    "*(Uncertainty is modeled mathematically.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### n-gram Language Models\n",
    "\n",
    "- **What**: Count-based probabilistic models over short word sequences\n",
    "- **Key concept**: Markov assumption — next word depends on the previous *(n−1)* words; smoothed conditional probabilities\n",
    "- **Used for**: Language modeling, predictive text, decoding in speech recognition and MT\n",
    "- **Why it works**: Converts large corpora into likelihoods that capture dominant local usage patterns\n",
    "- **Builds on**: Extends rule-based frequency tables with statistical estimation and smoothing learned directly from corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How n-gram Assigns Probability to a Sentence\n",
    "\n",
    "**Sentence Probability = Product of Local Conditional Probabilities**\n",
    "\n",
    "Using the chain rule, the probability of a sentence:\n",
    "\n",
    "*P(w₁, w₂, …, wₜ)*\n",
    "\n",
    "is approximated by an n-gram model as:\n",
    "\n",
    "*P(w₁) × P(w₂|w₁) × P(w₃|w₁, w₂) × …*\n",
    "\n",
    "Under an n-gram assumption:\n",
    "\n",
    "*P(wₜ|wₜ₋₁, …, wₜ₋ₙ₊₁)*\n",
    "\n",
    "**Example (bigram):**\n",
    "\n",
    "Sentence: \"I want to eat pizza\"\n",
    "\n",
    "*P(I) × P(want|I) × P(to|want) × P(eat|to) × P(pizza|eat)*\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- Each conditional probability measures how \"natural\" that local phrase is\n",
    "- Multiplying them gives an overall fluency score for the whole sentence\n",
    "- High score → common, natural sentence\n",
    "- Very low score → unnatural or incorrect sentence\n",
    "\n",
    "*(This is the core job of a language model.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### How n-gram Probabilities Add Value in Real Applications\n",
    "\n",
    "**n-gram LM = Fluency/Correctness Scorer**\n",
    "\n",
    "| Application | What Creates Candidates | What n-gram Adds | Result |\n",
    "|------------|------------------------|------------------|--------|\n",
    "| **Spell Checking** | String-similarity corrections (receive / revise / relieve) | Scores each candidate in sentence context | Picks correct word in context |\n",
    "| **Speech Recognition** | Acoustic model generates many possible word sequences | Scores each sequence by fluency | Picks the natural sentence, not just sound match |\n",
    "| **Machine Translation** | Translation model proposes literal or alternative translations | Scores target-language fluency | Picks most natural translation |\n",
    "| **Predictive Text** | All words in vocabulary | Predictive next-word probabilities | Produces useful next-word suggestions |\n",
    "\n",
    "**Key Insight**\n",
    "\n",
    "Across all tasks, n-gram LM is the component that ensures the output \"sounds like real language.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hidden Markov Models (HMMs)\n",
    "\n",
    "- **What**: Generative sequence models with hidden state chains emitting observed words or acoustic frames\n",
    "- **Key concept**: Transition matrix over tags/states + emission probabilities; decoded efficiently via Viterbi\n",
    "- **Used for**: POS tagging, speech recognition, shallow parsing\n",
    "- **Why it works**: Separates latent linguistic structure from surface tokens and models uncertainty end-to-end\n",
    "- **Builds on**: Adds probabilistic state transitions on top of n-gram assumptions and reuses linguistic tagsets from rule-based systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Math of \\(P(s, x)\\) in a Hidden Markov Model\n",
    "\n",
    "We model the joint probability of:\n",
    "\n",
    "- Hidden states: $s = (s_1, s_2, ..., s_T)$\n",
    "- Observed words: $x = (x_1, x_2, ..., x_T)$\n",
    "\n",
    "The HMM factorizes the joint probability as:\n",
    "\n",
    "$$P(s, x) = P(s_1) \\cdot \\prod_{t=2}^{T} P(s_t \\mid s_{t-1}) \\cdot \\prod_{t=1}^{T} P(x_t \\mid s_t)$$\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- $P(s_1)$: probability of the starting state\n",
    "- $P(s_t \\mid s_{t-1})$: transition probability\n",
    "- $P(x_t \\mid s_t)$: emission probability  \n",
    "\n",
    "This expresses the generative story of an HMM:\n",
    "1. Pick a starting hidden tag  \n",
    "2. Emit a word based on that tag  \n",
    "3. Transition to the next hidden tag  \n",
    "4. Emit the next word  \n",
    "5. Repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of \\(P(s, x)\\) for POS Tagging\n",
    "\n",
    "**Sentence:** `The dog barks`  \n",
    "**Hidden states (POS):**  \n",
    "\\( s = (DT, NN, VBZ) \\)  \n",
    "**Observed words:**  \n",
    "\\( x = (The, dog, barks) \\)\n",
    "\n",
    "$$P(s, x) = P(DT) \\cdot P(NN \\mid DT) \\cdot P(VBZ \\mid NN) \\cdot P(The \\mid DT) \\cdot P(dog \\mid NN) \\cdot P(barks \\mid VBZ)$$\n",
    "\n",
    "**Meaning:**\n",
    "- Probability of starting in DT  \n",
    "- Probability that NN follows DT  \n",
    "- Probability that VBZ follows NN  \n",
    "- Probability each word is emitted by its tag  \n",
    "\n",
    "Multiply all terms → the joint probability of generating both the tag sequence and the sentence under the HMM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Maximum Entropy (Log-Linear) Models\n",
    "\n",
    "- **What**: Conditional probability models that weight arbitrary linguistic features to predict labels\n",
    "- **Key concept**: Choose weights that maximize entropy subject to empirical feature expectations (a flexible logistic regression)\n",
    "- **Used for**: Text classification, POS/NER taggers, feature-rich decision components in MT pipelines\n",
    "- **Why it works**: Combines many overlapping signals without independence assumptions; learns optimal weights from data\n",
    "- **Builds on**: Takes handcrafted linguistic cues from earlier systems but replaces manual rules with data-driven weight learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conditional Random Fields (CRFs)\n",
    "\n",
    "- **What**: Discriminative sequence models that directly learn \\(P(\\mathbf{y}\\mid\\mathbf{x})\\) over label sequences\n",
    "- **Key concept**: Log-linear factors over adjacent labels and rich features spanning the whole sentence; avoids label bias\n",
    "- **Used for**: NER, segmentation, POS, bioNLP entity/event extraction\n",
    "- **Why it works**: Supports arbitrary, overlapping features while enforcing global consistency across the sequence\n",
    "- **Builds on**: Extends Maximum Entropy to structured outputs and replaces HMM generative assumptions with conditional training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Math of a Linear-Chain Conditional Random Field (CRF)\n",
    "\n",
    "A CRF models the conditional probability of a label sequence **y** given the entire observation sequence **x**:\n",
    "\n",
    "$$P(y \\mid x) = \\frac{1}{Z(x)} \\exp\\left( \\sum_{t=1}^{T} \\sum_{k} \\lambda_k \\, f_k(y_{t-1}, y_t, x, t) \\right)$$\n",
    "\n",
    "**Components**\n",
    "\n",
    "- $f_k(\\cdot)$: feature functions  \n",
    "  (e.g., \"is the word capitalized?\", \"previous label = B-LOC?\")\n",
    "- $\\lambda_k$: learned weights for each feature\n",
    "- $Z(x)$: normalization term (ensures probabilities sum to 1)\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- The model scores a whole **label sequence** using rich features.\n",
    "- Then normalizes over *all possible label sequences*.\n",
    "- Training = learn weights $\\lambda_k$ that best fit data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of CRF for NER (Named Entity Recognition)\n",
    "\n",
    "**Sentence:**  \n",
    "`New York City is busy`\n",
    "\n",
    "**Labels (BIO scheme):**  \n",
    "`B-LOC   I-LOC   I-LOC   O   O`\n",
    "\n",
    "CRF uses **features across words and labels**:\n",
    "\n",
    "**Examples of features:**\n",
    "- Current word is capitalized?\n",
    "- Previous label = B-LOC?\n",
    "- Next word starts with capital?\n",
    "- Word shape (\"Xx\")\n",
    "- Suffix \"-ity\" etc.\n",
    "\n",
    "CRF score for the sequence:\n",
    "\n",
    "$$\\text{score}(y, x) = \\sum_{t,k} \\lambda_k \\, f_k(y_{t-1}, y_t, x, t)$$\n",
    "\n",
    "Prediction:\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y \\mid x)$$\n",
    "\n",
    "**Key idea:**  \n",
    "The CRF can learn transitions like:\n",
    "\n",
    "- B-LOC → I-LOC (good)  \n",
    "- I-LOC → O (common)  \n",
    "- B-LOC → B-PER (bad, penalized)\n",
    "\n",
    "CRF chooses the best entire sequence, not token-by-token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HMM vs CRF (Key Differences)\n",
    "\n",
    "| Aspect | Hidden Markov Model (HMM) | Conditional Random Field (CRF) |\n",
    "|-------|----------------------------|--------------------------------|\n",
    "| Model Type | Generative | Discriminative |\n",
    "| What It Learns | P(y) and P(x | y), combines to P(x, y) | Directly learns P(y | x) |\n",
    "| Assumption | Strong independence assumptions (word depends only on its tag) | No independence assumptions on features |\n",
    "| Features | Limited: emissions must be simple (word, maybe morphology) | Arbitrary, overlapping, global features allowed |\n",
    "| Label Bias Problem | Yes | No |\n",
    "| Sequence Consistency | Local (only via transitions) | Global (entire sequence optimized jointly) |\n",
    "| Typical Use | POS tagging (early), speech recognition (with acoustics) | NER, segmentation, chunking, POS tagging (modern) |\n",
    "| Strength | Simple, fast, usable with small data | Much more accurate, flexible, powerful |\n",
    "| Weakness | Cannot use rich features; weaker accuracy | Needs labeled data and more compute |\n",
    "\n",
    "**Summary**  \n",
    "- **HMM** models how words are *generated* from hidden states.  \n",
    "- **CRF** models how labels should be assigned *given the whole sentence*.  \n",
    "- CRFs outperform HMMs on most NLP sequence-labeling tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "\n",
    "**What it is**\n",
    "- Statistical method that discovers hidden semantic structure in text\n",
    "- Based on co-occurrence counts and **SVD (Singular Value Decomposition)**\n",
    "\n",
    "**Key idea**\n",
    "- Words that appear in similar contexts have similar meanings\n",
    "- Compress the word–document matrix into a low-dimensional semantic space\n",
    "\n",
    "**Math**\n",
    "1. Build term-document matrix (TF–IDF)\n",
    "2. Apply SVD:  M ≈ U Σ Vᵀ   (keep top k dimensions)\n",
    "3. Use vectors from U/V as semantic embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "\n",
    "**Used for**\n",
    "- Document similarity, clustering  \n",
    "- Topic modeling (pre-LDA era)  \n",
    "- Early summarization and search  \n",
    "- Foundation for word embeddings\n",
    "\n",
    "**Why it mattered**\n",
    "- First successful statistical method capturing semantic similarity  \n",
    "- Precursor to word2vec/GloVe  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "**What it is**\n",
    "- A **probabilistic topic model**\n",
    "- Assumes each document is a mixture of topics\n",
    "- Each topic is a distribution over words\n",
    "\n",
    "**Generative idea**\n",
    "1. For each document, choose topic proportions (Dirichlet).\n",
    "2. For each word:\n",
    "   - Pick a topic from the doc’s topic proportions\n",
    "   - Pick a word from that topic’s word distribution\n",
    "\n",
    "**Key equations**\n",
    "- Topic distribution per document:  θ_d ~ Dirichlet(α)\n",
    "- Word distribution per topic:    φ_k ~ Dirichlet(β)\n",
    "- Word generation:  z ~ Mult(θ_d),  w ~ Mult(φ_z)\n",
    "\n",
    "**Why it’s useful**\n",
    "- Discovers hidden themes in large text collections\n",
    "- Each doc gets a vector of topic weights → useful for clustering, search, classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applications Powered by Statistical NLP\n",
    "\n",
    "- Search engines → relevance ranking (TF–IDF, probabilistic IR)\n",
    "- Spam filtering → Naive Bayes, SVM\n",
    "- Speech recognition → HMM-based decoding\n",
    "- Machine translation → IBM statistical models\n",
    "- NER, POS tagging → HMM/CRF taggers\n",
    "- Sentiment analysis → supervised classification\n",
    "\n",
    "*(Scaled to real-world data for the first time.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Strengths of Statistical NLP\n",
    "\n",
    "- Learns from data → scalable\n",
    "- Handles messy real-world text\n",
    "- Manages ambiguity probabilistically\n",
    "- Reduces manual rule writing\n",
    "- Foundation for early web search & speech engines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Limitations → Motivation for Deep Learning\n",
    "\n",
    "- Heavy feature engineering required\n",
    "- Sparse data → poor generalization\n",
    "- Struggles with long-distance dependencies\n",
    "- Requires labeled training data\n",
    "- Words treated as symbols (no notion of meaning)\n",
    "\n",
    "*(Set the stage for word embeddings and neural networks.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Deep Learning Eliminates Sparsity (Word-Level & Above)\n",
    "\n",
    "**1. Word Embeddings → Fix sparsity above the word level**\n",
    "- Before: each word = isolated symbol (Paris, London, Berlin unrelated)  \n",
    "  → unseen word combinations = zero counts, brittle models  \n",
    "- After: dense word vectors  \n",
    "  → similar words cluster; models generalize to unseen phrases/sentences\n",
    "\n",
    "**2. Subword/Character Embeddings → Fix sparsity at the word level**\n",
    "- Before: OOV words = impossible to represent (probability = 0)  \n",
    "- After: BPE/WordPiece split words into pieces (un-, -tion, -ing…)  \n",
    "  → even unseen words have vectors; morphology becomes learnable\n",
    "\n",
    "**Result:**  \n",
    "Word embeddings handle semantic similarity, subwords handle OOV/morphology, and neural layers build contextual meaning → sparsity becomes a minor issue compared to statistical NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Deep Learning Fixes “Words Treated as Symbols” (with definition of meaning)\n",
    "\n",
    "**What is “meaning” in NLP?**  \n",
    "- Meaning is relational: a word is defined by the contexts it appears in and its similarity to other words.\n",
    "\n",
    "**Before (Statistical NLP):**\n",
    "- Words were **IDs only**, no semantic relation (Paris ≠ London ≠ Berlin in the model).\n",
    "- Only **primitive meaning signals** existed (POS, morphology), which tell *role* but not *true meaning*.\n",
    "  - e.g., *dog* and *cat* both → noun (POS), but models cannot tell they are semantically close.\n",
    "\n",
    "**After (Deep Learning):**\n",
    "- **Word embeddings** learn relational meaning:\n",
    "  - Similar contexts → nearby vectors (king–man + woman ≈ queen).\n",
    "- **Contextual embeddings** give different meanings based on usage:\n",
    "  - “bank” (river) vs “bank” (finance).\n",
    "- **Subword embeddings** capture morphology and rare words:\n",
    "  - “bioluminescence” → bio + lumi + nescence.\n",
    "\n",
    "**Result:**  \n",
    "Deep learning replaces symbolic word IDs with **rich semantic, relational, context-dependent representations**, providing real “meaning” instead of shallow syntactic categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Word Embeddings and Deep Learning\n",
    "\n",
    "**Deep Learning NLP (2010s–2018)**\n",
    "\n",
    "## Evolution\n",
    "\n",
    "- **Word2Vec (2013)** — meaning as vectors\n",
    "- **RNN/LSTM (2014–2016)** — sequential modeling\n",
    "- **Attention (2017)**\n",
    "- **Transformer (2018)**\n",
    "\n",
    "## Capabilities\n",
    "\n",
    "- Better translation\n",
    "- Better speech recognition\n",
    "- Better sentiment, NER\n",
    "- End of feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Why We Need Word Embeddings\n",
    "\n",
    "**Statistical NLP limitations:**\n",
    "- Words treated as **IDs only** → no semantic similarity  \n",
    "- Sparse data → unseen combinations get **zero counts**  \n",
    "- Heavy reliance on **manual features** (suffixes, capitalization, POS)  \n",
    "- No way to capture relationships like:  \n",
    "  *Paris ≈ London ≈ Berlin*  \n",
    "  *run ≈ jog ≈ sprint*\n",
    "\n",
    "**Core problem:**  \n",
    "The model sees no relation between similar words — all are isolated symbols.\n",
    "\n",
    "**Motivation for embeddings:**  \n",
    "We need a **dense, learned, continuous representation** so that:\n",
    "- similar words are near each other,\n",
    "- rare/unseen words still get meaningful vectors,\n",
    "- models can generalize beyond observed counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide: subslide -->\n",
    "\n",
    "### What Is a Word Embedding?\n",
    "\n",
    "**Definition:**  \n",
    "A *word embedding* is a **dense vector** (typically 50–300 dimensions) that represents a word based on **its context and usage**.\n",
    "\n",
    "**Key properties:**\n",
    "- Similar words → **nearby vectors**  \n",
    "  - *Paris*, *London*, *Berlin* cluster together  \n",
    "  - *king – man + woman ≈ queen*\n",
    "- Encodes **semantic** and **syntactic** information  \n",
    "- Learned automatically from large corpora (distributional hypothesis)\n",
    "\n",
    "**Conceptual picture:**\n",
    "- One-hot:  \n",
    "  `[0 0 0 ... 1 ... 0]`  → no relationship\n",
    "- Embedding:  \n",
    "  `[0.39, -0.12, 0.85, ...]` → captures meaning via geometry\n",
    "\n",
    "**Why it matters:**  \n",
    "Embeddings transform language from **symbolic** to **semantic**, enabling neural networks to learn meaning instead of memorizing patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Timeline of Word Embeddings (2000–2017)\n",
    "\n",
    "**2000–2010 — Neural Language Models (early DL)**  \n",
    "- Bengio et al. (2003): neural LM with learned embeddings  \n",
    "- Too slow to scale, but introduced the core idea: **embedding matrix + neural prediction**\n",
    "\n",
    "**2013 — Word2Vec (Mikolov et al.)**  \n",
    "- Skip-Gram and CBOW  \n",
    "- Efficient training on billions of tokens  \n",
    "- Sparked the modern embedding revolution\n",
    "\n",
    "**2014 — GloVe (Pennington et al.)**  \n",
    "- Global co-occurrence matrix factorization  \n",
    "- Complement to window-based Word2Vec\n",
    "\n",
    "**2014–2016 — fastText (Facebook)**  \n",
    "- Subword/character n-gram embeddings  \n",
    "- Solved OOV and morphology limitations\n",
    "\n",
    "**2015–2017 — Contextual Precursor Models**  \n",
    "- ELMo (2018 precursor architecture)  \n",
    "- Bi-LSTM + embeddings → context-dependent meaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### How Word Embeddings Are Trained (Intuition)\n",
    "\n",
    "**Key idea: Distributional Hypothesis**  \n",
    "“You shall know a word by the company it keeps.”\n",
    "\n",
    "Words appearing in similar contexts → acquire similar vectors.\n",
    "\n",
    "Two major training styles:\n",
    "\n",
    "**1. Skip-Gram (Predict context given a word)**  \n",
    "- Input: “bank”  \n",
    "- Predict nearby words: *river, water, money, loan*  \n",
    "- Objective: words with similar neighborhoods → similar vectors\n",
    "\n",
    "**2. CBOW (Continuous Bag of Words — Predict the word from its context)**  \n",
    "- Input: “the *?* is flowing fast”  \n",
    "- Predict: *river*\n",
    "\n",
    "**Common idea:**  \n",
    "Train on massive corpora → embeddings emerge automatically from co-occurrence behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Skip-Gram (Window Size = 2)\n",
    "\n",
    "Sentence: **The quick brown fox jumps over the lazy dog**  \n",
    "Indexes:  \n",
    "1:The  2:quick  3:brown  4:fox  5:jumps  6:over  7:the  8:lazy  9:dog\n",
    "\n",
    "Window size = **2**\n",
    "\n",
    "#### Center = “fox” (position 4)\n",
    "Context positions = **2, 3, 5, 6**\n",
    "\n",
    "Training pairs:\n",
    "- (fox → quick)\n",
    "- (fox → brown)\n",
    "- (fox → jumps)\n",
    "- (fox → over)\n",
    "\n",
    "#### Center = “jumps” (position 5)\n",
    "Context positions = **3, 4, 6, 7**\n",
    "\n",
    "Training pairs:\n",
    "- (jumps → brown)\n",
    "- (jumps → fox)\n",
    "- (jumps → over)\n",
    "- (jumps → the)\n",
    "\n",
    "**Summary:**  \n",
    "Skip-Gram predicts **context words from the center word**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### CBOW (Window Size = 2)\n",
    "\n",
    "Sentence: **The quick brown fox jumps over the lazy dog**  \n",
    "Indexes:  \n",
    "1:The  2:quick  3:brown  4:fox  5:jumps  6:over  7:the  8:lazy  9:dog\n",
    "\n",
    "Window size = **2**\n",
    "\n",
    "#### Center = “fox” (position 4)\n",
    "Context positions = **2, 3, 5, 6**\n",
    "\n",
    "- Input: **{quick, brown, jumps, over}**  \n",
    "- Predict: **fox**\n",
    "\n",
    "#### Center = “jumps” (position 5)\n",
    "Context positions = **3, 4, 6, 7**\n",
    "\n",
    "- Input: **{brown, fox, over, the}**  \n",
    "- Predict: **jumps**\n",
    "\n",
    "**Summary:**  \n",
    "CBOW predicts the **center word from surrounding context words**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Why Word Embeddings Exploded Around 2013\n",
    "\n",
    "**1. Data got big enough**  \n",
    "- Web-scale corpora (billions of words) became available  \n",
    "- Finally enough signal to learn stable semantic patterns\n",
    "\n",
    "**2. Compute got fast enough**  \n",
    "- Multicore CPUs + GPUs made vector math cheap  \n",
    "- Training neural models became practical (hours, not weeks)\n",
    "\n",
    "**3. Algorithms became efficient**  \n",
    "- Word2Vec introduced **negative sampling** + **subsampling**  \n",
    "- Shallow SG/CBOW architectures scaled to huge vocabularies\n",
    "\n",
    "**Result:**  \n",
    "Embeddings became fast, scalable, and meaningful — enabling modern neural NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Why Word Embeddings Mattered (The Real Breakthrough)\n",
    "\n",
    "**Embeddings finally showed *true semantic structure*.**  \n",
    "This was the moment the NLP community realized neural methods worked.\n",
    "\n",
    "**Shocking results at the time:**\n",
    "- *king – man + woman ≈ queen*  \n",
    "- *Paris ≈ London ≈ Berlin*  \n",
    "- *walk, walking, walked* clustering naturally\n",
    "\n",
    "**What made this transformative:**\n",
    "- **Linear analogies** emerged from geometry  \n",
    "- **High-quality nearest neighbors** (semantic and syntactic)  \n",
    "- **Smooth, meaningful clusters** across massive vocabularies  \n",
    "\n",
    "Earlier methods (LSA, early neural LMs) hinted at similarity,  \n",
    "but **never at this clarity, scale, or consistency**.\n",
    "\n",
    "**Impact:**  \n",
    "This single discovery convinced researchers that  \n",
    "**dense vectors encode real meaning**, launching modern neural NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### GloVe: What It Adds on Top of Word2Vec\n",
    "\n",
    "**What Word2Vec uses:**  \n",
    "- Local context windows (Skip-Gram / CBOW)  \n",
    "- Learns meaning from *predicting* nearby words  \n",
    "- Great semantic clusters but only local statistics\n",
    "\n",
    "---\n",
    "\n",
    "### **What GloVe adds: Global Co-Occurrence Information**\n",
    "- Builds a **global word–word co-occurrence matrix**  \n",
    "- Learns embeddings by **factorizing** it with a weighted least-squares loss  \n",
    "- Embeddings capture **ratios** of probabilities  \n",
    "  - ice : cold  vs.  steam : hot  \n",
    "- Encodes semantic structure that **local windows miss**\n",
    "\n",
    "---\n",
    "\n",
    "### **How GloVe + Word2Vec connect**\n",
    "- Word2Vec = **local prediction**  \n",
    "- GloVe = **global co-occurrence factorization**  \n",
    "- Both produce dense vectors, but GloVe injects **global corpus structure** into the embedding space\n",
    "\n",
    "**Result:**  \n",
    "More stable embeddings and better analogy structure on some tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### fastText: What It Adds on Top of Word2Vec\n",
    "\n",
    "**What Word2Vec lacks:**  \n",
    "- Treats each word as a whole symbol  \n",
    "- Fails on **OOV** (out-of-vocabulary) words  \n",
    "- Weak on morphology (walk, walking, walker)\n",
    "\n",
    "---\n",
    "\n",
    "### **What fastText adds: Subword / Character n-grams**\n",
    "- Splits each word into character-level n-grams  \n",
    "  - “playing” → play, lay, yin, ing, pla…  \n",
    "- Embedding = **sum of subword embeddings**  \n",
    "- Even unseen words are representable (OOV solved)  \n",
    "- Morphology becomes learnable automatically\n",
    "\n",
    "---\n",
    "\n",
    "### **How fastText + Word2Vec connect**\n",
    "- fastText still uses **Skip-Gram / CBOW**  \n",
    "- But replaces “word as a single token” with  \n",
    "  **word = bag of character n-grams**\n",
    "- Keeps Word2Vec’s training idea  \n",
    "- Extends it to handle **rare words, morphology, OOV**\n",
    "\n",
    "**Result:**  \n",
    "Word2Vec semantics + subword structure = robust, generalizable embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Embedding Dimensions: Typical Sizes & What They Mean\n",
    "\n",
    "#### **Typical Dimensions**\n",
    "| Model       | Common Dim |\n",
    "|-------------|------------|\n",
    "| **Word2Vec** | 100–300 |\n",
    "| **GloVe**     | 50–300 |\n",
    "| **fastText**  | 100–300 |\n",
    "| **Contextual Models (BERT/GPT)** | 768–12k+ |\n",
    "\n",
    "**Why 100–300?**  \n",
    "- Big enough to encode rich semantic structure  \n",
    "- Small enough to train efficiently  \n",
    "- Empirically the best tradeoff for static embeddings\n",
    "\n",
    "---\n",
    "\n",
    "### **Do dimensions have actual meaning?**\n",
    "\n",
    "**Not individually**  \n",
    "- No single dimension corresponds cleanly to a topic or concept\n",
    "- You cannot point to dimension **#47** and say “this is the *animal* dimension”  \n",
    "\n",
    "**Think of it like a coordinate system:**  \n",
    "Individual axes don’t mean anything alone,  \n",
    "but **distances and directions** encode meaningful relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### From Embeddings to Deep Neural Models\n",
    "\n",
    "Once words are represented as dense vectors, they can be fed into neural networks:\n",
    "\n",
    "**1. RNNs (Recurrent Neural Networks)**  \n",
    "- Read tokens sequentially  \n",
    "- Capture short-range dependencies  \n",
    "- Still struggled with long-distance context\n",
    "\n",
    "**2. LSTMs / GRUs**  \n",
    "- Improved memory  \n",
    "- Enabled early neural MT, tagging, classification\n",
    "\n",
    "**3. CNNs for Text**  \n",
    "- Extract local n-gram patterns  \n",
    "- Worked well for sentiment and classification\n",
    "\n",
    "**Embeddings → enable neural networks to reason over meaning,  \n",
    "not just symbols or counts.**\n",
    "\n",
    "This sets the stage for:\n",
    "- Attention (2017)  \n",
    "- Transformers (2018)  \n",
    "- Modern LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Transformers and the LLM Revolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.1 Self-Attention Mechanism\n",
    "\n",
    "*[Content to be filled]*\n",
    "\n",
    "- The attention mechanism\n",
    "- Query, Key, Value (QKV) framework\n",
    "- Multi-head attention\n",
    "- Why attention is powerful\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.2 Transformer Architecture\n",
    "\n",
    "*[Content to be filled]*\n",
    "\n",
    "- Encoder-decoder architecture\n",
    "- Positional encoding\n",
    "- Layer normalization and residual connections\n",
    "- Feed-forward networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 5.3 Major Transformer Models\n",
    "\n",
    "*[Content to be filled]*\n",
    "\n",
    "- **BERT**: Bidirectional Encoder Representations\n",
    "- **GPT**: Generative Pre-trained Transformer\n",
    "- **T5**: Text-to-Text Transfer Transformer\n",
    "- Other notable models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Modern LLM Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.1 Core NLP Tasks\n",
    "\n",
    "*[Content to be filled]*\n",
    "\n",
    "- **Text Summarization**: Extractive and abstractive\n",
    "- **Question Answering**: Reading comprehension\n",
    "- **Sentiment Analysis**: Understanding emotions and opinions\n",
    "- **Named Entity Recognition**: Identifying entities in text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.2 Advanced Applications\n",
    "\n",
    "*[Content to be filled]*\n",
    "\n",
    "- **AI Agents**: Autonomous systems with NLP capabilities\n",
    "- **Conversational AI**: Chatbots and dialogue systems\n",
    "- **Code Generation**: Programming with natural language\n",
    "- **Multimodal Systems**: Text, images, and beyond\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.3 Training Paradigms\n",
    "\n",
    "*[Content to be filled]*\n",
    "\n",
    "- **Fine-tuning**: Adapting pre-trained models to specific tasks\n",
    "- **Instruction-tuning**: Training models to follow instructions\n",
    "- **In-context learning**: Few-shot and zero-shot capabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 6.4 Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "*[Content to be filled]*\n",
    "\n",
    "- Combining retrieval with generation\n",
    "- Knowledge bases and vector databases\n",
    "- Reducing hallucinations and improving accuracy\n",
    "- Practical applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary and Key Takeaways\n",
    "\n",
    "*[Content to be filled]*\n",
    "\n",
    "- Evolution from rules to LLMs\n",
    "- Key concepts and techniques\n",
    "- Modern applications and future directions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?\n",
    "\n",
    "---\n",
    "\n",
    "Thank you!\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "nbconvert": {
   "custom_css": "<style>\n@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');\n\n/* Apply Inter font and light background to all slides (for reveal.js / nbconvert) */\n.reveal {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n  background-color: #f5f5f5;\n}\n\n.reveal .slides {\n  background-color: #f5f5f5;\n}\n\n.reveal .slides section {\n  background-color: #fafafa;\n  color: #333333;\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n}\n\n.reveal h1,\n.reveal h2,\n.reveal h3,\n.reveal h4,\n.reveal h5,\n.reveal h6 {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n  font-weight: 600;\n  color: #1a1a1a;\n}\n\n.reveal p,\n.reveal li,\n.reveal td,\n.reveal th {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n  color: #333333;\n}\n\n/* Ensure code blocks maintain readability */\n.reveal pre,\n.reveal code {\n  font-family: 'Fira Code', 'Consolas', 'Monaco', 'Courier New', monospace;\n  background-color: #ffffff;\n}\n\n/* Table styling */\n.reveal table {\n  background-color: #ffffff;\n  border-collapse: collapse;\n}\n\n.reveal table th {\n  background-color: #e8e8e8;\n  color: #1a1a1a;\n}\n\n.reveal table td {\n  background-color: #ffffff;\n}\n\n/* Ensure proper contrast for links */\n.reveal a {\n  color: #0066cc;\n}\n\n.reveal a:hover {\n  color: #0052a3;\n}\n\n/* Additional styling for RISE slideshow */\n.rise-enabled .reveal {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n  background-color: #f5f5f5;\n}\n\n.rise-enabled .reveal .slides section {\n  background-color: #fafafa;\n  color: #333333;\n}\n\n/* For notebook cell rendering (non-slideshow view) */\n.jp-RenderedHTMLCommon {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n}\n\n.jp-RenderedMarkdown {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n}\n</style>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
