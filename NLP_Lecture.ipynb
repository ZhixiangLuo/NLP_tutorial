{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Introduction to Natural Language Processing (NLP)\n",
    "#### *from classical methods to modern LLMs*\n",
    "\n",
    "**Part I — Lecture (1.5 hours)**\n",
    "\n",
    "___\n",
    "\n",
    "### Speaker: Jeffrey Luo, Ph.D.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## About me\n",
    "\n",
    "[LinkedIn](https://www.linkedin.com/in/zhixiang-jeffrey-luo-70850124/) | [Google Scholar](https://scholar.google.com/citations?user=gzyW_GUAAAAJ&hl=en)\n",
    "\n",
    "**Ph.D. in Physics**, UNC Chapel Hill  \n",
    "**B.S. in Mathematics and Physics**, Tsinghua University\n",
    "### Experience\n",
    "- **Workday**: LLM-based agentic systems for chat, search, and automation\n",
    "- **Gradient AI**: Deep learning models for health insurance underwriting\n",
    "- **Wolters Kluwer**: ML models for infectious disease prediction \n",
    "- **T2 Biosystems**: NMR systems for sepsis detection\n",
    "- **Schlumberger**: NMR study of porous materials\n",
    "\n",
    "### Hobby Project\n",
    "**OrcaEcho.ai**: AI assistant for presentation creation and editing  \n",
    "[Google Slides Add-on](https://workspace.google.com/marketplace/app/orcarina/404235414546) | [Website](https://orcaecho.ai)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part I: Lecture Overview\n",
    "\n",
    "1. **What is NLP and Why It Matters**\n",
    "2. **Rule-Based NLP (1950s–1980s)**\n",
    "3. **Statistical NLP (1990s–2010s)**\n",
    "4. **Word Embeddings and Deep Learning (2010-2018)**\n",
    "5. **Transformers and the LLM Revolution (2018-today)**\n",
    "6. **Modern LLM Applications**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. What is NLP and Why It Matters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.1 What is NLP?\n",
    "\n",
    "**Natural Language Processing (NLP)** is a field of AI focused on enabling computers to understand, interpret, generate, and interact with human language.\n",
    "\n",
    "### Some common things powered by NLP:\n",
    "\n",
    "- **Translation** (e.g., English → Chinese)\n",
    "- **Chatbots & assistants**\n",
    "- **Speech-to-text / text-to-speech**\n",
    "- **Summarization**\n",
    "- **Sentiment analysis** (detecting positive/negative tone)\n",
    "- **Search and question-answering**\n",
    "\n",
    "NLP blends **linguistics**, **computer science**, and **machine learning** so that machines can handle language in a useful way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.2 Evolution of NLP Approaches\n",
    "\n",
    "### Before Computers: Foundation (Ancient Times - 1950s)\n",
    "\n",
    "**Linguistics**: Analysis of grammar, syntax, semantics, sentence structure, and meaning \n",
    "**Mathematical Logic & Formal Models**: Scholars created systems to describe language in precise, machine-like terms: Formal grammars, symbolic logic etc.\n",
    "\n",
    "### After Computers: NLP Timeline\n",
    "\n",
    "**1954**: First machine translation experiment (Georgetown-IBM)\n",
    "\n",
    "**1960s–90s**: Rule-based language systems  \n",
    "→ Translation, parsing algorithms, computational linguistics\n",
    "\n",
    "**2000s–2010s**: Statistical and neural models  \n",
    "→ Probabilistic approaches, early neural networks\n",
    "\n",
    "**2018+**: Transformers and modern LLMs  \n",
    "→ BERT, GPT, and the transformer revolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.3 Major NLP Applications\n",
    "\n",
    "| **Application Category**                   | **Examples & Notes**                                      |\n",
    "|--------------------------------------------|------------------------------------------------------------|\n",
    "| **Machine Translation (1950s–)**           | Georgetown-IBM demo; Google Translate, DeepL              |\n",
    "| **Search & Information Retrieval (1960s–)**| SMART system; AltaVista; Google Search                    |\n",
    "| **Spell/Grammar & Text Classification (1980s–)** | WordPerfect, spam filters; Grammarly                |\n",
    "| **Speech Recognition (1990s–)**            | Dragon Dictate; Siri, Alexa, Google Assistant             |\n",
    "| **Named Entity & Information Extraction (1990s–)** | MUC NER systems; news/event extraction          |\n",
    "| **Chatbots & QA Systems (2000s–)**         | TREC QA, IBM Watson; ChatGPT, Claude                      |\n",
    "| **Text Generation & LLMs (2018–)**         | GPT series, Claude; rewriting, summarization              |\n",
    "| **AI Agents & Tool Use (2023–)**           | LLMs executing actions via APIs/tools                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Rule-Based NLP (1950s–1980s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What Is Rule-Based NLP? (1950s–1980s)\n",
    "\n",
    "Early NLP relied on **manually written rules** from linguists\n",
    "\n",
    "- **Symbolic, deterministic systems**\n",
    "- **No machine learning or statistical models**\n",
    "- Worked only in **narrow, controlled domains**\n",
    "\n",
    "**Example rule:** \"If a sentence starts with 'Who', classify it as a question.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Language Basics from School\n",
    "\n",
    "**Sentence Structure:**\n",
    "- Sentences have parts: subject, verb, object\n",
    "- Words group into phrases: noun phrase (NP), verb phrase (VP)\n",
    "\n",
    "**Parts of Speech (POS):**\n",
    "- Noun, verb, adjective, adverb\n",
    "- Prepositions, conjunctions\n",
    "\n",
    "You already identify these intuitively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Language Basics from School\n",
    "\n",
    "**Morphology:**\n",
    "- Plural forms: cat → cats\n",
    "- Verb tenses: walk → walked\n",
    "- Irregulars: go → went, mouse → mice\n",
    "\n",
    "**Common Patterns:**\n",
    "- \"Who…?\" → question\n",
    "- \"I feel X\" → emotional statement\n",
    "- \"If…then…\" → condition\n",
    "\n",
    "These everyday language concepts form the foundation of early NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Rule-Based NLP Extended Linguistic Concepts\n",
    "\n",
    "**1. What NLP Inherited from Linguistics**\n",
    "\n",
    "- Sentence structure → NP, VP, subject, object\n",
    "- Parts of speech → nouns, verbs, adjectives, etc.\n",
    "- Morphology concepts → plurals, tenses, irregulars\n",
    "- Lexicon (linguistic sense) → words + meanings\n",
    "\n",
    "*(These are the same concepts students learn in school grammar.)*\n",
    "\n",
    "**2. What NLP Extended into Machine-Readable Form**\n",
    "\n",
    "- **Formal grammars:** Phrase structure rules rewritten as explicit production rules (e.g., S → NP VP, NP → Det N)\n",
    "- **Computational lexicon:** Structured entries storing POS, forms, syntactic frames, semantic features\n",
    "\n",
    "*(Linguistic ideas made precise and explicit for computer use.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Rule-Based NLP Extended Linguistic Concepts\n",
    "\n",
    "**3. What NLP Created Anew for Computation**\n",
    "\n",
    "- **Morphological rule engines:** Algorithms for generating/analyzing word forms, handling exceptions\n",
    "- **Parsing algorithms:** Automatic construction of parse trees from grammar rules\n",
    "\n",
    "*(These are engineering mechanisms that did not exist in pure linguistics.)*\n",
    "\n",
    "**4. Pattern Rules (New for Early NLP Applications)**\n",
    "\n",
    "- IF text matches pattern → THEN respond or extract\n",
    "- *Example: \"I feel X\" → \"Why do you feel X?\"*\n",
    "- Enabled early chatbots (ELIZA) and information extraction\n",
    "- Not derived from linguistics; purely application-driven\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Classic Systems (ELIZA, SHRDLU, MT)\n",
    "\n",
    "**ELIZA (1966) — Shallow rules**\n",
    "\n",
    "- Pattern matching; no understanding\n",
    "- *Example: \"I feel sad.\" → \"Why do you feel sad?\"*\n",
    "\n",
    "**SHRDLU (1970s) — Deep rules in a tiny world**\n",
    "\n",
    "- Real parsing + reasoning, but only in the \"blocks world\"\n",
    "- *Example: \"Put the red block on the green cube.\"*\n",
    "\n",
    "**Rule-Based MT (SYSTRAN) — Large-scale rules**\n",
    "\n",
    "- Thousands of grammar rules; used in real translation systems\n",
    "- Expensive to maintain; brittle outside covered patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Strengths, Weaknesses & Why It Ended\n",
    "\n",
    "**Strengths:**\n",
    "- Interpretable, linguistically grounded\n",
    "- Reliable in narrow domains\n",
    "\n",
    "**Weaknesses:**\n",
    "- Brittle, unscalable\n",
    "- Huge rule sets, fails on new inputs\n",
    "- Could not handle ambiguity or real-world language variation\n",
    "\n",
    "**Shift in 1990s:** Rise of digital text + computing power enabled statistical NLP  \n",
    "Data-driven models outperformed handcrafted rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Statistical NLP (1990s–2010s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What Statistical NLP Built On\n",
    "\n",
    "- Parts of speech → still needed for tagging\n",
    "- Phrase structure → still used for parsing\n",
    "- Lexicon concepts → still needed for word categories\n",
    "- Morphology → still informs word forms\n",
    "- Linguistic features → used as input signals (suffixes, capitalization)\n",
    "\n",
    "*(Kept linguistic structure, but learned patterns instead of hand-writing them.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data-Driven Learning\n",
    "\n",
    "From hand-written rules → data-driven models.\n",
    "\n",
    "- Learn patterns from corpora instead of experts\n",
    "- Use probabilities to handle ambiguity\n",
    "- Built the foundation for modern NLP applications\n",
    "\n",
    "*Example:* \"What word comes next?\" → choose the most probable one from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Corpora Fueling Statistical NLP\n",
    "\n",
    "- **POS Tagging:** Penn Treebank WSJ (PTB-POS), Brown Corpus\n",
    "\n",
    "- **Parsing:** PTB phrase-structure trees, PropBank / FrameNet (predicate–argument)\n",
    "\n",
    "- **NER:** CoNLL-2003 (EN/DE), MUC, ACE\n",
    "\n",
    "- **Machine Translation:** IBM Canadian Hansard (EN–FR), EuroParl, early WMT corpora\n",
    "\n",
    "- **Language Modeling:** PTB LM split, Gigaword, Google N-grams\n",
    "\n",
    "- **Text Classification:** Reuters-21578, 20 Newsgroups, TREC QA\n",
    "\n",
    "- *(Data availability enabled empirical training and benchmarking.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why Penn Treebank Became the Backbone\n",
    "\n",
    "- **High-quality annotation:** POS tags, phrase-structure trees, consistent WSJ text\n",
    "\n",
    "- **Used across tasks:** POS tagging, parsing, LM, syntactic features for NER/QA\n",
    "\n",
    "- **Right size for 1990s–2000s compute:** large enough for statistics, small enough to train\n",
    "\n",
    "- **Standard benchmarks:** shared splits enabled reproducible research\n",
    "\n",
    "- **Reliable source (LDC @ Penn):** clear licensing, consistent versioning\n",
    "\n",
    "→ Became the de-facto shared dataset for statistical NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Penn Treebank Example (Data Backbone)\n",
    "\n",
    "A PTB item pairs a sentence with its phrase-structure tree and POS tags.\n",
    "\n",
    "```\n",
    "(S\n",
    "  (NP-SBJ (DT The) (NN company))\n",
    "  (VP (VBD said)\n",
    "      (SBAR\n",
    "        (S (NP-SBJ (PRP it))\n",
    "           (VP (MD would)\n",
    "               (VP (VB cut)\n",
    "                   (NP (NNS costs))\n",
    "                   (PP (IN by)\n",
    "                       (NP (CD 10) (NN percent)))))))\n",
    "  (. .))\n",
    "```\n",
    "Raw sentence: *The company said it would cut costs by 10 percent.*\n",
    "\n",
    "- POS tags: DT, NN, VBD, PRP …\n",
    "- Phrase labels: NP, VP, PP, SBAR, S\n",
    "- Bracketed tree = hallmark PTB format\n",
    "\n",
    "*(Statistical models learned probabilities from thousands of such trees.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key concepts and assumptions\n",
    "**1. Conditional probability**\n",
    "\n",
    "Language is unpredictable → we model how likely each word or label is\n",
    "P(A|B) = \"probability of A given B\"\n",
    "\n",
    "**Example:** P(\"morning\" | \"good\") >> P(\"hippopotamus\" | \"good\")\n",
    "\n",
    "**2. Word frequency**\n",
    "\n",
    "How often something appears in a corpus\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If \"San Francisco\" appears 10,000 times and \"San Jose\" appears 5,000 times, the model learns \"San Francisco\" is more common.\n",
    "\n",
    "**3. Local Dependencies (Markov Idea)**\n",
    "\n",
    "The next word depends mostly on a few previous words.\n",
    "\n",
    "**Example:** P(\"you\" | \"thank\") is high, P(\"saucepan\" | \"thank\") is near zero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key concepts and assumptions\n",
    "\n",
    "**4. Features from Linguistic Clues** Models use signals from the text to make decisions.\n",
    "\n",
    "**Examples of features:**\n",
    "\n",
    "- word suffix \"-ed\" → likely past tense\n",
    "- capitalized → maybe a name\n",
    "- previous word = \"Mr.\" → next likely PER\n",
    "- contains digits → maybe a date/number\n",
    "\n",
    "**These features feed:** Naive Bayes, MaxEnt classifiers, CRFs\n",
    "\n",
    "**5. Joint Decision for Sequences**\n",
    "\n",
    "For tasks like POS tagging or NER, labels influence each other.\n",
    "\n",
    "**Example:** \"New York City\" → all three tokens should be labeled as LOCATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Probabilistic Models\n",
    "\n",
    "- **n-grams** for language modeling\n",
    "- **Hidden Markov Models (HMMs)** for POS tagging & speech recognition\n",
    "- **Maximum Entropy models** for classification\n",
    "- **Conditional Random Fields (CRFs)** for sequence labeling (NER, segmentation)\n",
    "\n",
    "*(Uncertainty is modeled mathematically.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### n-gram Language Models\n",
    "\n",
    "- **What**: Count-based probabilistic models over short word sequences, **P(next word | previous n-1 words)**\n",
    "- **Key concept**: Markov assumption — next word depends on the previous *(n−1)* words; smoothed conditional probabilities\n",
    "- **Used for**: Language modeling, predictive text, decoding in speech recognition and MT\n",
    "- **Why it works**: Converts large corpora into likelihoods that capture dominant local usage patterns\n",
    "- **Builds on**: Extends rule-based frequency tables with statistical estimation and smoothing learned directly from corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How n-gram Assigns Probability to a Sentence\n",
    "\n",
    "**Sentence Probability = Product of Local Conditional Probabilities**\n",
    "\n",
    "Using the chain rule, the probability of a sentence:\n",
    "\n",
    "*P(w₁, w₂, …, wₜ) ~ P(w₁) × P(w₂|w₁) × P(w₃|w₁, w₂) × … P(wₜ|wₜ₋₁, …, wₜ₋ₙ₊₁)*\n",
    "\n",
    "**Example (bigram):**\n",
    "\n",
    "Sentence: P(\"I want to eat pizza\") = *P(I) × P(want|I) × P(to|want) × P(eat|to) × P(pizza|eat)*\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- Each conditional probability measures how \"natural\" that local phrase is\n",
    "- Multiplying them gives an overall fluency score for the whole sentence\n",
    "- High score → common, natural sentence\n",
    "- Very low score → unnatural or incorrect sentence\n",
    "\n",
    "*(This is the core job of a language model.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How n-gram Probabilities Add Value in Real Applications\n",
    "\n",
    "**n-gram LM = Fluency/Correctness Scorer**\n",
    "\n",
    "| Application | What Creates Candidates | What n-gram Adds | Result |\n",
    "|------------|------------------------|------------------|--------|\n",
    "| **Spell Checking** | String-similarity corrections (receive / revise / relieve) | Scores each candidate in sentence context | Picks correct word in context |\n",
    "| **Speech Recognition** | Acoustic model generates many possible word sequences | Scores each sequence by fluency | Picks the natural sentence, not just sound match |\n",
    "| **Machine Translation** | Translation model proposes literal or alternative translations | Scores target-language fluency | Picks most natural translation |\n",
    "| **Predictive Text** | All words in vocabulary | Predictive next-word probabilities | Produces useful next-word suggestions |\n",
    "\n",
    "**Key Insight**\n",
    "\n",
    "Across all tasks, n-gram LM is the component that ensures the output \"sounds like real language.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hidden Markov Models (HMMs)\n",
    "\n",
    "- **What**: Generative sequence models with hidden state chains emitting observed words or acoustic frames\n",
    "- **Key concept**: Transition matrix over tags/states + emission probabilities; decoded efficiently via Viterbi\n",
    "- **Used for**: POS tagging, speech recognition, shallow parsing\n",
    "- **Why it works**: Separates latent linguistic structure from surface tokens and models uncertainty end-to-end\n",
    "- **Builds on**: Adds probabilistic state transitions on top of n-gram assumptions and reuses linguistic tagsets from rule-based systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Hidden Markov Model Works\n",
    "\n",
    "HMM is concerned about two things: word sequence $x = (x_1, x_2, ..., x_T)$, and a hiden state sequence $s = (s_1, s_2, ..., s_T)$ representing some preperty of each word.\n",
    "\n",
    "The sentense probability is calculated by\n",
    "$$P(s, x) = P(s_1) \\cdot \\prod_{t=2}^{T} P(s_t \\mid s_{t-1}) \\cdot \\prod_{t=1}^{T} P(x_t \\mid s_t)$$\n",
    "\n",
    "Where\n",
    "\n",
    "- $P(s_1)$: probability of the starting state\n",
    "- $P(s_t \\mid s_{t-1})$: transition probability\n",
    "- $P(x_t \\mid s_t)$: emission probability  \n",
    "\n",
    "In training, transition probability and emission probability are calculated from the data.\n",
    "\n",
    "In application, those probabilities are used to calculate the probability of the candidate (s,x) to find the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of P(s, x) for POS Tagging\n",
    "\n",
    "**Sentence:** `The dog barks`  \n",
    "**Hidden states (POS):**  \n",
    "\\( s = (DT, NN, VBZ) \\)  \n",
    "**Observed words:**  \n",
    "\\( x = (The, dog, barks) \\)\n",
    "\n",
    "$$P(s, x) = P(DT) \\cdot P(NN \\mid DT) \\cdot P(VBZ \\mid NN) \\cdot P(The \\mid DT) \\cdot P(dog \\mid NN) \\cdot P(barks \\mid VBZ)$$\n",
    "\n",
    "**Meaning:**\n",
    "- Probability of starting in DT  \n",
    "- Probability that NN follows DT  \n",
    "- Probability that VBZ follows NN  \n",
    "- Probability each word is emitted by its tag  \n",
    "\n",
    "Multiply all terms → the joint probability of generating both the tag sequence and the sentence under the HMM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Maximum Entropy (Log-Linear) Models\n",
    "\n",
    "- **What**: Conditional probability models that weight arbitrary linguistic features to predict labels\n",
    "- **Key concept**: Choose weights that maximize entropy subject to empirical feature expectations (a flexible logistic regression)\n",
    "- **Used for**: Text classification, POS/NER taggers, feature-rich decision components in MT pipelines\n",
    "- **Why it works**: Combines many overlapping signals without independence assumptions; learns optimal weights from data\n",
    "- **Builds on**: Takes handcrafted linguistic cues from earlier systems but replaces manual rules with data-driven weight learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conditional Random Fields (CRFs)\n",
    "\n",
    "- **What**: Discriminative sequence models that directly learn \\(P(\\mathbf{y}\\mid\\mathbf{x})\\) over label sequences\n",
    "- **Key concept**: Log-linear factors over adjacent labels and rich features spanning the whole sentence; avoids label bias\n",
    "- **Used for**: NER, segmentation, POS, bioNLP entity/event extraction\n",
    "- **Why it works**: Supports arbitrary, overlapping features while enforcing global consistency across the sequence\n",
    "- **Builds on**: Extends Maximum Entropy to structured outputs and replaces HMM generative assumptions with conditional training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Math of a Linear-Chain Conditional Random Field (CRF)\n",
    "\n",
    "A CRF models the conditional probability of a label sequence **y** given the entire observation sequence **x**:\n",
    "\n",
    "$$P(y \\mid x) = \\frac{1}{Z(x)} \\exp\\left( \\sum_{t=1}^{T} \\sum_{k} \\lambda_k \\, f_k(y_{t-1}, y_t, x, t) \\right)$$\n",
    "\n",
    "**Components**\n",
    "\n",
    "- $f_k(\\cdot)$: feature functions  \n",
    "  (e.g., \"is the word capitalized?\", \"previous label = B-LOC?\")\n",
    "- $\\lambda_k$: learned weights for each feature\n",
    "- $Z(x)$: normalization term (ensures probabilities sum to 1)\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- The model scores a whole **label sequence** using rich features.\n",
    "- Then normalizes over *all possible label sequences*.\n",
    "- Training = learn weights $\\lambda_k$ that best fit data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of CRF for NER (Named Entity Recognition)\n",
    "\n",
    "**Sentence:**  `New York City is busy`\n",
    "\n",
    "**Labels (BIO scheme):**  `B-LOC   I-LOC   I-LOC   O   O`\n",
    "\n",
    "CRF uses **features across words and labels**:\n",
    "\n",
    "**Examples of features:**\n",
    "- Current word is capitalized?\n",
    "- Previous label = B-LOC?\n",
    "- Next word starts with capital?\n",
    "- Word shape (\"Xx\")\n",
    "- Suffix \"-ity\" etc.\n",
    "\n",
    "CRF score for the sequence: $\\text{score}(y, x) = \\sum_{t,k} \\lambda_k \\, f_k(y_{t-1}, y_t, x, t)$\n",
    "\n",
    "Prediction: $\\hat{y} = \\arg\\max_y P(y \\mid x)$\n",
    "\n",
    "CRF chooses the best entire sequence, not token-by-token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HMM vs CRF (Key Differences)\n",
    "\n",
    "| Aspect | Hidden Markov Model (HMM) | Conditional Random Field (CRF) |\n",
    "|-------|----------------------------|--------------------------------|\n",
    "| Model Type | Generative | Discriminative |\n",
    "| Assumption | Strong independence assumptions (word depends only on its tag) | No independence assumptions on features |\n",
    "| Features | Limited: emissions must be simple (word, maybe morphology) | Arbitrary, overlapping, global features allowed |\n",
    "| Sequence Consistency | Local (only via transitions) | Global (entire sequence optimized jointly) |\n",
    "| Typical Use | POS tagging (early), speech recognition (with acoustics) | NER, segmentation, chunking, POS tagging (modern) |\n",
    "| Strength | Simple, fast, usable with small data | Much more accurate, flexible, powerful |\n",
    "| Weakness | Cannot use rich features; weaker accuracy | Needs labeled data and more compute |\n",
    "\n",
    "**Summary**  \n",
    "- **HMM** models how words are *generated* from hidden states.  \n",
    "- **CRF** models how labels should be assigned *given the whole sentence*.  \n",
    "- CRFs outperform HMMs on most NLP sequence-labeling tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "\n",
    "**What it is**\n",
    "- Statistical method that discovers hidden semantic structure in text\n",
    "- Based on co-occurrence counts and **SVD (Singular Value Decomposition)**\n",
    "\n",
    "**Key idea**\n",
    "- Words that appear in similar contexts have similar meanings\n",
    "- Compress the word–document matrix into a low-dimensional semantic space\n",
    "\n",
    "**Math**\n",
    "1. Build term-document matrix (TF–IDF)\n",
    "2. Apply SVD:  M ≈ U Σ Vᵀ   (keep top k dimensions)\n",
    "3. Use vectors from U/V as semantic embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "\n",
    "**Used for**\n",
    "- Document similarity, clustering  \n",
    "- Topic modeling (pre-LDA era)  \n",
    "- Early summarization and search  \n",
    "- Foundation for word embeddings\n",
    "\n",
    "**Why it mattered**\n",
    "- First successful statistical method capturing semantic similarity  \n",
    "- Precursor to word2vec/GloVe  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "**What it is**\n",
    "- A **probabilistic topic model**\n",
    "- Assumes each document is a mixture of topics\n",
    "- Each topic is a distribution over words\n",
    "\n",
    "**Generative idea**\n",
    "1. For each document, choose topic proportions (Dirichlet).\n",
    "2. For each word:\n",
    "   - Pick a topic from the doc’s topic proportions\n",
    "   - Pick a word from that topic’s word distribution\n",
    "\n",
    "**Why it’s useful**\n",
    "- Discovers hidden themes in large text collections\n",
    "- Each doc gets a vector of topic weights → useful for clustering, search, classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applications Powered by Statistical NLP\n",
    "\n",
    "- Search engines → relevance ranking (TF–IDF, probabilistic IR)\n",
    "- Spam filtering → Naive Bayes, SVM\n",
    "- Speech recognition → HMM-based decoding\n",
    "- Machine translation → IBM statistical models\n",
    "- NER, POS tagging → HMM/CRF taggers\n",
    "- Sentiment analysis → supervised classification\n",
    "\n",
    "*(Scaled to real-world data for the first time.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Strengths of Statistical NLP\n",
    "\n",
    "- Learns from data → scalable\n",
    "- Handles messy real-world text\n",
    "- Manages ambiguity probabilistically\n",
    "- Reduces manual rule writing\n",
    "- Foundation for early web search & speech engines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Limitations → Motivation for Deep Learning\n",
    "\n",
    "- Heavy feature engineering required\n",
    "- Sparse data → poor generalization\n",
    "- Struggles with long-distance dependencies\n",
    "- Requires labeled training data\n",
    "- Words treated as symbols (no notion of meaning)\n",
    "\n",
    "*(Set the stage for word embeddings and neural networks.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Deep Learning Eliminates Sparsity (Word-Level & Above)\n",
    "\n",
    "**1. Word Embeddings → Fix sparsity above the word level**\n",
    "- Before: each word = isolated symbol (Paris, London, Berlin unrelated)  \n",
    "  → unseen word combinations = zero counts, brittle models  \n",
    "- After: dense word vectors  \n",
    "  → similar words cluster; models generalize to unseen phrases/sentences\n",
    "\n",
    "**2. Subword/Character Embeddings → Fix sparsity at the word level**\n",
    "- Before: OOV words = impossible to represent (probability = 0)  \n",
    "- After: BPE/WordPiece split words into pieces (un-, -tion, -ing…)  \n",
    "  → even unseen words have vectors; morphology becomes learnable\n",
    "\n",
    "**Result:**  \n",
    "Word embeddings handle semantic similarity, subwords handle OOV/morphology, and neural layers build contextual meaning → sparsity becomes a minor issue compared to statistical NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How Deep Learning Fixes “Words Treated as Symbols” (with definition of meaning)\n",
    "\n",
    "**What is “meaning” in NLP?**  \n",
    "- Meaning is relational: a word is defined by the contexts it appears in and its similarity to other words.\n",
    "\n",
    "**Before (Statistical NLP):**\n",
    "- Words were **IDs only**, no semantic relation (Paris ≠ London ≠ Berlin in the model).\n",
    "- Only **primitive meaning signals** existed (POS, morphology), which tell *role* but not *true meaning*.\n",
    "  - e.g., *dog* and *cat* both → noun (POS), but models cannot tell they are semantically close.\n",
    "\n",
    "**After (Deep Learning):**\n",
    "- **Word embeddings** learn relational meaning:\n",
    "  - Similar contexts → nearby vectors (king–man + woman ≈ queen).\n",
    "- **Contextual embeddings** give different meanings based on usage:\n",
    "  - “bank” (river) vs “bank” (finance).\n",
    "- **Subword embeddings** capture morphology and rare words:\n",
    "  - “bioluminescence” → bio + lumi + nescence.\n",
    "\n",
    "**Result:**  \n",
    "Deep learning replaces symbolic word IDs with **rich semantic, relational, context-dependent representations**, providing real “meaning” instead of shallow syntactic categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Word Embeddings and Deep Learning (2010s–2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Why We Need Word Embeddings\n",
    "\n",
    "**Statistical NLP limitations:**\n",
    "- Words treated as **IDs only** → no semantic similarity  \n",
    "- Sparse data → unseen combinations get **zero counts**  \n",
    "- Heavy reliance on **manual features** (suffixes, capitalization, POS)  \n",
    "- No way to capture relationships like:  \n",
    "  *Paris ≈ London ≈ Berlin*  \n",
    "  *run ≈ jog ≈ sprint*\n",
    "\n",
    "**Core problem:**  \n",
    "The model sees no relation between similar words — all are isolated symbols.\n",
    "\n",
    "**Motivation for embeddings:**  \n",
    "We need a **dense, learned, continuous representation** so that:\n",
    "- similar words are near each other,\n",
    "- rare/unseen words still get meaningful vectors,\n",
    "- models can generalize beyond observed counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide: subslide -->\n",
    "\n",
    "### What Is a Word Embedding?\n",
    "\n",
    "**Definition:**  \n",
    "A *word embedding* is a **dense vector** (typically 50–300 dimensions) that represents a word based on **its context and usage**.\n",
    "\n",
    "**Key properties:**\n",
    "- Similar words → **nearby vectors**  \n",
    "  - *Paris*, *London*, *Berlin* cluster together  \n",
    "  - *king – man + woman ≈ queen*\n",
    "- Encodes **semantic** and **syntactic** information  \n",
    "- Learned automatically from large corpora (distributional hypothesis)\n",
    "\n",
    "**Conceptual picture:**\n",
    "- One-hot:  \n",
    "  `[0 0 0 ... 1 ... 0]`  → no relationship\n",
    "- Embedding:  \n",
    "  `[0.39, -0.12, 0.85, ...]` → captures meaning via geometry\n",
    "\n",
    "**Why it matters:**  \n",
    "Embeddings transform language from **symbolic** to **semantic**, enabling neural networks to learn meaning instead of memorizing patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Timeline of Word Embeddings (2000–2017)\n",
    "\n",
    "**2000–2010 — Neural Language Models (early DL)**  \n",
    "- Bengio et al. (2003): neural LM with learned embeddings  \n",
    "- Too slow to scale, but introduced the core idea: **embedding matrix + neural prediction**\n",
    "\n",
    "**2013 — Word2Vec (Mikolov et al.)**  \n",
    "- Skip-Gram and CBOW  \n",
    "- Efficient training on billions of tokens  \n",
    "- Sparked the modern embedding revolution\n",
    "\n",
    "**2014 — GloVe (Pennington et al.)**  \n",
    "- Global co-occurrence matrix factorization  \n",
    "- Complement to window-based Word2Vec\n",
    "\n",
    "**2014–2016 — fastText (Facebook)**  \n",
    "- Subword/character n-gram embeddings  \n",
    "- Solved OOV and morphology limitations\n",
    "\n",
    "**2015–2017 — Contextual Precursor Models**  \n",
    "- ELMo (2018 precursor architecture)  \n",
    "- Bi-LSTM + embeddings → context-dependent meaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### How Word Embeddings Are Trained (Intuition)\n",
    "\n",
    "**Key idea: Distributional Hypothesis**  \n",
    "“You shall know a word by the company it keeps.”\n",
    "\n",
    "Words appearing in similar contexts → acquire similar vectors.\n",
    "\n",
    "Two major training styles:\n",
    "\n",
    "**1. Skip-Gram (Predict context given a word)**  \n",
    "- Input: “bank”  \n",
    "- Predict nearby words: *river, water, money, loan*  \n",
    "- Objective: words with similar neighborhoods → similar vectors\n",
    "\n",
    "**2. CBOW (Continuous Bag of Words — Predict the word from its context)**  \n",
    "- Input: “the *?* is flowing fast”  \n",
    "- Predict: *river*\n",
    "\n",
    "**Common idea:**  \n",
    "Train on massive corpora → embeddings emerge automatically from co-occurrence behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Skip-Gram (Window Size = 2)\n",
    "\n",
    "Sentence: **The quick brown fox jumps over the lazy dog**  \n",
    "Indexes:  \n",
    "1:The  2:quick  3:brown  4:fox  5:jumps  6:over  7:the  8:lazy  9:dog\n",
    "\n",
    "Window size = **2**\n",
    "\n",
    "#### Center = “fox” (position 4)\n",
    "Context positions = **2, 3, 5, 6**\n",
    "\n",
    "Training pairs:\n",
    "- (fox → quick)\n",
    "- (fox → brown)\n",
    "- (fox → jumps)\n",
    "- (fox → over)\n",
    "\n",
    "#### Center = “jumps” (position 5)\n",
    "Context positions = **3, 4, 6, 7**\n",
    "\n",
    "Training pairs:\n",
    "- (jumps → brown)\n",
    "- (jumps → fox)\n",
    "- (jumps → over)\n",
    "- (jumps → the)\n",
    "\n",
    "**Summary:**  \n",
    "Skip-Gram predicts **context words from the center word**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### CBOW (Window Size = 2)\n",
    "\n",
    "Sentence: **The quick brown fox jumps over the lazy dog**  \n",
    "Indexes:  \n",
    "1:The  2:quick  3:brown  4:fox  5:jumps  6:over  7:the  8:lazy  9:dog\n",
    "\n",
    "Window size = **2**\n",
    "\n",
    "#### Center = “fox” (position 4)\n",
    "Context positions = **2, 3, 5, 6**\n",
    "\n",
    "- Input: **{quick, brown, jumps, over}**  \n",
    "- Predict: **fox**\n",
    "\n",
    "#### Center = “jumps” (position 5)\n",
    "Context positions = **3, 4, 6, 7**\n",
    "\n",
    "- Input: **{brown, fox, over, the}**  \n",
    "- Predict: **jumps**\n",
    "\n",
    "**Summary:**  \n",
    "CBOW predicts the **center word from surrounding context words**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Why Word Embeddings Exploded Around 2013\n",
    "\n",
    "**1. Data got big enough**  \n",
    "- Web-scale corpora (billions of words) became available  \n",
    "- Finally enough signal to learn stable semantic patterns\n",
    "\n",
    "**2. Compute got fast enough**  \n",
    "- Multicore CPUs + GPUs made vector math cheap  \n",
    "- Training neural models became practical (hours, not weeks)\n",
    "\n",
    "**3. Algorithms became efficient**  \n",
    "- Word2Vec introduced **negative sampling** + **subsampling**  \n",
    "- Shallow SG/CBOW architectures scaled to huge vocabularies\n",
    "\n",
    "**Result:**  \n",
    "Embeddings became fast, scalable, and meaningful — enabling modern neural NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Why Word Embeddings Mattered (The Real Breakthrough)\n",
    "\n",
    "**Embeddings finally showed *true semantic structure*.**  \n",
    "This was the moment the NLP community realized neural methods worked.\n",
    "\n",
    "**Shocking results at the time:**\n",
    "- *king – man + woman ≈ queen*  \n",
    "- *Paris ≈ London ≈ Berlin*  \n",
    "- *walk, walking, walked* clustering naturally\n",
    "\n",
    "**What made this transformative:**\n",
    "- **Linear analogies** emerged from geometry  \n",
    "- **High-quality nearest neighbors** (semantic and syntactic)  \n",
    "- **Smooth, meaningful clusters** across massive vocabularies  \n",
    "\n",
    "Earlier methods (LSA, early neural LMs) hinted at similarity,  \n",
    "but **never at this clarity, scale, or consistency**.\n",
    "\n",
    "**Impact:**  \n",
    "This single discovery convinced researchers that  \n",
    "**dense vectors encode real meaning**, launching modern neural NLP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### GloVe: What It Adds on Top of Word2Vec\n",
    "\n",
    "**What Word2Vec uses:**  \n",
    "- Local context windows (Skip-Gram / CBOW)  \n",
    "- Learns meaning from *predicting* nearby words  \n",
    "- Great semantic clusters but only local statistics\n",
    "\n",
    "___\n",
    "\n",
    "### **What GloVe adds: Global Co-Occurrence Information**\n",
    "- Builds a **global word–word co-occurrence matrix**  \n",
    "- Learns embeddings by **factorizing** it with a weighted least-squares loss  \n",
    "- Embeddings capture **ratios** of probabilities  \n",
    "  - ice : cold  vs.  steam : hot  \n",
    "- Encodes semantic structure that **local windows miss**\n",
    "\n",
    "___\n",
    "\n",
    "### **How GloVe + Word2Vec connect**\n",
    "- Word2Vec = **local prediction**  \n",
    "- GloVe = **global co-occurrence factorization**  \n",
    "- Both produce dense vectors, but GloVe injects **global corpus structure** into the embedding space\n",
    "\n",
    "**Result:**  \n",
    "More stable embeddings and better analogy structure on some tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### fastText: What It Adds on Top of Word2Vec\n",
    "\n",
    "**What Word2Vec lacks:**  \n",
    "- Treats each word as a whole symbol  \n",
    "- Fails on **OOV** (out-of-vocabulary) words  \n",
    "- Weak on morphology (walk, walking, walker)\n",
    "\n",
    "___\n",
    "\n",
    "### **What fastText adds: Subword / Character n-grams**\n",
    "- Splits each word into character-level n-grams  \n",
    "  - “playing” → play, lay, yin, ing, pla…  \n",
    "- Embedding = **sum of subword embeddings**  \n",
    "- Even unseen words are representable (OOV solved)  \n",
    "- Morphology becomes learnable automatically\n",
    "\n",
    "___\n",
    "\n",
    "### **How fastText + Word2Vec connect**\n",
    "- fastText still uses **Skip-Gram / CBOW**  \n",
    "- But replaces “word as a single token” with  \n",
    "  **word = bag of character n-grams**\n",
    "- Keeps Word2Vec’s training idea  \n",
    "- Extends it to handle **rare words, morphology, OOV**\n",
    "\n",
    "**Result:**  \n",
    "Word2Vec semantics + subword structure = robust, generalizable embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Embedding Dimensions: Typical Sizes & What They Mean\n",
    "\n",
    "#### **Typical Dimensions**\n",
    "| Model       | Common Dim |\n",
    "|-------------|------------|\n",
    "| **Word2Vec** | 100–300 |\n",
    "| **GloVe**     | 50–300 |\n",
    "| **fastText**  | 100–300 |\n",
    "| **Contextual Models (BERT/GPT)** | 768–12k+ |\n",
    "\n",
    "**Why 100–300?**  \n",
    "- Big enough to encode rich semantic structure  \n",
    "- Small enough to train efficiently  \n",
    "- Empirically the best tradeoff for static embeddings\n",
    "\n",
    "___\n",
    "\n",
    "### **Do dimensions have actual meaning?**\n",
    "\n",
    "**Not individually**  \n",
    "- No single dimension corresponds cleanly to a topic or concept\n",
    "- You cannot point to dimension **#47** and say “this is the *animal* dimension”  \n",
    "\n",
    "**Think of it like a coordinate system:**  \n",
    "Individual axes don’t mean anything alone,  \n",
    "but **distances and directions** encode meaningful relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### From Embeddings to Neural Models (2013–2017)\n",
    "\n",
    "Once words are dense vectors, neural networks can finally **use meaning**, not symbols.\n",
    "\n",
    "**1. RNNs (2013–2014)**  \n",
    "- Read tokens sequentially  \n",
    "- Capture short-range patterns  \n",
    "- Weak long-distance memory\n",
    "\n",
    "**2. LSTMs / GRUs (2014–2016)**  \n",
    "- Add gating + memory  \n",
    "- Handle long dependencies  \n",
    "- Powered early neural MT, tagging, sentiment\n",
    "\n",
    "**3. CNNs for Text (2014–2016)**  \n",
    "- Convolution over embeddings  \n",
    "- Strong for sentiment & classification\n",
    "\n",
    "**Key impact:**  \n",
    "Embeddings + these neural models replaced feature engineering and enabled **end-to-end learned NLP**, paving the way for **attention (2017)** and **Transformers**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### What Is a Neural Network (NN)?\n",
    "\n",
    "A neural network is a model built from layers of simple mathematical units (“neurons”) that learn to transform inputs into useful outputs.\n",
    "\n",
    "**1. A neuron = weighted sum + nonlinearity**\n",
    "$$\n",
    "\\sigma(w \\cdot x + b)\n",
    "$$\n",
    "- $x$: input  \n",
    "- $w$: learned weights  \n",
    "- $\\sigma$: nonlinear activation (ReLU, tanh…)\n",
    "\n",
    "\n",
    "This is the basic building block.\n",
    "\n",
    "**2. A NN = many neurons arranged in layers**\n",
    "- Input layer receives embeddings or raw features  \n",
    "- Hidden layers learn internal patterns  \n",
    "- Output layer predicts the next word / label / score  \n",
    "- Stacking layers → learn complex relationships\n",
    "\n",
    "**3. NNs learn by adjusting weights**\n",
    "- Compare prediction vs. truth  \n",
    "- Compute error  \n",
    "- Update weights via gradient descent  \n",
    "- Repeat over millions of examples\n",
    "\n",
    "This allows the network to gradually discover patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "### Why Neural Networks Matter\n",
    "\n",
    "**✔ Learn features automatically**  \n",
    "- No manual feature engineering — useful patterns emerge from data.\n",
    "\n",
    "**✔ Capture nonlinear relationships**  \n",
    "- Language structure is nonlinear; NNs model complex interactions naturally.\n",
    "\n",
    "**✔ Scale with data**  \n",
    "- Larger datasets + bigger networks → better performance.\n",
    "\n",
    "___\n",
    "\n",
    "**Summary:**  \n",
    "A neural network is a layered function that learns to map inputs (like embeddings) to outputs by adjusting parameters to capture complex patterns in data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "## NN Language Modeling (Setup)\n",
    "\n",
    "We use the previous *N* words to predict the next one.\n",
    "\n",
    "Example (window = 5):\n",
    "\n",
    "The cat sat on the → mat\n",
    "\n",
    "\n",
    "**Flow:**\n",
    "\n",
    "1. **Token IDs → embeddings (300 dim)**  \n",
    "   Each token ID is looked up in an embedding matrix:  \n",
    "   `IDs → [e₁, e₂, e₃, e₄, e₅]`\n",
    "\n",
    "2. **Combine embeddings, 1500 dim**  \n",
    "   Usually concatenation:  \n",
    "   `[e₁ ; e₂ ; e₃ ; e₄ ; e₅]`\n",
    "\n",
    "3. **Feed into a neural network**  \n",
    "   One or more layers transform the window into a context representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- slide -->\n",
    "## NN Processing → Prediction\n",
    "\n",
    "Neural layers transform the input:\n",
    "\n",
    "$$h_1 = f_1(\\text{input})$$  \n",
    "$$h_2 = f_2(h_1) \\dots$$  \n",
    "$$h_{\\text{last}}$$  \n",
    "\n",
    "Final linear layer:\n",
    "\n",
    "$$\\text{logits} = W_{\\text{out}} \\cdot h_{\\text{last}} + b_{\\text{out}}$$\n",
    "\n",
    "Softmax gives probabilities:\n",
    "\n",
    "$$p(w) = \\text{softmax}(\\text{logits})$$\n",
    "\n",
    "**Output:** a probability distribution over the entire vocabulary  \n",
    "(e.g., all 50k possible next-word candidates).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "## Training With Cross-Entropy Loss\n",
    "\n",
    "If the true next word is **“mat”**, the loss is:\n",
    "\n",
    "$$\n",
    "L = -\\log p(\\text{\"mat\"})\n",
    "$$\n",
    "\n",
    "Backpropagation updates **all** parameters:\n",
    "\n",
    "- network layers  \n",
    "- output layer  \n",
    "- embedding matrix (learned like any other parameter)\n",
    "\n",
    "**Summary:**  \n",
    "A neural LM takes **token IDs → embeddings → neural layers → softmax**,  \n",
    "and learns to predict the next token using a **fixed context window**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "## What Is an RNN?\n",
    "\n",
    "A **Recurrent Neural Network (RNN)** processes a sequence **one token at a time**, keeping a running memory.\n",
    "\n",
    "At each step it uses  \n",
    "- current input **\\(x_t\\)**  \n",
    "- previous hidden state **\\(h_{t-1}\\)**  \n",
    "\n",
    "to compute the next hidden state:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1})\n",
    "$$\n",
    "\n",
    "**Meaning of components**\n",
    "- **\\(W_x\\)** — processes the *current token*\n",
    "- **\\(W_h\\)** — updates/retains *past information*\n",
    "- **\\(h_t\\)** — compressed summary of all tokens so far\n",
    "\n",
    "**Why it matters:**  \n",
    "RNNs create a **learned, persistent memory** rather than relying on a fixed window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "## How RNN Improves NNLM (Window-Based Models)\n",
    "\n",
    "Traditional NNLMs use a **fixed window** (e.g., last 5 tokens):\n",
    "\n",
    "- Only see limited context  \n",
    "- No memory beyond the window  \n",
    "- Cannot integrate long-distance patterns  \n",
    "\n",
    "**RNN removes the fixed window:**\n",
    "- Reads tokens sequentially  \n",
    "- Hidden state evolves over time:\n",
    "  $$\n",
    "  h_t \\rightarrow h_{t+1} \\rightarrow h_{t+2}\n",
    "  $$\n",
    "- Memory grows with sequence length\n",
    "\n",
    "**Advantages over NNLM:**\n",
    "- Learns *what* to remember  \n",
    "- Learns *how long* to remember it  \n",
    "- Captures dependencies across arbitrarily long sequences  \n",
    "- Far more expressive than fixed-window neural models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "## Why Vanilla RNNs Fail on Long Sequences\n",
    "\n",
    "**Core problem: vanishing gradients**\n",
    "- Training signal must pass through every time step.\n",
    "- At each step, gradients are multiplied by terms < 1.\n",
    "- Over long sequences, this product → **0**, so early information cannot influence learning.\n",
    "\n",
    "**Resulting limitations**\n",
    "- Cannot learn dependencies far in the past.\n",
    "- Forget subjects in long sentences.\n",
    "- Memory fades exponentially with sequence length.\n",
    "- Performs only slightly better than fixed-window NNLMs for long-range tasks.\n",
    "\n",
    "**Motivation for LSTM/GRU**\n",
    "- We need a recurrent model that can **decide what to keep**,  \n",
    "  **what to forget**, and **how long to remember** —  \n",
    "  without losing gradients over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "\n",
    "## LSTM Timeline — Why a 1997 Idea Took Over in 2014\n",
    "\n",
    "**1997 — LSTM Invented**  \n",
    "Hochreiter & Schmidhuber propose LSTM to address vanishing gradients by introducing memory cells and gating mechanisms.\n",
    "\n",
    "**2000s — Little Adoption**  \n",
    "LSTMs remain rare because hardware was too slow, datasets were small, optimization methods were immature, and training deep recurrent networks was difficult.\n",
    "\n",
    "**2013 — Conditions Become Right**  \n",
    "GPUs become powerful enough, large text corpora become available, Word2Vec produces high-quality embeddings, and improved optimizers such as Adam make training stable.\n",
    "\n",
    "**2014–2016 — Breakthrough Era**  \n",
    "LSTMs win across speech, handwriting, and early translation tasks; Seq2Seq models (Sutskever et al., 2014) push them into mainstream NLP.\n",
    "\n",
    "**2014 — GRU Introduced**  \n",
    "GRU offers a simpler gated architecture with performance similar to LSTM.\n",
    "\n",
    "**2017 — Transformers Replace RNNs**  \n",
    "Self-attention scales better than recurrence, and LSTM/GRU usage rapidly declines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"21cceeab741f.svg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "## LSTM Architecture — How It Works\n",
    "\n",
    "An LSTM keeps two states at each step:  \n",
    "- **hidden state** \\( h_t \\) — short-term signal  \n",
    "- **cell state** \\( C_t \\) — long-term memory  \n",
    "\n",
    "### 1. Forget gate — decide what to erase\n",
    "$$\n",
    "f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "### 2. Input gate — decide what new info to add\n",
    "$$\n",
    "i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "\n",
    "Candidate memory:\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "\n",
    "### 3. Update cell state — combine past + new\n",
    "$$\n",
    "C_t = f_t \\odot C_{t-1} \\;+\\; i_t \\odot \\tilde{C}_t\n",
    "$$\n",
    "\n",
    "### 4. Output gate — decide what to expose as hidden state\n",
    "$$\n",
    "o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "\n",
    "Hidden state:\n",
    "$$\n",
    "h_t = o_t \\odot \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "### Intuition\n",
    "- **Forget gate** removes irrelevant old memory.  \n",
    "- **Input gate + candidate** add useful new information.  \n",
    "- **Cell state** carries long-term memory with **minor gradient decay**.  \n",
    "- **Output gate** controls what part of the memory becomes the next hidden state.\n",
    "\n",
    "This gating structure solves **vanishing gradients** and allows LSTMs to remember information over long sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "## What RNNs / LSTMs / GRUs Solved — and What Still Remained\n",
    "\n",
    "### ✔ What They Solved\n",
    "- Learned **longer dependencies** than fixed-window NNLMs  \n",
    "- Added **memory** that adapts to the sequence  \n",
    "- Reduced **vanishing gradients** with gates (LSTM/GRU)  \n",
    "- Enabled early **neural machine translation**, tagging, speech  \n",
    "\n",
    "### ✘ What Still Remained (Major Limitations)\n",
    "- **Sequential processing is slow**  \n",
    "  RNNs read one token at a time → no parallelism.\n",
    "\n",
    "- **Hard to model very long-range dependencies**  \n",
    "  Even LSTMs eventually forget as sequence grows.\n",
    "\n",
    "- **Memory bottleneck**  \n",
    "  All information must pass through one hidden state \\(h_t\\).\n",
    "\n",
    "- **Difficulty capturing relationships between distant tokens**  \n",
    "  RNNs don’t selectively focus on important words.\n",
    "\n",
    "- **Training unstable / expensive**  \n",
    "  Long sequences → exploding/vanishing gradients → tricky optimization.\n",
    "\n",
    "___\n",
    "\n",
    "### Summary\n",
    "RNN/LSTM/GRU added memory and solved short-term dependency issues,  \n",
    "but **speed, parallelism, and truly long-range reasoning** were still limited —  \n",
    "*opening the door for attention and Transformers.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "## What NN / RNN / LSTM / GRU Solved — and What Still Remained\n",
    "\n",
    "### ✔ What They Solved\n",
    "\n",
    "**Neural Networks (NN)**\n",
    "- Learned **nonlinear features automatically**  \n",
    "- Removed the need for **manual feature engineering**  \n",
    "- Enabled **end-to-end learning** from embeddings  \n",
    "\n",
    "**Recurrent Neural Networks (RNN)**\n",
    "- Replaced fixed windows with **learned memory over sequences**  \n",
    "- Captured **short- and mid-range dependencies**  \n",
    "- Modeled sequences more naturally than NNLMs  \n",
    "\n",
    "**LSTM / GRU**\n",
    "- Added **gating mechanisms** → solved vanishing gradients  \n",
    "- Preserved **longer-term memory**  \n",
    "- Enabled early **neural MT, tagging, speech, handwriting**  \n",
    "- Much more stable to train than vanilla RNNs  \n",
    "\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ✘ What Still Remained (Major Limitations)\n",
    "\n",
    "- **Sequential processing is slow**  \n",
    "  RNNs must process tokens one-by-one → no parallelism.\n",
    "\n",
    "- **Still struggle with very long-range dependencies**  \n",
    "  Even LSTM/GRU memory fades as sequences grow.\n",
    "\n",
    "- **Single-state bottleneck**  \n",
    "  All information flows through a single hidden state \\(h_t\\).\n",
    "\n",
    "- **No selective focus**  \n",
    "  RNNs treat all past tokens uniformly; cannot highlight important ones.\n",
    "\n",
    "- **Training instability / cost**  \n",
    "  Long sequences → exploding/vanishing gradients → difficult optimization.\n",
    "\n",
    "___\n",
    "\n",
    "### Summary  \n",
    "NN → RNN → LSTM/GRU steadily improved feature learning, sequence modeling, and memory.  \n",
    "But **speed, parallelism, and truly long-range reasoning** remained unsolved —  \n",
    "creating the need for **attention** and ultimately **Transformers**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Transformers and the LLM Revolution (2018-today)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Self-Attention Mechanism — Intuition\n",
    "\n",
    "**Self-attention** lets each token look at *all other tokens* in the sequence and decide which ones matter.\n",
    "\n",
    "- Each token computes weights over all other tokens instead of just relying on a single hidden state.\n",
    "- The model builds a **contextual representation** by taking a weighted sum of other token representations.\n",
    "- Long-range dependencies (e.g., subject ↔ verb far apart) can be captured in **one step**, not gradually over time.\n",
    "\n",
    "**Contrast with RNNs:**\n",
    "\n",
    "- RNNs process tokens **sequentially** and compress everything into one hidden state.\n",
    "- Self-attention processes all tokens **in parallel**, removing the sequential bottleneck.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Query–Key–Value (QKV) Framework\n",
    "\n",
    "For each token’s current representation \\(x\\), the model learns three projections:\n",
    "\n",
    "- **Query (Q):** what this token is *looking for* in other tokens  \n",
    "- **Key (K):** what this token *offers* for others to match against  \n",
    "- **Value (V):** the *information* this token contributes when attended to  \n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "Q = W_Q x\n",
    "$$\n",
    "\n",
    "$$\n",
    "K = W_K x\n",
    "$$\n",
    "\n",
    "$$\n",
    "V = W_V x\n",
    "$$\n",
    "\n",
    "Attention score between token \\(i\\) and \\(j\\):\n",
    "\n",
    "$$\n",
    "\\text{score}(i,j) = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Attention output:\n",
    "\n",
    "$$\n",
    "\\text{output}_i = \\sum_j \\text{softmax}(\\text{score}(i,j)) \\cdot V_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformer Architecture\n",
    "\n",
    "- Encoder-decoder architecture\n",
    "- Positional encoding\n",
    "- Layer normalization and residual connections\n",
    "- Feed-forward networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"transformer.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Structure of a Single Transformer Block\n",
    "\n",
    "A Transformer block has **two sublayers**, both preserving the model dimension $(d_{\\text{model}})$\n",
    "\n",
    "**1. Multi-Head Self-Attention (MHSA)**  \n",
    "Input:\n",
    "$$\n",
    "X \\in \\mathbb{R}^{T \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "Linear projections:\n",
    "$$\n",
    "Q = X W_Q,\\quad K = X W_K,\\quad V = X W_V\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times (h \\cdot d_k)}\n",
    "$$\n",
    "\n",
    "Heads: split into \\(h\\) heads, each of size\n",
    "$$\n",
    "d_k = \\frac{d_{\\text{model}}}{h}\n",
    "$$\n",
    "\n",
    "Concatenate heads and project back:\n",
    "$$\n",
    "W_O \\in \\mathbb{R}^{(h \\cdot d_k) \\times d_{\\text{model}}}\n",
    "$$\n",
    "\n",
    "**Output shape:** $(T \\times d_{\\text{model}}$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**2. Feed-Forward Network (FFN)**  \n",
    "Two-layer MLP applied per token:\n",
    "$$\n",
    "d_{\\text{model}} \\;\\rightarrow\\; d_{\\text{ff}} \\;\\rightarrow\\; d_{\\text{model}}\n",
    "$$\n",
    "\n",
    "Typical:\n",
    "$$\n",
    "d_{\\text{ff}} = 4\\, d_{\\text{model}}\n",
    "$$\n",
    "\n",
    "___\n",
    "\n",
    "**3. Residual + LayerNorm**  \n",
    "Applied around both sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What One Transformer Block Can Learn\n",
    "\n",
    "A single block (MHSA + FFN) gives **one round of contextualization**:\n",
    "\n",
    "- Learns **local context**: pronoun–noun links, adjective–noun, negation, short-range dependencies  \n",
    "- Forms **attention patterns**: heads focusing on punctuation, capitalization, subjects, nearby important tokens  \n",
    "- Captures **shallow structure**: basic roles (noun/verb), phrase boundaries, simple entity signals  \n",
    "- FFN adds **nonlinear refinement**: polarity, simple logic (“A but B”)  \n",
    "- Enables **basic next-token prediction**: common continuations, grammar, punctuation  \n",
    "\n",
    "**Cannot:** handle long-range dependencies, multi-step reasoning, deep semantics, or world knowledge.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hi-Level Summary of Modern LLM Structure\n",
    "\n",
    "Modern LLMs (GPT-3/4/5, LLaMA-3, Mistral, Gemini) share the same core design:\n",
    "\n",
    "**1. Embeddings**\n",
    "- Token embeddings of size $d_{\\text{model}}$\n",
    "- Add positional info (learned or RoPE)\n",
    "- Output: $T \\times d_{\\text{model}}$\n",
    "\n",
    "**2. Repeated Transformer Blocks (12–120+)**\n",
    "- **Multi-Head Self-Attention (MHSA):** mixes information across all tokens  \n",
    "- **Feed-Forward Network (FFN):** $d_{\\text{model}} \\rightarrow d_{\\text{ff}} \\rightarrow d_{\\text{model}}$  \n",
    "- **Residual + LayerNorm:** stabilize training, keep dimensions fixed  \n",
    "- Shape preserved: $T \\times d_{\\text{model}}$ throughout\n",
    "\n",
    "**3. Final LayerNorm + LM Head**\n",
    "- Project hidden states to vocabulary\n",
    "- Softmax → next-token probabilities\n",
    "\n",
    "**Modern Additions:** RoPE, gated FFNs, long-context attention, sparse/MoE routing, multimodality.\n",
    "\n",
    "**Core Idea:** LLMs = deep stacks of identical Transformer blocks trained on massive data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 🧠 LLM Evolution Timeline \n",
    "\n",
    "| Model (Large Version)     | Release Date | d_model | Heads | Layers | FFN Dim | Params | Notes |\n",
    "|---------------------------|--------------|---------|--------|---------|----------|---------|-------|\n",
    "| **Transformer Big**       | Jun 2017     | 1024    | 16     | 6 enc / 6 dec | 4096 | ~213M | Original Transformer (Attention Is All You Need) |\n",
    "| **BERT Large**            | Oct 2018     | 1024    | 16     | 24 enc | 4096     | 340M  | Bidirectional encoder-only model |\n",
    "| **GPT-2 XL**              | Feb 2019     | 1600    | 25     | 48     | 6400     | 1.5B  | Largest GPT-2 model |\n",
    "| **GPT-3 (175B)**          | Jun 2020     | 12288   | 96     | 96     | 49152    | 175B  | Major scaling breakthrough |\n",
    "| **GPT-3.5**               | Nov 2022     | ~12288  | ~96    | ~96    | ~49k     | 175–180B | Powered ChatGPT v1 |\n",
    "| **GPT-4** (closed)        | Mar 2023     | ~15k–20k | ~120  | ~120   | ~60k–80k | ~500B–1T est. | MoE; ~220B active |\n",
    "| **LLaMA-3 (70B)**         | Apr 2024     | ~11k    | ~88    | 80     | ~43k     | 70B   | Meta’s 2024 flagship |\n",
    "| **Mistral Large**         | early 2024   | ~8192   | ~64    | ~80    | ~28k     | ~80B  | High-efficiency dense model |\n",
    "| **Gemini 2.0 Pro**        | Dec 2024     | —       | —      | —      | —        | >20B (MoE) | Sparse MoE; partial activation |\n",
    "| **Gemini 2.5 Pro**        | Feb 2025     | —       | —      | —      | —        | unknown | Long-context, multimodal |\n",
    "| **GPT-5**                 | 2025 | —       | —      | —      | —        | unknown | Likely MoE, > GPT-4 |\n",
    "| **Claude 3.7**            | 2025         | —       | —      | —      | —        | unknown | NeMo architecture |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 📚 How LLMs Learn — Sources of Training Data\n",
    "\n",
    "| Source Type | Examples | Approx. Scale | Purpose in Training | Quality / Notes |\n",
    "|-------------|----------|----------------|----------------------|------------------|\n",
    "| **Reference & Knowledge Bases** | Wikipedia, Gutenberg, open-access research (arXiv), PubMed, government docs | **10–100B words** (≈ 15–130B tokens) | Build factual grounding, structured knowledge, reasoning | 🟢 High quality |\n",
    "| **Modern Writing & Media** | News articles (Reuters, BBC), educational content, technical docs | **100–300B words** (≈ 150–400B tokens) | Teach current language, tone, domain context | 🟢 High quality (partly licensed) |\n",
    "| **Web & Conversational Data** | Common Crawl, Reddit, Stack Overflow (licensed), blogs, forums | **1–3T words** (≈ 1.3–4T tokens) | Capture natural phrasing, diversity, informal usage | 🟡 Mixed quality; heavily filtered |\n",
    "| **Code & Technical Corpora** | GitHub open-source code, API docs, comments | **100M+ repos** (≈ 20–80B lines of code) | Teach logic, structure, problem-solving | 🟢 Reliable when curated |\n",
    "| **Licensed & Curated Sets** | Paid news, textbooks, alignment datasets (RLHF) | **10–100B words** (≈ 15–130B tokens) | Improve factual reliability & human intent alignment | 🟢 High quality; proprietary |\n",
    "| **Excluded / Not Used** | Private data, paywalled content, nondigitized archives | — | — | 🔴 Excluded due to privacy/copyright |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What Large-Scale LLMs Solve (vs. Earlier DL Models)\n",
    "\n",
    "**1. Long-Range Reasoning**\n",
    "- Attention over *entire sequences* (not limited memory)\n",
    "- Handles multi-sentence context, documents, instructions\n",
    "\n",
    "**2. Rich Knowledge & Generalization**\n",
    "- Trained on trillion-token corpora → broad world knowledge  \n",
    "- Learns patterns not present in earlier small-data models\n",
    "\n",
    "**3. Compositional & Multi-Step Reasoning**\n",
    "- Chain-of-thought, planning, tool use, multi-hop logic  \n",
    "- Earlier models handled only local, shallow correlations\n",
    "\n",
    "**4. Robustness & Versatility**\n",
    "- Works across tasks (QA, code, translation, dialogue) without task-specific training\n",
    "- Earlier models needed separate architectures per task\n",
    "\n",
    "**5. Emergent Abilities from Scale**\n",
    "- In-context learning, few-shot prompting, instruction following  \n",
    "- Not present in RNN/LSTM or small Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 🧩 Traditional NLP Pre-Processing (Before Deep Learning)\n",
    "\n",
    "**Goal:** Clean and normalize text before feature extraction.\n",
    "\n",
    "**1. Tokenization**\n",
    "- Rule-based splitting (whitespace, punctuation)\n",
    "- Produces whole-word tokens\n",
    "\n",
    "**2. Normalization**\n",
    "- Lowercasing\n",
    "- Remove punctuation / stopwords\n",
    "- Handle numbers, dates, URLs\n",
    "\n",
    "**3. Stemming**\n",
    "- Rule-based suffix stripping  \n",
    "  *(running → run, studies → studi)*\n",
    "- Fast but crude; loses grammatical meaning\n",
    "\n",
    "**4. Lemmatization**\n",
    "- Dictionary-based canonical form  \n",
    "  *(better → good, mice → mouse)*\n",
    "- More accurate but slower\n",
    "\n",
    "**5. Why so much preprocessing?**\n",
    "- Traditional models used **bags-of-words**, **n-grams**, **TF–IDF**  \n",
    "- Needed clean, consistent token forms to reduce sparsity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 🔡 Tokenization in Modern LLMs (Deep Learning Era)\n",
    "\n",
    "**Goal:** Convert text → model-friendly tokens while preserving meaning.\n",
    "\n",
    "**1. Subword Tokenization (dominant today)**\n",
    "- Algorithms: **BPE**, **WordPiece**, **SentencePiece**\n",
    "- Break rare/complex words into meaningful chunks  \n",
    "  *“bioluminescence” → bio + lumi + nescence*\n",
    "- Keeps vocabulary small (≈ 30k–50k tokens)\n",
    "\n",
    "**2. Minimal Normalization**\n",
    "- No stemming or lemmatization  \n",
    "- Casing, morphology, punctuation kept intact  \n",
    "- Models learn meaning directly from data\n",
    "\n",
    "**3. Why subwords?**\n",
    "- Handles rare words and typos  \n",
    "- Preserves semantic structure  \n",
    "- Enables open vocabulary\n",
    "\n",
    "**Key shift:**  \n",
    "Traditional NLP relied on heavy text cleaning.  \n",
    "LLMs rely on **subword tokenization + large training data**, and learn everything else in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<!-- slide -->\n",
    "### 🔣 How LLM Tokenizers Handle Punctuation & Special Characters\n",
    "\n",
    "**Modern LLM tokenizers (BPE / WordPiece / SentencePiece):**\n",
    "- Treat punctuation as **meaningful tokens**, not noise  \n",
    "- Keep characters like `, . ! ? ; : - ' \"` as **separate subword units**\n",
    "- Handle emojis, math symbols, and Unicode via **dedicated tokens**\n",
    "- Preserve case, punctuation, and spacing → model learns patterns directly\n",
    "\n",
    "**Examples**\n",
    "- `\"Hello!\"` → `[\"Hello\", \"!\"]`\n",
    "- `\"don't\"` → `[\"don\", \"'\", \"t\"]` (subword splits)\n",
    "- `\"C++\"` → `[\"C\", \"+\", \"+\"]`\n",
    "- Emojis → single token: `\"😊\"`  \n",
    "- Mixed scripts → tokenizer keeps Unicode blocks intact\n",
    "\n",
    "**Why this matters**\n",
    "- Preserves semantics (e.g., **“not good”**, **quotes**, **exclamation emphasis**)  \n",
    "- Reduces vocabulary size while still handling rare forms  \n",
    "- Lets the model learn punctuation-based structure: sentence boundaries, lists, emphasis, dialogue\n",
    "\n",
    "**Key idea:**  \n",
    "LLMs *do not strip or normalize punctuation* — they **learn its function** directly from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Modern LLM Applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Core NLP Tasks\n",
    "\n",
    "\n",
    "| Task | Impact of LLMs | Simple Reason |\n",
    "|------|----------------|----------------|\n",
    "| **Summarization** | **Revolutionary** | First time models can *truly rewrite* text with abstraction, coherence, and long-range understanding. |\n",
    "| **Question Answering** | **Revolutionary** | Moves from keyword lookup → *actual comprehension and reasoning* across sentences/documents. |\n",
    "| **Sentiment Analysis** | **Large improvement, not revolutionary** | Much better at nuance and sarcasm, but the task was already well-solved with older methods. |\n",
    "| **NER** | **Large improvement, not revolutionary** | More robust and contextual, but earlier CRF/BiLSTM systems already handled core patterns well. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advanced Applications\n",
    "\n",
    "| Application | What It Is | Key Capabilities |\n",
    "|------------|------------|------------------|\n",
    "| **AI Agents** | Autonomous LLM-driven systems | Planning, tool use, API calls, workflow automation |\n",
    "| **Conversational AI** | Dialogue systems & chatbots | Multi-turn reasoning, task chat, interactive support |\n",
    "| **Code Generation** | Natural language → executable code | Autocomplete, debugging, refactoring, API usage |\n",
    "| **Multimodal Systems** | Models using text + images/audio/video | Captioning, OCR, visual reasoning, grounded understanding |\n",
    "\n",
    "**Impact:**  \n",
    "These application ideas aren’t new — but **LLMs transformed them into reality**, delivering **human-level fluency**, **reasoning**, and **cross-task versatility** at **real-world scale** for the first time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### What It Is\n",
    "RAG combines **retrieval** (finding relevant external information) with **generation** (LLM reasoning and synthesis) to produce accurate, grounded answers.\n",
    "\n",
    "### How It Works\n",
    "1. **Query → Embedding**  \n",
    "   Convert the user query into a vector.\n",
    "2. **Vector Search**  \n",
    "   Retrieve top-k relevant documents from a knowledge base or vector store.\n",
    "3. **Augment Context**  \n",
    "   Insert retrieved text into the LLM prompt.\n",
    "4. **Generate Answer**  \n",
    "   LLM synthesizes a response grounded in retrieved evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why It Matters\n",
    "- Provides **fresh, domain-specific knowledge** not in training data  \n",
    "- **Reduces hallucinations** by anchoring output in retrieved text  \n",
    "- Scales with **your documents**, not model size  \n",
    "- Knowledge can be **updated instantly** without retraining  \n",
    "\n",
    "### Practical Applications\n",
    "- Enterprise QA over docs, wikis, RCAs, Slack  \n",
    "- Customer support using manuals and troubleshooting guides  \n",
    "- Semantic search assistants for large corpora  \n",
    "- Code assistants retrieving APIs and examples  \n",
    "- Research tools surfacing papers and reports  \n",
    "\n",
    "**Core Idea:**  \n",
    "RAG = **retrieval for facts** + **LLM for reasoning**, producing responses that are accurate, grounded, and up-to-date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AI agent\n",
    "\n",
    "<img src=\"agent-overview.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How an AI Agent Behaves Like a Human\n",
    "\n",
    "The LLM's ability to **understand and generate natural language** enables AI agent to mirror the way a human thinks and acts:\n",
    "\n",
    "1. **Brain = LLM (Central Reasoning Core)**  \n",
    "   The large language model functions like the agent’s *brain* — it interprets user input, understands intentions, and decides what to do next.\n",
    "\n",
    "2. **Perception → Understanding Intent**  \n",
    "   When the user provides an instruction, the LLM processes the text the way a person listens:  \n",
    "   - extracts meaning  \n",
    "   - infers goals  \n",
    "   - identifies missing information  \n",
    "   - forms an initial plan  \n",
    "\n",
    "3. **Planning → Figuring Out What’s Needed**  \n",
    "   The “brain” determines:  \n",
    "   - What info do I already have?  \n",
    "   - What info do I need to gather?  \n",
    "   - What tools can help me?  \n",
    "   - What steps should I take next?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4. **Tools = Human Abilities (Memory, Search, Calculator, APIs)**  \n",
    "   Just as humans use notebooks, search engines, calendars, or calculators, the AI agent uses external tools:  \n",
    "   - search  \n",
    "   - code interpreter  \n",
    "   - calendar/api calls  \n",
    "   - knowledge retrieval  \n",
    "   - long-term memory  \n",
    "\n",
    "5. **Reasoning Loop (Perception → Action → Reflection)**  \n",
    "   The agent repeatedly:  \n",
    "   - retrieves info using tools  \n",
    "   - returns it to the “brain”  \n",
    "   - re-evaluates what it knows  \n",
    "   - refines the plan  \n",
    "   This loop continues until the agent feels it has enough information.\n",
    "\n",
    "6. **Decision + Action**  \n",
    "   Once ready, the agent:  \n",
    "   - **responds to the user**, *or*  \n",
    "   - **executes actions** using tools (run code, fetch data, update a system, etc.)\n",
    "\n",
    "**Core idea:**  \n",
    "An AI agent behaves like a human problem-solver — the LLM acts as the brain, tools act as abilities, and the agent cycles through understanding, planning, gathering information, and acting until the task is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?\n",
    "\n",
    "___\n",
    "\n",
    "Thank you!\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "nbconvert": {
   "custom_css": "<style>\n@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');\n\n/* Apply Inter font and light background to all slides (for reveal.js / nbconvert) */\n.reveal {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n  background-color: #f5f5f5;\n}\n\n.reveal .slides {\n  background-color: #f5f5f5;\n}\n\n.reveal .slides section {\n  background-color: #fafafa;\n  color: #333333;\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n}\n\n.reveal h1,\n.reveal h2,\n.reveal h3,\n.reveal h4,\n.reveal h5,\n.reveal h6 {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n  font-weight: 600;\n  color: #1a1a1a;\n}\n\n.reveal p,\n.reveal li,\n.reveal td,\n.reveal th {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n  color: #333333;\n}\n\n/* Ensure code blocks maintain readability */\n.reveal pre,\n.reveal code {\n  font-family: 'Fira Code', 'Consolas', 'Monaco', 'Courier New', monospace;\n  background-color: #ffffff;\n}\n\n/* Table styling */\n.reveal table {\n  background-color: #ffffff;\n  border-collapse: collapse;\n}\n\n.reveal table th {\n  background-color: #e8e8e8;\n  color: #1a1a1a;\n}\n\n.reveal table td {\n  background-color: #ffffff;\n}\n\n/* Ensure proper contrast for links */\n.reveal a {\n  color: #0066cc;\n}\n\n.reveal a:hover {\n  color: #0052a3;\n}\n\n/* Additional styling for RISE slideshow */\n.rise-enabled .reveal {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n  background-color: #f5f5f5;\n}\n\n.rise-enabled .reveal .slides section {\n  background-color: #fafafa;\n  color: #333333;\n}\n\n/* For notebook cell rendering (non-slideshow view) */\n.jp-RenderedHTMLCommon {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n}\n\n.jp-RenderedMarkdown {\n  font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;\n}\n</style>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
