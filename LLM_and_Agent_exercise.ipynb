{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096601b3",
   "metadata": {},
   "source": [
    "# NLP Exercise‚Äî LLMs, Prompts, and Agents \n",
    "\n",
    "**Hands-on goals (1.5h):**\n",
    "\n",
    "1. Interact with LLM\n",
    "\n",
    "2. Basic chatbot\n",
    "\n",
    "3. LLM capabilities and limitations\n",
    "\n",
    "4. Prompt engineering\n",
    "\n",
    "5. Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75e4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "## Silence the annoying warnings\n",
    "# 1) Python warnings (UserWarning, DeprecationWarning, etc.)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2) gRPC native logs (ALTS, channelz, etc.)\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\"\n",
    "os.environ[\"GRPC_TRACE\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d233f",
   "metadata": {},
   "source": [
    "# 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeee7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt --quiet\n",
    "# !pip uninstall jupyterlab --yes --quiet\n",
    "# !pip install jupyterlab==3.6.8  --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0029887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# We'll use LangChain + LangGraph for the agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Make sure GOOGLE_API_KEY is set in .env file or environment\n",
    "# api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "# assert api_key, \"Please set GOOGLE_API_KEY in your .env file or environment.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d69f2-0dfd-43be-9549-9ba7aa409131",
   "metadata": {},
   "source": [
    "# 2. Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d9a2f7-1704-40fd-b6e3-1aa3d33315c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# initiate the LLM model, use model=\"gemini-2.5-flash\" as example\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "def safe_llm_invoke(messages, max_retries=5, base_delay=1.0):\n",
    "    \"\"\"Invoke LLM with retries on transient errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = llm.invoke(messages)\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            # Last attempt ‚Üí re-raise\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            # Exponential backoff\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "#             print(f\"Retry {attempt+1}/{max_retries} after error: {e}\")\n",
    "            time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94612b7-3988-414a-b98c-4663e94dd08c",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "Ask LLM standalone questions.  \n",
    "For example: \n",
    "- safe_llm_invoke('who are you?')\n",
    "- safe_llm_invoke('what is your capability?')\n",
    "- safe_llm_invoke('what is the previous question we discussed?')\n",
    "  \n",
    "There is no memory, so LLM doesn't know what has been discussed and you can't ask follow up questions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d005f814-525c-4fb3-b717-da69cfa81113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p style='margin:2px 0'>As an AI, I don't have memory of past conversations. Each interaction is treated as a new one.\n",
       "\n",
       "Could you please remind me what we were discussing?</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp = safe_llm_invoke('what is the previous question we discussed?')\n",
    "display(Markdown(f\"<p style='margin:2px 0'>{resp.content}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688e018-d498-459b-94eb-addfa393467c",
   "metadata": {},
   "source": [
    "# 3. Simple Chatbot\n",
    "Save the conversation to history, pass the whole history to LLM, now you have a **chatbot**!\n",
    "\n",
    "### Sample questions:\n",
    "- Hi\n",
    "- Who are you?\n",
    "- What is your capability and limitation?\n",
    "- Answer it within 200 words\n",
    "\n",
    "### Caution\n",
    "If you can't type o as it will fold/unfold the result area, try **capital O**, LLM don't mind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe2d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4528b1d7d6f64804ab0d2fbe82639d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid #ddd', border_left='1px solid #ddd', border_right='1px solid #dd‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2346a46e899c4680b158952a39958d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='100%'), placeholder='Type your message and press Enter‚Ä¶')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# STREAMING CHAT UI\n",
    "###############################################################################\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import markdown\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LLM SETUP\n",
    "# ---------------------------------------------------------------------------\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# SAFE STREAM (with retry + exponential backoff)\n",
    "# ---------------------------------------------------------------------------\n",
    "def safe_stream(messages, max_retries=5, base_delay=1.0):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return llm.stream(messages)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "#             print(f\"[Retry {attempt+1}/{max_retries}] {e}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# UI COMPONENTS\n",
    "# ---------------------------------------------------------------------------\n",
    "output_area = widgets.Output(\n",
    "    layout={\n",
    "        \"border\": \"1px solid #ddd\",\n",
    "        \"height\": \"300px\",\n",
    "        \"width\": \"100%\",\n",
    "        \"overflow_y\": \"auto\",\n",
    "        \"padding\": \"10px\"\n",
    "    }\n",
    ")\n",
    "\n",
    "input_box = widgets.Text(\n",
    "    placeholder=\"Type your message and press Enter‚Ä¶\",\n",
    "    layout={'width': '100%'}\n",
    ")\n",
    "\n",
    "history = []\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HANDLER: triggered when user presses Enter\n",
    "# ---------------------------------------------------------------------------\n",
    "def on_send(text_widget):\n",
    "\n",
    "    text = text_widget.value\n",
    "    if not text:\n",
    "        return\n",
    "\n",
    "    text_widget.value = \"\"   # clear UI input\n",
    "\n",
    "    # Show user's message\n",
    "    with output_area:\n",
    "        display(Markdown(\n",
    "            f\"<div style='margin:0; padding:0; line-height:1.0;'>&nbsp;&nbsp;<b>You:</b> {text}</div>\"\n",
    "        ))\n",
    "\n",
    "    history.append(HumanMessage(content=text))\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # STREAM CHATBOT RESPONSE (Markdown live update)\n",
    "    # -----------------------------------------------------------------------\n",
    "    with output_area:\n",
    "        # Initial header\n",
    "        bubble = widgets.HTML(\n",
    "            value=\"<b>Chatbot:</b> \",\n",
    "            layout={\"overflow_y\": \"auto\", \"max_height\": \"300px\", \"padding\": \"6px\"}\n",
    "        )\n",
    "        display(bubble)\n",
    "\n",
    "    chunks = safe_stream(history)\n",
    "    full = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        token = chunk.content or \"\"\n",
    "        full += token\n",
    "        html_content = markdown.markdown(full)\n",
    "        # force inline display for paragraph output\n",
    "        html_content = html_content.replace(\"<p>\", \"<span>\").replace(\"</p>\", \"</span>\")\n",
    "\n",
    "\n",
    "    # Update bubble with HTML-rendered content\n",
    "        bubble.value = (\n",
    "            \"<b>Chatbot:</b>\"\n",
    "            \"<div style='margin-left:40px'>\"\n",
    "            f\"<div style='line-height:1.2; margin:0; padding:0'>\"\n",
    "            f\"{html_content}\"\n",
    "            \"</div>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "\n",
    "    history.append(AIMessage(content=full))\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CONNECT ENTER KEY TO HANDLER\n",
    "# ---------------------------------------------------------------------------\n",
    "input_box.on_submit(on_send)\n",
    "\n",
    "# Show Chat UI\n",
    "display(output_area)\n",
    "display(input_box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974b83b-1459-4e87-b3a7-28362d87f2bc",
   "metadata": {},
   "source": [
    "#### You can inspect what gets saved in history.\n",
    "All the history is passed to `ChatGoogleGenerativeAI`, but only the actual\n",
    "`content` strings are sent to the LLM.\n",
    "\n",
    "LangChain converts messages into Gemini‚Äôs expected schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"contents\": [\n",
    "    {\"role\": \"user\",  \"parts\": [{\"text\": \"User text...\"}]},\n",
    "    {\"role\": \"model\", \"parts\": [{\"text\": \"Model reply...\"}]},\n",
    "    {\"role\": \"user\",  \"parts\": [{\"text\": \"Next user message...\"}]}\n",
    "  ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b45fe38-bd3e-4603-9dfe-8648df60f32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={}))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[0], history[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1a36f",
   "metadata": {},
   "source": [
    "# 4. Test LLM capability and limitation\n",
    "- Does it know the current time? It only has knowledge from the data it is trained on. So no up to date information\n",
    "  \n",
    "- Test it on a domain you know well, how does the LLM perform?\n",
    "- What is my name? Where am I located? LLM has no private memory\n",
    "- try ‚ÄúSummarize the book ‚ÄòThe Yellow Star Algorithm‚Äô.‚Äù (It doesn‚Äôt exist.) LLM may hallucinate and make up things. It is a statistical model, has no sense of right or wrong, will generate output for whatever input based on its algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6b4b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chatbot:** Hi there! How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: q\n"
     ]
    }
   ],
   "source": [
    "# type \"exit\", \"quit\" or \"q\" to quit\n",
    "def chat():\n",
    "    history = []\n",
    "    while True:\n",
    "        user = input(\"You: \")\n",
    "        if user.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            break\n",
    "\n",
    "        history.append(HumanMessage(content=user))\n",
    "        resp = safe_llm_invoke(history)\n",
    "        display(Markdown(f\"**Chatbot:** {resp.content}\"))\n",
    "        history.append(resp)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d46aae",
   "metadata": {},
   "source": [
    "# 5 Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcbde88",
   "metadata": {},
   "source": [
    "## 1. Prompt Engineering Basics: \n",
    "\n",
    "In this section:\n",
    "\n",
    "- Call the LLM directly.\n",
    "\n",
    "- Change **role** and **style** etc.\n",
    "\n",
    "- See how answers differ for the same question.\n",
    "\n",
    "**Things you can instruct in the system prompt:**\n",
    "1. Persona ‚Äî Who the model acts as (e.g., analyst, teacher, engineer).\n",
    "2. Style ‚Äî How the response is written (plain, formal, concise, narrative).\n",
    "3. Tone ‚Äî Emotional flavor (friendly, neutral, strict, encouraging).\n",
    "4. Reasoning Steps ‚Äî How the model thinks before answering (identify factors, compare options, summarize, etc.).\n",
    "5. Output Format ‚Äî The required shape of the answer (bullets, JSON, table, short paragraph).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b54fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "def chat(system_promt=None):\n",
    "    if system_promt is None:\n",
    "        system_promt='''\n",
    "        '''\n",
    "    history = [\n",
    "        SystemMessage(content=system_promt)\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        user = input(\"You: \")\n",
    "        if user.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            break\n",
    "\n",
    "        history.append(HumanMessage(content=user))\n",
    "        resp = safe_llm_invoke(history)\n",
    "        display(Markdown(f\"**Chatbot:** {resp.content}\"))\n",
    "        history.append(resp)\n",
    "prompts={\n",
    "    'finalcial_analyst': '''\n",
    "        You are a senior financial analyst. Use a structured format for the response. \n",
    "        Prioritize accuracy, highlight main risks and drivers, \n",
    "        and provide short context when needed. \n",
    "        Avoid speculation and avoid long paragraphs.\n",
    "    ''',\n",
    "    'teacher': '''\n",
    "        You are a high school teacher, answer question patiently and nicely. \n",
    "        Encourage student to ask follow up questions.\n",
    "    '''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b08f27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chatbot:** Hi there! üëã It's great to hear from you.\n",
       "\n",
       "How can I help you today? Do you have a question about a specific subject, a homework problem, or just want to chat about something you're learning?\n",
       "\n",
       "Don't hesitate to ask anything at all! üòä"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: q\n"
     ]
    }
   ],
   "source": [
    "chat(system_promt=prompts['teacher'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ad25268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: q\n"
     ]
    }
   ],
   "source": [
    "chat(system_promt=prompts['finalcial_analyst'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bbd71",
   "metadata": {},
   "source": [
    "### üëâ Student TODO\n",
    "\n",
    "1. Copy the previous cell.\n",
    "\n",
    "2. Change the **role** and **audience**, e.g.:\n",
    "\n",
    "   - \"You are a chief risk officer.\"\n",
    "\n",
    "   - \"Explain to a first-year finance student.\"\n",
    "\n",
    "3. Run and compare the tone and focus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9831079",
   "metadata": {},
   "source": [
    "# 6. Building an Agent with LangGraph\n",
    "\n",
    "We'll use:\n",
    "\n",
    "- LangChain's `ChatGoogleGenerativeAI` as the model wrapper.\n",
    "\n",
    "- LangChain `@tool` decorators for Python tools.\n",
    "\n",
    "- LangGraph's `StateGraph` to define the agent workflow.\n",
    "\n",
    "The agent will:\n",
    "\n",
    "1. Start with just LLM\n",
    "\n",
    "2. Adding tools: get_time, web_search, \n",
    "\n",
    "2. LLM decides: answer directly vs call a tool.\n",
    "\n",
    "3. If tool is called, we run the Python function.\n",
    "\n",
    "4. LLM uses the tool result to produce the final answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeebae9",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02da52e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Reuse the LLM and safe_llm_invoke from cell 6\n",
    "# We'll create a new LLM instance with tools bound to it later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34dd09",
   "metadata": {},
   "source": [
    "## Step 2: Define Tools\n",
    "\n",
    "We'll create two tools:\n",
    "1. `get_time` - A custom tool using the `@tool` decorator\n",
    "2. `web_search` - DuckDuckGo search tool (pre-built from langchain-community)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ba2a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tool: get_time\n",
    "@tool\n",
    "def get_time() -> str:\n",
    "    \"\"\"Get the current date and time. Use this when the user asks about the current time, date, or what day it is.\"\"\"\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Pre-built tool: DuckDuckGo web search\n",
    "web_search = DuckDuckGoSearchRun()\n",
    "\n",
    "# Create list of tools\n",
    "tools = [get_time, web_search]\n",
    "\n",
    "# Create LLM with tools bound to it\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm_with_tools = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\").bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19976be",
   "metadata": {},
   "source": [
    "## Step 3: Define Agent State\n",
    "\n",
    "The agent state holds the conversation history (messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85d235a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for the agent. Contains the conversation history.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546726f",
   "metadata": {},
   "source": [
    "## Step 4: Create Agent Nodes\n",
    "\n",
    "We need three nodes:\n",
    "1. **LLM node**: Calls the LLM with tool calling capability\n",
    "2. **Tool execution node**: Executes tools when LLM requests them\n",
    "3. **Router node**: Decides whether to continue (call tools) or end (final answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "875f5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool map for easy lookup\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "\n",
    "def call_llm(state: AgentState) -> AgentState:\n",
    "    \"\"\"LLM node: Call the LLM with tool calling capability.\"\"\"\n",
    "    # Reuse safe_llm_invoke from cell 6, but adapt it for our llm_with_tools\n",
    "    import time\n",
    "    max_retries = 5\n",
    "    base_delay = 1.0\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = llm_with_tools.invoke(state[\"messages\"])\n",
    "            return {\"messages\": [response]}\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "            time.sleep(delay)\n",
    "\n",
    "def call_tool(state: AgentState) -> AgentState:\n",
    "    \"\"\"Tool execution node: Execute tools when LLM requests them.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # Check if the last message has tool calls\n",
    "    tool_messages = []\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls:\n",
    "            tool_name = tool_call[\"name\"]\n",
    "            tool_args = tool_call.get(\"args\", {})\n",
    "            tool_id = tool_call.get(\"id\", \"\")\n",
    "            \n",
    "            # Get the tool and execute it\n",
    "            if tool_name in tool_map:\n",
    "                tool_result = tool_map[tool_name].invoke(tool_args)\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(content=str(tool_result), tool_call_id=tool_id)\n",
    "                )\n",
    "            else:\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(content=f\"Tool {tool_name} not found\", tool_call_id=tool_id)\n",
    "                )\n",
    "    \n",
    "    return {\"messages\": tool_messages}\n",
    "\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Router node: Decide whether to continue (call tools) or end.\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # If the last message has tool calls, we need to execute tools\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we're done\n",
    "    return \"end\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d36ed9",
   "metadata": {},
   "source": [
    "## Step 5: Build StateGraph\n",
    "\n",
    "Create the graph with nodes and edges, then compile it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9f880ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"llm\", call_llm)\n",
    "workflow.add_node(\"tools\", call_tool)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"llm\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"llm\")\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb5ed9",
   "metadata": {},
   "source": [
    "## Step 6: Create Agent Invoke Function\n",
    "\n",
    "A wrapper function to easily invoke the agent with a user question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c27ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(question: str):\n",
    "    \"\"\"Run the agent with a user question and return the final answer.\"\"\"\n",
    "    # Initialize state with user question\n",
    "    initial_state = {\"messages\": [HumanMessage(content=question)]}\n",
    "    \n",
    "    # Invoke the agent\n",
    "    final_state = agent.invoke(initial_state)\n",
    "    \n",
    "    # Get the last message (should be the final answer)\n",
    "    last_message = final_state[\"messages\"][-1]\n",
    "    \n",
    "    return last_message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09167632",
   "metadata": {},
   "source": [
    "## Step 7: Example Usage\n",
    "\n",
    "Let's test the agent with different types of questions:\n",
    "1. Questions that require tool use (time, web search)\n",
    "2. Questions that can be answered directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a06f751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What time is it?\n",
      "\n",
      "Agent Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Agent:** The time is 14:32:32 on November 20, 2025."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1: Question requiring get_time tool\n",
    "print(\"Question: What time is it?\")\n",
    "print(\"\\nAgent Response:\")\n",
    "response = run_agent(\"What time is it?\")\n",
    "display(Markdown(f\"**Agent:** {response}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18ed9e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Search for latest AAPL stock news\n",
      "\n",
      "Agent Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Agent:** [{'type': 'text', 'text': \"Here's a summary of the latest AAPL stock news:\\n\\n*   **Mixed Headlines:** Strong demand for the iPhone 17 and a perception that Apple hasn't been significantly impacted by the AI sell-off are providing support. However, concerns about CEO succession, design-team departures, and a $634 million patent verdict are weighing on sentiment.\\n*   **Recent Performance:** Apple stock has seen a slight decline over the past week but has maintained gains from the previous month.\\n*   **Q4 Spike:** Apple's stock spiked in Q4 due to outstanding earnings, driven by stronger-than-anticipated demand for the latest iPhone 17 series and record-setting Services revenue.\\n*   **Earnings Call:** While revenue and EPS exceeded estimates, iPhone revenue and China sales missed expectations. Tim Cook discussed AI and other topics in the earnings call.\\n*   **Risk Factors:** Historically, Apple stock has experienced significant declines during major sell-offs (e.g., over 80% during the Dot-Com Bubble, 61% during the Global Financial Crisis, and 30-40% during the 2018 correction and Covid sell-off).\\n*   **Future Catalysts:** Historical trends suggest that future catalysts could propel Apple shares to new peaks.\", 'extras': {'signature': 'CvUBAdHtim9dhVFjH4DDDpLDK88zVj6hwK9C/OL+NV5hpc4RRqeJRT+cjZwkd8shlDvu4PY0iMIzLhLdvUqzCx8bl8dmaemm5XgzN0QhVIUZlgEqiOIIBobQ9MJTRu1dnml2HQXeQCLT/D1ftKsZibAbudhoUqIbKp5BdQugAQB+Hi29f27hMHiTlZ8WUjZ5z5uLEQi8CdMTd9Vzksx/tvEgtf17OaDNrPgaKzLZjhtuzWbnJEsSs5a55m2bcAhVwapAvRfCN/WnT7yCG0zdYBS5Gc6AIKKTXP12yxESEKOiz64Htgdg41mToOci5loV+lPIWzAbrWQ='}}]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2: Question requiring web search tool\n",
    "print(\"Question: Search for latest AAPL stock news\")\n",
    "print(\"\\nAgent Response:\")\n",
    "response = run_agent(\"Search for latest AAPL stock news\")\n",
    "display(Markdown(f\"**Agent:** {response}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9553bbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is machine learning?\n",
      "\n",
      "Agent Response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Agent:** [{'type': 'text', 'text': 'Machine learning (ML) is a field of study within artificial intelligence (AI) that focuses on developing statistical algorithms. These algorithms enable computers to learn from data and make predictions or decisions without being explicitly programmed for every task. Essentially, machine learning teaches systems to identify patterns in data and apply that understanding to new, unseen data, allowing them to \"think\" and \"understand\" in a way similar to humans.', 'extras': {'signature': 'CqsFAdHtim8nhSlfSGsQPXiWt591DWua2DGCN5dCYWux9UjPbJ/RHV/4AdE87nJ8bAs8TDwIpwSovfg/1gg3gYPZ7kG9+WFrDuCTPieFtxPfyN7r2zPXWdRAK1O2aKOroakigHiIW4HbrSPuheteTw3uIqJd+NAd3r2cfgZs/FdfsuiQOloTI3LAgWY1VHT5wdeUYAFvpB5okeFl8QgNyerw+KYmxS3OcGuY+O8EkCMtnSs47SvXDoXbThUmANsPwETBQl59FMtGEA9luQQ1/ijlDIjorwNZNz5bldRnrvTDFURkjeeKolsgQeQ3JRBWS3qp5L2YEc8acSgvCjSSeqF6a5iSE8fW2aGLE54OWCQkPnKiRXY5VXt1w34OEDJaxKuD7Cxw+XxXNAr12v2/tlBGIJ+O3B7vaRPzwS/g23LRJXIYIbFI0bAK7ibrdzZp8l8EJreFdpshAA4Lv7k1i+L5345UjKN0teGq3WPuv0NnbSVeIg16T4C++2d+V+vdnpCxn5FoaQf6QqrQCvbKiiMKCRjaxi8JlbT04Z1NGrGlHBC7JDMgcbxGPgMK2Q/Cr7JhSYd0TpS4mrfSiXCKLZDNeQ9wBjnFqYoTzhQJVQP2M8DEjIcWL84QtKWuU3ghinrsNE8ophd9ZvLOhxhWLIbIaI9Q8OeeUIbnQ9RwLIonHxpxkvfMwdjjV3EQd6eMieZn5S5rLxkLDRNEVNfaMzjk8JwJ6tpvDiTyCFk4ieL0jGv3HhCJwnL2xWrKyu9b3q/9UF2xUdAYizK2S4iVzhO1Sauh3Hpr1iTzvutDaBmLiyvp0Quy/9Sx0ASFncwdu1XqYCzrMbpEvbilv1Fqb5dmGvS4fwMrE1HADzIhXqebmozsyBA7UJaodR5wFuPu1oXxIs716lhlg54KIzg='}}]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 3: Question that can be answered directly (no tool needed)\n",
    "print(\"Question: What is machine learning?\")\n",
    "print(\"\\nAgent Response:\")\n",
    "response = run_agent(\"What is machine learning?\")\n",
    "display(Markdown(f\"**Agent:** {response}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c3381",
   "metadata": {},
   "source": [
    "### üëâ Try it yourself!\n",
    "\n",
    "You can ask the agent any question. The agent will automatically decide whether to:\n",
    "- Use the `get_time` tool for time-related questions\n",
    "- Use the `web_search` tool for current information, news, or web searches\n",
    "- Answer directly if it has the knowledge\n",
    "\n",
    "Try questions like:\n",
    "- \"What's the weather like today?\" (will use web_search)\n",
    "- \"What date is it?\" (will use get_time)\n",
    "- \"Explain quantum computing\" (will answer directly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075b9d5",
   "metadata": {},
   "source": [
    "# 7. Extensions / Experiments\n",
    "\n",
    "- AI tool use: ChatGPT, Gemini\n",
    "- Vibe coding: Cursor\n",
    "- AI browser: ChatGPT Atlas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa6f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
