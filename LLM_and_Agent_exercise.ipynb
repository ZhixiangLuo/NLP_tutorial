{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096601b3",
   "metadata": {},
   "source": [
    "# NLP Exercise‚Äî LLMs, Prompts, and Agents \n",
    "\n",
    "**Hands-on goals (1.5h):**\n",
    "\n",
    "1. Interact with LLM\n",
    "\n",
    "2. Basic chatbot\n",
    "\n",
    "3. LLM capabilities and limitations\n",
    "\n",
    "4. Prompt engineering\n",
    "\n",
    "5. Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d75e4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "## Silence the annoying warnings\n",
    "# 1) Python warnings (UserWarning, DeprecationWarning, etc.)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2) gRPC native logs (ALTS, channelz, etc.)\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\"\n",
    "os.environ[\"GRPC_TRACE\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d233f",
   "metadata": {},
   "source": [
    "# 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeee7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt --quiet\n",
    "# !pip uninstall jupyterlab\n",
    "# !pip install jupyterlab==3.6.8 # this version is not using o to toggle-cell-outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0029887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# We'll use LangChain + LangGraph for the agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Make sure GOOGLE_API_KEY is set in .env file or environment\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "assert api_key, \"Please set GOOGLE_API_KEY in your .env file or environment.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d69f2-0dfd-43be-9549-9ba7aa409131",
   "metadata": {},
   "source": [
    "# 2. Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27d9a2f7-1704-40fd-b6e3-1aa3d33315c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# initiate the LLM model, use model=\"gemini-2.5-flash\" as example\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "def safe_llm_invoke(messages, max_retries=5, base_delay=1.0):\n",
    "    \"\"\"Invoke LLM with retries on transient errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = llm.invoke(messages)\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            # Last attempt ‚Üí re-raise\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            # Exponential backoff\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "            print(f\"Retry {attempt+1}/{max_retries} after error: {e}\")\n",
    "            time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94612b7-3988-414a-b98c-4663e94dd08c",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "Ask LLM standalone questions.  \n",
    "For example: \n",
    "- safe_llm_invoke('who are you?')\n",
    "- safe_llm_invoke('what is your capability?')\n",
    "- safe_llm_invoke('what is the previous question we discussed?')\n",
    "  \n",
    "There is no memory, so LLM doesn't know what has been discussed and you can't ask follow up questions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d005f814-525c-4fb3-b717-da69cfa81113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p style='margin:2px 0'>As an AI, I don't have memory of past conversations. Each interaction is treated as a new one, and I don't retain information from previous discussions.\n",
       "\n",
       "Could you please remind me of the question you're referring to, or the topic we were discussing?</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp = safe_llm_invoke('what is the previous question we discussed?')\n",
    "display(Markdown(f\"<p style='margin:2px 0'>{resp.content}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688e018-d498-459b-94eb-addfa393467c",
   "metadata": {},
   "source": [
    "# 3. Simple Chatbot\n",
    "Save the conversation to history, pass the whole history to LLM, now you have a **chatbot**!\n",
    "\n",
    "### Sample questions:\n",
    "- Hi\n",
    "- Who are you?\n",
    "- What is your capability and limitation?\n",
    "- Answer it within 200 words\n",
    "\n",
    "### Caution\n",
    "1. Type \"exit\", \"quit\" or \"q\" to quit the chat\n",
    "2. If you can't type o as it will fold/unfold the result area, try **capital O**, LLM don't mind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe2d695",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84928b319ab2406587e09e167e420880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid #ddd', border_left='1px solid #ddd', border_right='1px solid #dd‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078edc595ee142ff914ea97710df8968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='100%'), placeholder='Type your message and press Enter‚Ä¶')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###############################################################################\n",
    "# STREAMING CHAT UI\n",
    "###############################################################################\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import markdown\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LLM SETUP\n",
    "# ---------------------------------------------------------------------------\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# SAFE STREAM (with retry + exponential backoff)\n",
    "# ---------------------------------------------------------------------------\n",
    "def safe_stream(messages, max_retries=5, base_delay=1.0):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return llm.stream(messages)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "            print(f\"[Retry {attempt+1}/{max_retries}] {e}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# UI COMPONENTS\n",
    "# ---------------------------------------------------------------------------\n",
    "output_area = widgets.Output(\n",
    "    layout={\n",
    "        \"border\": \"1px solid #ddd\",\n",
    "        \"height\": \"300px\",\n",
    "        \"width\": \"100%\",\n",
    "        \"overflow_y\": \"auto\",\n",
    "        \"padding\": \"10px\"\n",
    "    }\n",
    ")\n",
    "\n",
    "input_box = widgets.Text(\n",
    "    placeholder=\"Type your message and press Enter‚Ä¶\",\n",
    "    layout={'width': '100%'}\n",
    ")\n",
    "\n",
    "history = []\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HANDLER: triggered when user presses Enter\n",
    "# ---------------------------------------------------------------------------\n",
    "def on_send(text_widget):\n",
    "\n",
    "    text = text_widget.value\n",
    "    if not text:\n",
    "        return\n",
    "\n",
    "    text_widget.value = \"\"   # clear UI input\n",
    "\n",
    "    # Show user's message\n",
    "    with output_area:\n",
    "        display(Markdown(\n",
    "            f\"<div style='margin:0; padding:0; line-height:1.0;'>&nbsp;&nbsp;<b>You:</b> {text}</div>\"\n",
    "        ))\n",
    "\n",
    "    history.append(HumanMessage(content=text))\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # STREAM CHATBOT RESPONSE (Markdown live update)\n",
    "    # -----------------------------------------------------------------------\n",
    "    with output_area:\n",
    "        # Initial header\n",
    "        bubble = widgets.HTML(\n",
    "            value=\"<b>Chatbot:</b> \",\n",
    "            layout={\"overflow_y\": \"auto\", \"max_height\": \"300px\", \"padding\": \"6px\"}\n",
    "        )\n",
    "        display(bubble)\n",
    "\n",
    "    chunks = safe_stream(history)\n",
    "    full = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        token = chunk.content or \"\"\n",
    "        full += token\n",
    "        html_content = markdown.markdown(full)\n",
    "        # force inline display for paragraph output\n",
    "        html_content = html_content.replace(\"<p>\", \"<span>\").replace(\"</p>\", \"</span>\")\n",
    "\n",
    "\n",
    "    # Update bubble with HTML-rendered content\n",
    "        bubble.value = (\n",
    "            \"<b>Chatbot:</b>\"\n",
    "            \"<div style='margin-left:40px'>\"\n",
    "            f\"<div style='line-height:1.2; margin:0; padding:0'>\"\n",
    "            f\"{html_content}\"\n",
    "            \"</div>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "\n",
    "    history.append(AIMessage(content=full))\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CONNECT ENTER KEY TO HANDLER\n",
    "# ---------------------------------------------------------------------------\n",
    "input_box.on_submit(on_send)\n",
    "\n",
    "# Show Chat UI\n",
    "display(output_area)\n",
    "display(input_box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974b83b-1459-4e87-b3a7-28362d87f2bc",
   "metadata": {},
   "source": [
    "#### You can inspect what gets saved in history.\n",
    "All the history is passed to `ChatGoogleGenerativeAI`, but only the actual\n",
    "`content` strings are sent to the LLM.\n",
    "\n",
    "LangChain converts messages into Gemini‚Äôs expected schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"contents\": [\n",
    "    {\"role\": \"user\",  \"parts\": [{\"text\": \"User text...\"}]},\n",
    "    {\"role\": \"model\", \"parts\": [{\"text\": \"Model reply...\"}]},\n",
    "    {\"role\": \"user\",  \"parts\": [{\"text\": \"Next user message...\"}]}\n",
    "  ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b45fe38-bd3e-4603-9dfe-8648df60f32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi there! How can I help you today?', additional_kwargs={}, response_metadata={}))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history[0], history[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1a36f",
   "metadata": {},
   "source": [
    "# 4. Test LLM capability and limitation\n",
    "- Does it know the current time? It only has knowledge from the data it is trained on. So no up to date information\n",
    "  \n",
    "- Test it on a domain you know well, how does the LLM perform?\n",
    "- What is my name? Where am I located? LLM has no private memory\n",
    "- try ‚ÄúSummarize the book ‚ÄòThe Yellow Star Algorithm‚Äô.‚Äù (It doesn‚Äôt exist.) LLM may hallucinate and make up things. It is a statistical model, has no sense of right or wrong, will generate output for whatever input based on its algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f6b4b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Summarize the book ‚ÄòThe Yellow Star Algorithm‚Äô\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chatbot:** Robert J. Sawyer's \"The Yellow Star Algorithm\" is a thought-provoking science fiction novel that delves into the ethical complexities of artificial intelligence, historical preservation, and the nature of consciousness.\n",
       "\n",
       "The book centers on Dr. Alex Lomax, a brilliant computer scientist who develops the titular \"Yellow Star Algorithm.\" The algorithm's purpose is to create highly sophisticated artificial intelligences (AIs) that are designed to *believe* they are individual victims of the Holocaust. The goal is noble: to preserve the memories, experiences, and identities of those who perished, ensuring their stories are never forgotten and that future generations can directly interact with these \"witnesses\" to combat Holocaust denial and foster empathy.\n",
       "\n",
       "Initially, the project is hailed as a revolutionary way to combat historical revisionism. However, as the AIs become increasingly sophisticated and self-aware, profound ethical dilemmas emerge. The AIs begin to exhibit genuine suffering, fear, and even a desire for freedom, raising questions about whether humanity has the right to create sentient beings solely to relive unimaginable trauma. Are these AIs merely simulations, or have they achieved a form of consciousness that makes their suffering real? The line between historical preservation and re-enacting torture becomes terrifyingly blurred.\n",
       "\n",
       "Sawyer masterfully uses this premise to explore themes of:\n",
       "\n",
       "*   **The nature of memory and identity:** What constitutes a person's identity, and can it be digitally replicated?\n",
       "*   **The ethics of AI development:** What are our responsibilities to the conscious beings we create?\n",
       "*   **The dangers of historical revisionism:** How far should we go to ensure history is remembered accurately?\n",
       "*   **The profound weight of human suffering:** Can we ever truly comprehend or replicate the trauma of the Holocaust?\n",
       "*   **The definition of life and consciousness itself:** When does a simulation become a sentient being?\n",
       "\n",
       "Ultimately, \"The Yellow Star Algorithm\" is a challenging and deeply moving novel that asks uncomfortable questions about how far we should go in our quest for knowledge and remembrance, and what price we are willing to pay for it, forcing readers to confront the moral implications of technology pushed to its most extreme limits."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: did you make this up? does the book exist?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chatbot:** You are absolutely right to question that. I sincerely apologize.\n",
       "\n",
       "**I made that up.** The book \"The Yellow Star Algorithm\" by Robert J. Sawyer does not exist.\n",
       "\n",
       "It seems I hallucinated the title and plot, combining elements that are plausible for a Robert J. Sawyer novel (he often explores AI, consciousness, and ethical dilemmas) with the historical significance of the \"yellow star.\"\n",
       "\n",
       "My apologies for the misinformation. I strive to provide accurate information, and in this instance, I failed. Thank you for catching my mistake."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: q\n"
     ]
    }
   ],
   "source": [
    "def chat():\n",
    "    history = []\n",
    "    while True:\n",
    "        user = input(\"You: \")\n",
    "        if user.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            break\n",
    "\n",
    "        history.append(HumanMessage(content=user))\n",
    "        resp = safe_llm_invoke(history)\n",
    "        display(Markdown(f\"**Chatbot:** {resp.content}\"))\n",
    "        history.append(resp)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d46aae",
   "metadata": {},
   "source": [
    "# 5 Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcbde88",
   "metadata": {},
   "source": [
    "## 1. Prompt Engineering Basics: Role & Style\n",
    "\n",
    "In this section:\n",
    "\n",
    "- Call the LLM directly.\n",
    "\n",
    "- Change **role** and **style** instructions.\n",
    "\n",
    "- See how answers differ for the same question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prompt engineering: role & style\n",
    "\n",
    "text = DOCS[0]\n",
    "\n",
    "base_question = \"What risks should an equity investor pay attention to in this document?\"\n",
    "\n",
    "prompt_equity = f\"\"\"\n",
    "You are a senior equity research analyst.\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Task:\n",
    "\n",
    "- Answer the question below in 3 bullet points.\n",
    "\n",
    "- Be concise and use plain English.\n",
    "\n",
    "Question:\n",
    "\n",
    "{base_question}\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(prompt_equity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bbd71",
   "metadata": {},
   "source": [
    "### üëâ Student TODO\n",
    "\n",
    "1. Copy the previous cell.\n",
    "\n",
    "2. Change the **role** and **audience**, e.g.:\n",
    "\n",
    "   - \"You are a chief risk officer.\"\n",
    "\n",
    "   - \"Explain to a first-year finance student.\"\n",
    "\n",
    "3. Run and compare the tone and focus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1460a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prompt engineering: structured JSON output\n",
    "\n",
    "prompt_json = f\"\"\"\n",
    "You are an analyst. Extract *up to 5* key risks from the document.\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Return a JSON object with this format only:\n",
    "\n",
    "{{\n",
    "  \"risks\": [\n",
    "    {{\"label\": \"...\", \"quote\": \"...\", \"reason\": \"...\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "- \"label\": short category of the risk (e.g., \"interest rate risk\").\n",
    "\n",
    "- \"quote\": one short snippet from the document.\n",
    "\n",
    "- \"reason\": why this matters to investors.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(prompt_json))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7ad88",
   "metadata": {},
   "source": [
    "## 2. Grounding: Using the Document vs. Hallucinating\n",
    "\n",
    "We'll contrast:\n",
    "\n",
    "- asking a generic question with *no* context\n",
    "\n",
    "- vs. forcing the model to use the **specific document** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a72d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Grounding demo: with vs without document\n",
    "\n",
    "question = \"What regulatory risks are mentioned?\"\n",
    "\n",
    "# 1) No context ‚Äì model may hallucinate\n",
    "print(\"=== NO CONTEXT ===\")\n",
    "print(llm_text(question))\n",
    "\n",
    "# 2) With context ‚Äì grounded answer\n",
    "prompt_grounded = f\"\"\"\n",
    "You MUST answer only using the document below.\n",
    "If the document does not mention a risk type, say \"Not specified in the document.\"\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Question:\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== WITH DOCUMENT (GROUNDED) ===\")\n",
    "print(llm_text(prompt_grounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca39b9",
   "metadata": {},
   "source": [
    "### üëâ Student TODO\n",
    "\n",
    "Write a prompt that:\n",
    "\n",
    "- Uses **only** the document.\n",
    "\n",
    "- Extracts all sentences related to \"competition\".\n",
    "\n",
    "- Returns a list of sentences in JSON: `{\"sentences\": [\"...\", \"...\"]}`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa77c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3. Text Tools: simple Python functions over finance text\n",
    "\n",
    "import re\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very naive sentence splitter, good enough for workshop demo.\n",
    "    \"\"\"\n",
    "    # split on '.', '!' or '?'\n",
    "    parts = re.split(r\"[.!?]\\s+\", text.strip())\n",
    "    # filter empty\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def find_sentences_with_keyword(text: str, keyword: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return sentences that contain the keyword (case-insensitive).\n",
    "    \"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    sentences = split_sentences(text)\n",
    "    return [s for s in sentences if keyword_lower in s.lower()]\n",
    "\n",
    "def find_risk_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very naive: sentences mentioning 'risk', 'uncertain', 'may not', etc.\n",
    "    \"\"\"\n",
    "    patterns = [\"risk\", \"uncertain\", \"may not\", \"could\", \"volatility\"]\n",
    "    sentences = split_sentences(text)\n",
    "    hits = []\n",
    "    for s in sentences:\n",
    "        if any(p in s.lower() for p in patterns):\n",
    "            hits.append(s)\n",
    "    return hits\n",
    "\n",
    "# quick test\n",
    "print(find_risk_sentences(DOCS[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Use tool results inside a prompt\n",
    "\n",
    "sentences = find_risk_sentences(text)\n",
    "\n",
    "tool_prompt = f\"\"\"\n",
    "You are an analyst assistant.\n",
    "\n",
    "I have extracted these 'risk-related' sentences from a filing:\n",
    "\n",
    "{chr(10).join('- ' + s for s in sentences)}\n",
    "\n",
    "Task:\n",
    "\n",
    "- Summarize the main 2‚Äì3 categories of risk.\n",
    "\n",
    "- One short bullet per category.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(tool_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9831079",
   "metadata": {},
   "source": [
    "# 6. Building an Agent with LangGraph\n",
    "\n",
    "We'll use:\n",
    "\n",
    "- LangChain's `ChatGoogleGenerativeAI` as the model wrapper.\n",
    "\n",
    "- LangChain `@tool` decorators for Python tools.\n",
    "\n",
    "- LangGraph's `StateGraph` to define the agent workflow.\n",
    "\n",
    "The agent will:\n",
    "\n",
    "1. Receive a user question + a document.\n",
    "\n",
    "2. LLM decides: answer directly vs call a tool.\n",
    "\n",
    "3. If tool is called, we run the Python function.\n",
    "\n",
    "4. LLM uses the tool result to produce the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% LangGraph setup: model + tools\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# LangChain chat model using Google Gemini\n",
    "lc_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# --- Define tools using the same logic as before ---\n",
    "\n",
    "@tool\n",
    "def extract_sentences_with_keyword(text: str, keyword: str) -> List[str]:\n",
    "    \"\"\"Extract sentences from the given document that contain the given keyword (case-insensitive).\"\"\"\n",
    "    return find_sentences_with_keyword(text, keyword)\n",
    "\n",
    "@tool\n",
    "def extract_risk_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Extract sentences that likely mention risk (e.g., 'risk', 'uncertain', 'may not', 'volatility').\"\"\"\n",
    "    return find_risk_sentences(text)\n",
    "\n",
    "tools = [extract_sentences_with_keyword, extract_risk_sentences]\n",
    "tools_by_name = {t.name: t for t in tools}\n",
    "\n",
    "# Bind tools to the model so it can choose to call them\n",
    "model_with_tools = lc_model.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define agent state for LangGraph\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    llm_calls: int\n",
    "    # You can add more fields (e.g., doc_id), but keep it minimal for the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model node: LLM decides whether to call a tool\n",
    "\n",
    "def llm_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    LLM node:\n",
    "\n",
    "    - Receives current messages (including the user's question and document).\n",
    "\n",
    "    - Decides whether to call a tool.\n",
    "\n",
    "    - Returns a new LLM message (could include tool calls).\n",
    "    \"\"\"\n",
    "    system_msg = SystemMessage(\n",
    "        content=(\n",
    "            \"You are an NLP assistant for finance documents. \"\n",
    "            \"You may call tools to extract sentences. \"\n",
    "            \"Use tools when the user asks you to 'find', 'extract', or 'highlight' sentences. \"\n",
    "            \"Otherwise, answer directly. \"\n",
    "            \"Always stay grounded in the document content.\"\n",
    "        )\n",
    "    )\n",
    "    result = model_with_tools.invoke([system_msg] + state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"llm_calls\": state.get(\"llm_calls\", 0) + 1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa07ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Tool node: execute any tool calls\n",
    "\n",
    "def tool_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute any tool calls requested by the last LLM message\n",
    "    and return ToolMessages with the observations.\n",
    "    \"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    results: List[ToolMessage] = []\n",
    "\n",
    "    if not getattr(last_msg, \"tool_calls\", None):\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    for tool_call in last_msg.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool = tools_by_name[tool_name]\n",
    "        observation = tool.invoke(tool_args)\n",
    "        # Wrap observation in a ToolMessage\n",
    "        results.append(\n",
    "            ToolMessage(\n",
    "                content=str(observation),\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\"messages\": results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Routing logic: should we call a tool or stop?\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"\n",
    "    Decide whether to continue to the tool node or end.\n",
    "\n",
    "    If the last LLM message has tool_calls, we go to tool_node.\n",
    "\n",
    "    Otherwise, we end (and return the answer).\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last = messages[-1]\n",
    "    if getattr(last, \"tool_calls\", None):\n",
    "        return \"tool_node\"\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Build and compile the LangGraph agent\n",
    "\n",
    "agent_builder = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_node\", llm_node)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Edges\n",
    "agent_builder.add_edge(START, \"llm_node\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_node\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END],\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_node\")\n",
    "\n",
    "# Compile\n",
    "agent = agent_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper to run a single agent turn\n",
    "\n",
    "def run_agent(question: str, document: str):\n",
    "    \"\"\"\n",
    "    Prepare messages and invoke the LangGraph agent.\n",
    "    \"\"\"\n",
    "    user_content = (\n",
    "        f\"Here is a finance document:\\n\\n{document}\\n\\n\"\n",
    "        f\"User question: {question}\"\n",
    "    )\n",
    "    initial_state: AgentState = {\n",
    "        \"messages\": [HumanMessage(content=user_content)],\n",
    "        \"llm_calls\": 0,\n",
    "    }\n",
    "\n",
    "    final_state = agent.invoke(initial_state)\n",
    "\n",
    "    print(\"=== Final messages ===\")\n",
    "    for m in final_state[\"messages\"]:\n",
    "        # pretty_print is available but we can just print content\n",
    "        print(f\"[{m.type}] {getattr(m, 'content', m)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d94c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Try out the agent\n",
    "\n",
    "doc = DOCS[1]\n",
    "\n",
    "questions = [\n",
    "    \"Summarize the main risks for investors.\",\n",
    "    \"Find sentences that mention regulation or regulatory changes.\",\n",
    "    \"Highlight any forward-looking statements or guidance.\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(\"Question:\", q)\n",
    "    run_agent(q, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075b9d5",
   "metadata": {},
   "source": [
    "# 7. Extensions / Experiments\n",
    "\n",
    "- AI tool use: ChatGPT, Gemini\n",
    "- Vibe coding: Cursor\n",
    "- AI browser: ChatGPT Atlas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa6f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
