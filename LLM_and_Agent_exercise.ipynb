{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096601b3",
   "metadata": {},
   "source": [
    "# NLP Exercise‚Äî LLMs, Prompts, and Agents \n",
    "\n",
    "**Hands-on goals (1.5h):**\n",
    "\n",
    "1. Interact with LLM\n",
    "\n",
    "2. Basic chatbot\n",
    "\n",
    "3. LLM capabilities and limitations\n",
    "\n",
    "4. Prompt engineering\n",
    "\n",
    "5. Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75e4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "## Silence the annoying warnings\n",
    "# 1) Python warnings (UserWarning, DeprecationWarning, etc.)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2) gRPC native logs (ALTS, channelz, etc.)\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\"\n",
    "os.environ[\"GRPC_TRACE\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d233f",
   "metadata": {},
   "source": [
    "# 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeee7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt --quiet\n",
    "# !pip uninstall jupyterlab --yes --quiet\n",
    "# !pip install jupyterlab==3.6.8  --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0029887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# We'll use LangChain + LangGraph for the agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Make sure GOOGLE_API_KEY is set in .env file or environment\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "assert api_key, \"Please set GOOGLE_API_KEY in your .env file or environment.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d69f2-0dfd-43be-9549-9ba7aa409131",
   "metadata": {},
   "source": [
    "# 2. Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d9a2f7-1704-40fd-b6e3-1aa3d33315c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# initiate the LLM model, use model=\"gemini-2.5-flash\" as example\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "def safe_llm_invoke(messages, max_retries=5, base_delay=1.0):\n",
    "    \"\"\"Invoke LLM with retries on transient errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = llm.invoke(messages)\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            # Last attempt ‚Üí re-raise\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            # Exponential backoff\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "#             print(f\"Retry {attempt+1}/{max_retries} after error: {e}\")\n",
    "            time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94612b7-3988-414a-b98c-4663e94dd08c",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "Ask LLM standalone questions.  \n",
    "For example: \n",
    "- safe_llm_invoke('who are you?')\n",
    "- safe_llm_invoke('what is your capability?')\n",
    "- safe_llm_invoke('what is the previous question we discussed?')\n",
    "  \n",
    "There is no memory, so LLM doesn't know what has been discussed and you can't ask follow up questions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d005f814-525c-4fb3-b717-da69cfa81113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<p style='margin:2px 0'>I don't have a memory of our previous conversations. Each interaction with me is a new one.\n",
       "\n",
       "Could you please remind me of the last question or topic we were discussing?</p>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resp = safe_llm_invoke('what is the previous question we discussed?')\n",
    "display(Markdown(f\"<p style='margin:2px 0'>{resp.content}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688e018-d498-459b-94eb-addfa393467c",
   "metadata": {},
   "source": [
    "# 3. Simple Chatbot\n",
    "Save the conversation to history, pass the whole history to LLM, now you have a **chatbot**!\n",
    "\n",
    "### Sample questions:\n",
    "- Hi\n",
    "- Who are you?\n",
    "- What is your capability and limitation?\n",
    "- Answer it within 200 words\n",
    "\n",
    "### Caution\n",
    "1. Type \"exit\", \"quit\" or \"q\" to quit the chat\n",
    "2. If you can't type o as it will fold/unfold the result area, try **capital O**, LLM don't mind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fe2d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76ebe36130d4b3dbc9905e0563eda71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid #ddd', border_left='1px solid #ddd', border_right='1px solid #dd‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cdaee2c44ed4ef0bb6bd621654b50bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', layout=Layout(width='100%'), placeholder='Type your message and press Enter‚Ä¶')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:232\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgeneration_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m FailedPrecondition \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1183\u001b[39m, in \u001b[36mGenerativeServiceClient.stream_generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1182\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:214\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    210\u001b[39m         error_list,\n\u001b[32m    211\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    212\u001b[39m         original_timeout,\n\u001b[32m    213\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:173\u001b[39m, in \u001b[36m_wrap_stream_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mon_send\u001b[39m\u001b[34m(text_widget)\u001b[39m\n\u001b[32m     87\u001b[39m chunks = safe_stream(history)\n\u001b[32m     88\u001b[39m full = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:523\u001b[39m, in \u001b[36mBaseChatModel.stream\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    521\u001b[39m index = -\u001b[32m1\u001b[39m\n\u001b[32m    522\u001b[39m index_type = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_id\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:2361\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._stream\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   2359\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m   2360\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.max_retries\n\u001b[32m-> \u001b[39m\u001b[32m2361\u001b[39m response: GenerateContentResponse = \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream_generate_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2364\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdefault_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2366\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2368\u001b[39m prev_usage_metadata: UsageMetadata | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# cumulative usage\u001b[39;00m\n\u001b[32m   2369\u001b[39m index = -\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:258\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    253\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    255\u001b[39m params = {\n\u001b[32m    256\u001b[39m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _allowed_params_prediction_service\n\u001b[32m    257\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_chat_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git_repos/NLP_tutorial/venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:244\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    243\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ResourceExhausted \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    246\u001b[39m     \u001b[38;5;66;03m# Handle quota-exceeded error with recommended retry delay\u001b[39;00m\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mretry_after\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mretry_after\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m) < kwargs.get(\n\u001b[32m    248\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwait_exponential_max\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m60.0\u001b[39m\n\u001b[32m    249\u001b[39m     ):\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Invalid argument provided to Gemini: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# STREAMING CHAT UI\n",
    "###############################################################################\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import markdown\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LLM SETUP\n",
    "# ---------------------------------------------------------------------------\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# SAFE STREAM (with retry + exponential backoff)\n",
    "# ---------------------------------------------------------------------------\n",
    "def safe_stream(messages, max_retries=5, base_delay=1.0):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return llm.stream(messages)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "#             print(f\"[Retry {attempt+1}/{max_retries}] {e}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# UI COMPONENTS\n",
    "# ---------------------------------------------------------------------------\n",
    "output_area = widgets.Output(\n",
    "    layout={\n",
    "        \"border\": \"1px solid #ddd\",\n",
    "        \"height\": \"300px\",\n",
    "        \"width\": \"100%\",\n",
    "        \"overflow_y\": \"auto\",\n",
    "        \"padding\": \"10px\"\n",
    "    }\n",
    ")\n",
    "\n",
    "input_box = widgets.Text(\n",
    "    placeholder=\"Type your message and press Enter‚Ä¶\",\n",
    "    layout={'width': '100%'}\n",
    ")\n",
    "\n",
    "history = []\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HANDLER: triggered when user presses Enter\n",
    "# ---------------------------------------------------------------------------\n",
    "def on_send(text_widget):\n",
    "\n",
    "    text = text_widget.value\n",
    "    if not text:\n",
    "        return\n",
    "\n",
    "    text_widget.value = \"\"   # clear UI input\n",
    "\n",
    "    # Show user's message\n",
    "    with output_area:\n",
    "        display(Markdown(\n",
    "            f\"<div style='margin:0; padding:0; line-height:1.0;'>&nbsp;&nbsp;<b>You:</b> {text}</div>\"\n",
    "        ))\n",
    "\n",
    "    history.append(HumanMessage(content=text))\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # STREAM CHATBOT RESPONSE (Markdown live update)\n",
    "    # -----------------------------------------------------------------------\n",
    "    with output_area:\n",
    "        # Initial header\n",
    "        bubble = widgets.HTML(\n",
    "            value=\"<b>Chatbot:</b> \",\n",
    "            layout={\"overflow_y\": \"auto\", \"max_height\": \"300px\", \"padding\": \"6px\"}\n",
    "        )\n",
    "        display(bubble)\n",
    "\n",
    "    chunks = safe_stream(history)\n",
    "    full = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        token = chunk.content or \"\"\n",
    "        full += token\n",
    "        html_content = markdown.markdown(full)\n",
    "        # force inline display for paragraph output\n",
    "        html_content = html_content.replace(\"<p>\", \"<span>\").replace(\"</p>\", \"</span>\")\n",
    "\n",
    "\n",
    "    # Update bubble with HTML-rendered content\n",
    "        bubble.value = (\n",
    "            \"<b>Chatbot:</b>\"\n",
    "            \"<div style='margin-left:40px'>\"\n",
    "            f\"<div style='line-height:1.2; margin:0; padding:0'>\"\n",
    "            f\"{html_content}\"\n",
    "            \"</div>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "\n",
    "    history.append(AIMessage(content=full))\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CONNECT ENTER KEY TO HANDLER\n",
    "# ---------------------------------------------------------------------------\n",
    "input_box.on_submit(on_send)\n",
    "\n",
    "# Show Chat UI\n",
    "display(output_area)\n",
    "display(input_box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974b83b-1459-4e87-b3a7-28362d87f2bc",
   "metadata": {},
   "source": [
    "#### You can inspect what gets saved in history.\n",
    "All the history is passed to `ChatGoogleGenerativeAI`, but only the actual\n",
    "`content` strings are sent to the LLM.\n",
    "\n",
    "LangChain converts messages into Gemini‚Äôs expected schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"contents\": [\n",
    "    {\"role\": \"user\",  \"parts\": [{\"text\": \"User text...\"}]},\n",
    "    {\"role\": \"model\", \"parts\": [{\"text\": \"Model reply...\"}]},\n",
    "    {\"role\": \"user\",  \"parts\": [{\"text\": \"Next user message...\"}]}\n",
    "  ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45fe38-bd3e-4603-9dfe-8648df60f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history[0], history[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1a36f",
   "metadata": {},
   "source": [
    "# 4. Test LLM capability and limitation\n",
    "- Does it know the current time? It only has knowledge from the data it is trained on. So no up to date information\n",
    "  \n",
    "- Test it on a domain you know well, how does the LLM perform?\n",
    "- What is my name? Where am I located? LLM has no private memory\n",
    "- try ‚ÄúSummarize the book ‚ÄòThe Yellow Star Algorithm‚Äô.‚Äù (It doesn‚Äôt exist.) LLM may hallucinate and make up things. It is a statistical model, has no sense of right or wrong, will generate output for whatever input based on its algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b4b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chatbot:** Hi there! How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: finally you are working\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Chatbot:** I'm always here and ready to help! What can I do for you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def chat():\n",
    "    history = []\n",
    "    while True:\n",
    "        user = input(\"You: \")\n",
    "        if user.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            break\n",
    "\n",
    "        history.append(HumanMessage(content=user))\n",
    "        resp = safe_llm_invoke(history)\n",
    "        display(Markdown(f\"**Chatbot:** {resp.content}\"))\n",
    "        history.append(resp)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d46aae",
   "metadata": {},
   "source": [
    "# 5 Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcbde88",
   "metadata": {},
   "source": [
    "## 1. Prompt Engineering Basics: Role & Style\n",
    "\n",
    "In this section:\n",
    "\n",
    "- Call the LLM directly.\n",
    "\n",
    "- Change **role** and **style** instructions.\n",
    "\n",
    "- See how answers differ for the same question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prompt engineering: role & style\n",
    "\n",
    "text = DOCS[0]\n",
    "\n",
    "base_question = \"What risks should an equity investor pay attention to in this document?\"\n",
    "\n",
    "prompt_equity = f\"\"\"\n",
    "You are a senior equity research analyst.\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Task:\n",
    "\n",
    "- Answer the question below in 3 bullet points.\n",
    "\n",
    "- Be concise and use plain English.\n",
    "\n",
    "Question:\n",
    "\n",
    "{base_question}\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(prompt_equity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bbd71",
   "metadata": {},
   "source": [
    "### üëâ Student TODO\n",
    "\n",
    "1. Copy the previous cell.\n",
    "\n",
    "2. Change the **role** and **audience**, e.g.:\n",
    "\n",
    "   - \"You are a chief risk officer.\"\n",
    "\n",
    "   - \"Explain to a first-year finance student.\"\n",
    "\n",
    "3. Run and compare the tone and focus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1460a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prompt engineering: structured JSON output\n",
    "\n",
    "prompt_json = f\"\"\"\n",
    "You are an analyst. Extract *up to 5* key risks from the document.\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Return a JSON object with this format only:\n",
    "\n",
    "{{\n",
    "  \"risks\": [\n",
    "    {{\"label\": \"...\", \"quote\": \"...\", \"reason\": \"...\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "- \"label\": short category of the risk (e.g., \"interest rate risk\").\n",
    "\n",
    "- \"quote\": one short snippet from the document.\n",
    "\n",
    "- \"reason\": why this matters to investors.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(prompt_json))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7ad88",
   "metadata": {},
   "source": [
    "## 2. Grounding: Using the Document vs. Hallucinating\n",
    "\n",
    "We'll contrast:\n",
    "\n",
    "- asking a generic question with *no* context\n",
    "\n",
    "- vs. forcing the model to use the **specific document** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a72d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Grounding demo: with vs without document\n",
    "\n",
    "question = \"What regulatory risks are mentioned?\"\n",
    "\n",
    "# 1) No context ‚Äì model may hallucinate\n",
    "print(\"=== NO CONTEXT ===\")\n",
    "print(llm_text(question))\n",
    "\n",
    "# 2) With context ‚Äì grounded answer\n",
    "prompt_grounded = f\"\"\"\n",
    "You MUST answer only using the document below.\n",
    "If the document does not mention a risk type, say \"Not specified in the document.\"\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Question:\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== WITH DOCUMENT (GROUNDED) ===\")\n",
    "print(llm_text(prompt_grounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca39b9",
   "metadata": {},
   "source": [
    "### üëâ Student TODO\n",
    "\n",
    "Write a prompt that:\n",
    "\n",
    "- Uses **only** the document.\n",
    "\n",
    "- Extracts all sentences related to \"competition\".\n",
    "\n",
    "- Returns a list of sentences in JSON: `{\"sentences\": [\"...\", \"...\"]}`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa77c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3. Text Tools: simple Python functions over finance text\n",
    "\n",
    "import re\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very naive sentence splitter, good enough for workshop demo.\n",
    "    \"\"\"\n",
    "    # split on '.', '!' or '?'\n",
    "    parts = re.split(r\"[.!?]\\s+\", text.strip())\n",
    "    # filter empty\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def find_sentences_with_keyword(text: str, keyword: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return sentences that contain the keyword (case-insensitive).\n",
    "    \"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    sentences = split_sentences(text)\n",
    "    return [s for s in sentences if keyword_lower in s.lower()]\n",
    "\n",
    "def find_risk_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very naive: sentences mentioning 'risk', 'uncertain', 'may not', etc.\n",
    "    \"\"\"\n",
    "    patterns = [\"risk\", \"uncertain\", \"may not\", \"could\", \"volatility\"]\n",
    "    sentences = split_sentences(text)\n",
    "    hits = []\n",
    "    for s in sentences:\n",
    "        if any(p in s.lower() for p in patterns):\n",
    "            hits.append(s)\n",
    "    return hits\n",
    "\n",
    "# quick test\n",
    "print(find_risk_sentences(DOCS[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Use tool results inside a prompt\n",
    "\n",
    "sentences = find_risk_sentences(text)\n",
    "\n",
    "tool_prompt = f\"\"\"\n",
    "You are an analyst assistant.\n",
    "\n",
    "I have extracted these 'risk-related' sentences from a filing:\n",
    "\n",
    "{chr(10).join('- ' + s for s in sentences)}\n",
    "\n",
    "Task:\n",
    "\n",
    "- Summarize the main 2‚Äì3 categories of risk.\n",
    "\n",
    "- One short bullet per category.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(tool_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9831079",
   "metadata": {},
   "source": [
    "# 6. Building an Agent with LangGraph\n",
    "\n",
    "We'll use:\n",
    "\n",
    "- LangChain's `ChatGoogleGenerativeAI` as the model wrapper.\n",
    "\n",
    "- LangChain `@tool` decorators for Python tools.\n",
    "\n",
    "- LangGraph's `StateGraph` to define the agent workflow.\n",
    "\n",
    "The agent will:\n",
    "\n",
    "1. Receive a user question + a document.\n",
    "\n",
    "2. LLM decides: answer directly vs call a tool.\n",
    "\n",
    "3. If tool is called, we run the Python function.\n",
    "\n",
    "4. LLM uses the tool result to produce the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% LangGraph setup: model + tools\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# LangChain chat model using Google Gemini\n",
    "lc_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# --- Define tools using the same logic as before ---\n",
    "\n",
    "@tool\n",
    "def extract_sentences_with_keyword(text: str, keyword: str) -> List[str]:\n",
    "    \"\"\"Extract sentences from the given document that contain the given keyword (case-insensitive).\"\"\"\n",
    "    return find_sentences_with_keyword(text, keyword)\n",
    "\n",
    "@tool\n",
    "def extract_risk_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Extract sentences that likely mention risk (e.g., 'risk', 'uncertain', 'may not', 'volatility').\"\"\"\n",
    "    return find_risk_sentences(text)\n",
    "\n",
    "tools = [extract_sentences_with_keyword, extract_risk_sentences]\n",
    "tools_by_name = {t.name: t for t in tools}\n",
    "\n",
    "# Bind tools to the model so it can choose to call them\n",
    "model_with_tools = lc_model.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define agent state for LangGraph\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    llm_calls: int\n",
    "    # You can add more fields (e.g., doc_id), but keep it minimal for the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model node: LLM decides whether to call a tool\n",
    "\n",
    "def llm_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    LLM node:\n",
    "\n",
    "    - Receives current messages (including the user's question and document).\n",
    "\n",
    "    - Decides whether to call a tool.\n",
    "\n",
    "    - Returns a new LLM message (could include tool calls).\n",
    "    \"\"\"\n",
    "    system_msg = SystemMessage(\n",
    "        content=(\n",
    "            \"You are an NLP assistant for finance documents. \"\n",
    "            \"You may call tools to extract sentences. \"\n",
    "            \"Use tools when the user asks you to 'find', 'extract', or 'highlight' sentences. \"\n",
    "            \"Otherwise, answer directly. \"\n",
    "            \"Always stay grounded in the document content.\"\n",
    "        )\n",
    "    )\n",
    "    result = model_with_tools.invoke([system_msg] + state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"llm_calls\": state.get(\"llm_calls\", 0) + 1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa07ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Tool node: execute any tool calls\n",
    "\n",
    "def tool_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute any tool calls requested by the last LLM message\n",
    "    and return ToolMessages with the observations.\n",
    "    \"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    results: List[ToolMessage] = []\n",
    "\n",
    "    if not getattr(last_msg, \"tool_calls\", None):\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    for tool_call in last_msg.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool = tools_by_name[tool_name]\n",
    "        observation = tool.invoke(tool_args)\n",
    "        # Wrap observation in a ToolMessage\n",
    "        results.append(\n",
    "            ToolMessage(\n",
    "                content=str(observation),\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\"messages\": results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Routing logic: should we call a tool or stop?\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"\n",
    "    Decide whether to continue to the tool node or end.\n",
    "\n",
    "    If the last LLM message has tool_calls, we go to tool_node.\n",
    "\n",
    "    Otherwise, we end (and return the answer).\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last = messages[-1]\n",
    "    if getattr(last, \"tool_calls\", None):\n",
    "        return \"tool_node\"\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Build and compile the LangGraph agent\n",
    "\n",
    "agent_builder = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_node\", llm_node)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Edges\n",
    "agent_builder.add_edge(START, \"llm_node\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_node\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END],\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_node\")\n",
    "\n",
    "# Compile\n",
    "agent = agent_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper to run a single agent turn\n",
    "\n",
    "def run_agent(question: str, document: str):\n",
    "    \"\"\"\n",
    "    Prepare messages and invoke the LangGraph agent.\n",
    "    \"\"\"\n",
    "    user_content = (\n",
    "        f\"Here is a finance document:\\n\\n{document}\\n\\n\"\n",
    "        f\"User question: {question}\"\n",
    "    )\n",
    "    initial_state: AgentState = {\n",
    "        \"messages\": [HumanMessage(content=user_content)],\n",
    "        \"llm_calls\": 0,\n",
    "    }\n",
    "\n",
    "    final_state = agent.invoke(initial_state)\n",
    "\n",
    "    print(\"=== Final messages ===\")\n",
    "    for m in final_state[\"messages\"]:\n",
    "        # pretty_print is available but we can just print content\n",
    "        print(f\"[{m.type}] {getattr(m, 'content', m)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d94c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Try out the agent\n",
    "\n",
    "doc = DOCS[1]\n",
    "\n",
    "questions = [\n",
    "    \"Summarize the main risks for investors.\",\n",
    "    \"Find sentences that mention regulation or regulatory changes.\",\n",
    "    \"Highlight any forward-looking statements or guidance.\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(\"Question:\", q)\n",
    "    run_agent(q, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075b9d5",
   "metadata": {},
   "source": [
    "# 7. Extensions / Experiments\n",
    "\n",
    "- AI tool use: ChatGPT, Gemini\n",
    "- Vibe coding: Cursor\n",
    "- AI browser: ChatGPT Atlas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa6f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
