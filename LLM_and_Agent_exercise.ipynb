{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096601b3",
   "metadata": {},
   "source": [
    "# NLP for Finance â€” LLMs, Prompt Engineering, and Agents (LangGraph)\n",
    "\n",
    "**Hands-on goals (1.5h):**\n",
    "\n",
    "1. Call an LLM from Python.\n",
    "\n",
    "2. See how prompts change behavior (prompt engineering).\n",
    "\n",
    "3. Ground the LLM on finance text to reduce hallucinations.\n",
    "\n",
    "4. Build a simple text tool over finance documents.\n",
    "\n",
    "5. Wrap it in a LangGraph agent that decides when to call tools and how to answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0029887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Setup: imports & keys\n",
    "\n",
    "# If running on Colab, you may need:\n",
    "# !pip install openai langchain langgraph langchain-openai tiktoken --quiet\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- OpenAI client (core LLM calls) ---\n",
    "from openai import OpenAI\n",
    "\n",
    "# We'll also use LangChain + LangGraph for the agent later\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Make sure OPENAI_API_KEY is set in your environment before running\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"Please set OPENAI_API_KEY in your environment.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Simple LLM helper\n",
    "\n",
    "def llm_text(\n",
    "    prompt: str,\n",
    "    model: str = \"gpt-4.1-mini\",\n",
    "    temperature: float = 0.2,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Minimal helper for single-turn prompts.\n",
    "    \"\"\"\n",
    "    resp = client.responses.create(\n",
    "        model=model,\n",
    "        input=prompt,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    # responses.create returns multiple output items; use the text helper:\n",
    "    return resp.output_text\n",
    "\n",
    "def llm_chat(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: str = \"gpt-4.1-mini\",\n",
    "    temperature: float = 0.2,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Chat-style wrapper using role/content messages.\n",
    "    \"\"\"\n",
    "    resp = client.responses.create(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return resp.output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbeaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load example finance documents (text only)\n",
    "\n",
    "# In practice, you'd load from files/CSV. For the workshop, keep it simple:\n",
    "DOCS = [\n",
    "    \"\"\"\n",
    "    Our business is subject to a number of risks, including fluctuations in interest rates,\n",
    "    exposure to foreign exchange rate volatility, and increased competition in our core markets.\n",
    "    We may not be able to maintain our current margins if macroeconomic conditions deteriorate.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Management is optimistic about revenue growth in the coming fiscal year, driven by strong demand\n",
    "    in our consumer lending segment. However, regulatory changes under consideration could increase\n",
    "    capital requirements and negatively impact returns.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(DOCS)} finance documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcbde88",
   "metadata": {},
   "source": [
    "## 1. Prompt Engineering Basics: Role & Style\n",
    "\n",
    "In this section:\n",
    "\n",
    "- Call the LLM directly.\n",
    "\n",
    "- Change **role** and **style** instructions.\n",
    "\n",
    "- See how answers differ for the same question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prompt engineering: role & style\n",
    "\n",
    "text = DOCS[0]\n",
    "\n",
    "base_question = \"What risks should an equity investor pay attention to in this document?\"\n",
    "\n",
    "prompt_equity = f\"\"\"\n",
    "You are a senior equity research analyst.\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Task:\n",
    "\n",
    "- Answer the question below in 3 bullet points.\n",
    "\n",
    "- Be concise and use plain English.\n",
    "\n",
    "Question:\n",
    "\n",
    "{base_question}\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(prompt_equity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bbd71",
   "metadata": {},
   "source": [
    "### ðŸ‘‰ Student TODO\n",
    "\n",
    "1. Copy the previous cell.\n",
    "\n",
    "2. Change the **role** and **audience**, e.g.:\n",
    "\n",
    "   - \"You are a chief risk officer.\"\n",
    "\n",
    "   - \"Explain to a first-year finance student.\"\n",
    "\n",
    "3. Run and compare the tone and focus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1460a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prompt engineering: structured JSON output\n",
    "\n",
    "prompt_json = f\"\"\"\n",
    "You are an analyst. Extract *up to 5* key risks from the document.\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Return a JSON object with this format only:\n",
    "\n",
    "{{\n",
    "  \"risks\": [\n",
    "    {{\"label\": \"...\", \"quote\": \"...\", \"reason\": \"...\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "- \"label\": short category of the risk (e.g., \"interest rate risk\").\n",
    "\n",
    "- \"quote\": one short snippet from the document.\n",
    "\n",
    "- \"reason\": why this matters to investors.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(prompt_json))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7ad88",
   "metadata": {},
   "source": [
    "## 2. Grounding: Using the Document vs. Hallucinating\n",
    "\n",
    "We'll contrast:\n",
    "\n",
    "- asking a generic question with *no* context\n",
    "\n",
    "- vs. forcing the model to use the **specific document** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a72d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Grounding demo: with vs without document\n",
    "\n",
    "question = \"What regulatory risks are mentioned?\"\n",
    "\n",
    "# 1) No context â€“ model may hallucinate\n",
    "print(\"=== NO CONTEXT ===\")\n",
    "print(llm_text(question))\n",
    "\n",
    "# 2) With context â€“ grounded answer\n",
    "prompt_grounded = f\"\"\"\n",
    "You MUST answer only using the document below.\n",
    "If the document does not mention a risk type, say \"Not specified in the document.\"\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Question:\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== WITH DOCUMENT (GROUNDED) ===\")\n",
    "print(llm_text(prompt_grounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca39b9",
   "metadata": {},
   "source": [
    "### ðŸ‘‰ Student TODO\n",
    "\n",
    "Write a prompt that:\n",
    "\n",
    "- Uses **only** the document.\n",
    "\n",
    "- Extracts all sentences related to \"competition\".\n",
    "\n",
    "- Returns a list of sentences in JSON: `{\"sentences\": [\"...\", \"...\"]}`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa77c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3. Text Tools: simple Python functions over finance text\n",
    "\n",
    "import re\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very naive sentence splitter, good enough for workshop demo.\n",
    "    \"\"\"\n",
    "    # split on '.', '!' or '?'\n",
    "    parts = re.split(r\"[.!?]\\s+\", text.strip())\n",
    "    # filter empty\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def find_sentences_with_keyword(text: str, keyword: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return sentences that contain the keyword (case-insensitive).\n",
    "    \"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    sentences = split_sentences(text)\n",
    "    return [s for s in sentences if keyword_lower in s.lower()]\n",
    "\n",
    "def find_risk_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very naive: sentences mentioning 'risk', 'uncertain', 'may not', etc.\n",
    "    \"\"\"\n",
    "    patterns = [\"risk\", \"uncertain\", \"may not\", \"could\", \"volatility\"]\n",
    "    sentences = split_sentences(text)\n",
    "    hits = []\n",
    "    for s in sentences:\n",
    "        if any(p in s.lower() for p in patterns):\n",
    "            hits.append(s)\n",
    "    return hits\n",
    "\n",
    "# quick test\n",
    "print(find_risk_sentences(DOCS[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Use tool results inside a prompt\n",
    "\n",
    "sentences = find_risk_sentences(text)\n",
    "\n",
    "tool_prompt = f\"\"\"\n",
    "You are an analyst assistant.\n",
    "\n",
    "I have extracted these 'risk-related' sentences from a filing:\n",
    "\n",
    "{chr(10).join('- ' + s for s in sentences)}\n",
    "\n",
    "Task:\n",
    "\n",
    "- Summarize the main 2â€“3 categories of risk.\n",
    "\n",
    "- One short bullet per category.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(tool_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9831079",
   "metadata": {},
   "source": [
    "## 4. Building an Agent with LangGraph\n",
    "\n",
    "We'll use:\n",
    "\n",
    "- LangChain's `ChatOpenAI` as the model wrapper.\n",
    "\n",
    "- LangChain `@tool` decorators for Python tools.\n",
    "\n",
    "- LangGraph's `StateGraph` to define the agent workflow.\n",
    "\n",
    "The agent will:\n",
    "\n",
    "1. Receive a user question + a document.\n",
    "\n",
    "2. LLM decides: answer directly vs call a tool.\n",
    "\n",
    "3. If tool is called, we run the Python function.\n",
    "\n",
    "4. LLM uses the tool result to produce the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% LangGraph setup: model + tools\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# LangChain chat model using OpenAI\n",
    "lc_model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# --- Define tools using the same logic as before ---\n",
    "\n",
    "@tool\n",
    "def extract_sentences_with_keyword(text: str, keyword: str) -> List[str]:\n",
    "    \"\"\"Extract sentences from the given document that contain the given keyword (case-insensitive).\"\"\"\n",
    "    return find_sentences_with_keyword(text, keyword)\n",
    "\n",
    "@tool\n",
    "def extract_risk_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Extract sentences that likely mention risk (e.g., 'risk', 'uncertain', 'may not', 'volatility').\"\"\"\n",
    "    return find_risk_sentences(text)\n",
    "\n",
    "tools = [extract_sentences_with_keyword, extract_risk_sentences]\n",
    "tools_by_name = {t.name: t for t in tools}\n",
    "\n",
    "# Bind tools to the model so it can choose to call them\n",
    "model_with_tools = lc_model.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define agent state for LangGraph\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    llm_calls: int\n",
    "    # You can add more fields (e.g., doc_id), but keep it minimal for the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model node: LLM decides whether to call a tool\n",
    "\n",
    "def llm_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    LLM node:\n",
    "\n",
    "    - Receives current messages (including the user's question and document).\n",
    "\n",
    "    - Decides whether to call a tool.\n",
    "\n",
    "    - Returns a new LLM message (could include tool calls).\n",
    "    \"\"\"\n",
    "    system_msg = SystemMessage(\n",
    "        content=(\n",
    "            \"You are an NLP assistant for finance documents. \"\n",
    "            \"You may call tools to extract sentences. \"\n",
    "            \"Use tools when the user asks you to 'find', 'extract', or 'highlight' sentences. \"\n",
    "            \"Otherwise, answer directly. \"\n",
    "            \"Always stay grounded in the document content.\"\n",
    "        )\n",
    "    )\n",
    "    result = model_with_tools.invoke([system_msg] + state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"llm_calls\": state.get(\"llm_calls\", 0) + 1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa07ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Tool node: execute any tool calls\n",
    "\n",
    "def tool_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute any tool calls requested by the last LLM message\n",
    "    and return ToolMessages with the observations.\n",
    "    \"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    results: List[ToolMessage] = []\n",
    "\n",
    "    if not getattr(last_msg, \"tool_calls\", None):\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    for tool_call in last_msg.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool = tools_by_name[tool_name]\n",
    "        observation = tool.invoke(tool_args)\n",
    "        # Wrap observation in a ToolMessage\n",
    "        results.append(\n",
    "            ToolMessage(\n",
    "                content=str(observation),\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\"messages\": results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Routing logic: should we call a tool or stop?\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"\n",
    "    Decide whether to continue to the tool node or end.\n",
    "\n",
    "    If the last LLM message has tool_calls, we go to tool_node.\n",
    "\n",
    "    Otherwise, we end (and return the answer).\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last = messages[-1]\n",
    "    if getattr(last, \"tool_calls\", None):\n",
    "        return \"tool_node\"\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Build and compile the LangGraph agent\n",
    "\n",
    "agent_builder = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_node\", llm_node)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Edges\n",
    "agent_builder.add_edge(START, \"llm_node\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_node\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END],\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_node\")\n",
    "\n",
    "# Compile\n",
    "agent = agent_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper to run a single agent turn\n",
    "\n",
    "def run_agent(question: str, document: str):\n",
    "    \"\"\"\n",
    "    Prepare messages and invoke the LangGraph agent.\n",
    "    \"\"\"\n",
    "    user_content = (\n",
    "        f\"Here is a finance document:\\n\\n{document}\\n\\n\"\n",
    "        f\"User question: {question}\"\n",
    "    )\n",
    "    initial_state: AgentState = {\n",
    "        \"messages\": [HumanMessage(content=user_content)],\n",
    "        \"llm_calls\": 0,\n",
    "    }\n",
    "\n",
    "    final_state = agent.invoke(initial_state)\n",
    "\n",
    "    print(\"=== Final messages ===\")\n",
    "    for m in final_state[\"messages\"]:\n",
    "        # pretty_print is available but we can just print content\n",
    "        print(f\"[{m.type}] {getattr(m, 'content', m)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d94c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Try out the agent\n",
    "\n",
    "doc = DOCS[1]\n",
    "\n",
    "questions = [\n",
    "    \"Summarize the main risks for investors.\",\n",
    "    \"Find sentences that mention regulation or regulatory changes.\",\n",
    "    \"Highlight any forward-looking statements or guidance.\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(\"Question:\", q)\n",
    "    run_agent(q, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075b9d5",
   "metadata": {},
   "source": [
    "## 5. Extensions / Experiments\n",
    "\n",
    "ðŸ‘‰ Ideas for students:\n",
    "\n",
    "1. Add a new tool, e.g.:\n",
    "\n",
    "   - `extract_forward_looking(text: str)` â€” sentences with \"expect\", \"anticipate\", \"guidance\", \"forecast\".\n",
    "\n",
    "2. Modify the system prompt in `llm_node` so the model:\n",
    "\n",
    "   - Uses the new tool when appropriate.\n",
    "\n",
    "   - Explains when it *chose not* to call a tool.\n",
    "\n",
    "3. Add a constraint:\n",
    "\n",
    "   - \"If you are unsure, say 'Not specified in the document.'\"\n",
    "\n",
    "   and observe how behavior changes.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
