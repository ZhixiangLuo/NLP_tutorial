{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096601b3",
   "metadata": {},
   "source": [
    "# NLP Exerciseâ€” LLMs, Prompts, and Agents \n",
    "\n",
    "**Hands-on goals (1.5h):**\n",
    "\n",
    "1. Interact with LLM\n",
    "\n",
    "2. Basic chatbot\n",
    "\n",
    "3. LLM capabilities and limitations\n",
    "\n",
    "4. Prompt engineering\n",
    "\n",
    "5. Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75e4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "## Silence the annoying warnings\n",
    "# 1) Python warnings (UserWarning, DeprecationWarning, etc.)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2) gRPC native logs (ALTS, channelz, etc.)\n",
    "os.environ[\"GRPC_VERBOSITY\"] = \"NONE\"\n",
    "os.environ[\"GRPC_TRACE\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d233f",
   "metadata": {},
   "source": [
    "# 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeee7d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt --quiet\n",
    "!pip uninstall jupyterlab --yes --quiet\n",
    "!pip install jupyterlab==3.6.8  --quiet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0029887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# We'll use LangChain + LangGraph for the agent\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Make sure GOOGLE_API_KEY is set in .env file or environment\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "assert api_key, \"Please set GOOGLE_API_KEY in your .env file or environment.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d69f2-0dfd-43be-9549-9ba7aa409131",
   "metadata": {},
   "source": [
    "# 2. Interact with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d9a2f7-1704-40fd-b6e3-1aa3d33315c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# initiate the LLM model, use model=\"gemini-2.5-flash\" as example\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "def safe_llm_invoke(messages, max_retries=5, base_delay=1.0):\n",
    "    \"\"\"Invoke LLM with retries on transient errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = llm.invoke(messages)\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            # Last attempt â†’ re-raise\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            # Exponential backoff\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "            print(f\"Retry {attempt+1}/{max_retries} after error: {e}\")\n",
    "            time.sleep(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94612b7-3988-414a-b98c-4663e94dd08c",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "Ask LLM standalone questions.  \n",
    "For example: \n",
    "- safe_llm_invoke('who are you?')\n",
    "- safe_llm_invoke('what is your capability?')\n",
    "- safe_llm_invoke('what is the previous question we discussed?')\n",
    "  \n",
    "There is no memory, so LLM doesn't know what has been discussed and you can't ask follow up questions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005f814-525c-4fb3-b717-da69cfa81113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised PermissionDenied: 403 Your API key was reported as leaked. Please use another API key..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised PermissionDenied: 403 Your API key was reported as leaked. Please use another API key..\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised PermissionDenied: 403 Your API key was reported as leaked. Please use another API key..\n"
     ]
    }
   ],
   "source": [
    "resp = safe_llm_invoke('what is the previous question we discussed?')\n",
    "display(Markdown(f\"<p style='margin:2px 0'>{resp.content}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688e018-d498-459b-94eb-addfa393467c",
   "metadata": {},
   "source": [
    "# 3. Simple Chatbot\n",
    "Save the conversation to history, pass the whole history to LLM, now you have a **chatbot**!\n",
    "\n",
    "### Sample questions:\n",
    "- Hi\n",
    "- Who are you?\n",
    "- What is your capability and limitation?\n",
    "- Answer it within 200 words\n",
    "\n",
    "### Caution\n",
    "1. Type \"exit\", \"quit\" or \"q\" to quit the chat\n",
    "2. If you can't type o as it will fold/unfold the result area, try **capital O**, LLM don't mind it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# STREAMING CHAT UI\n",
    "###############################################################################\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import markdown\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# LLM SETUP\n",
    "# ---------------------------------------------------------------------------\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# SAFE STREAM (with retry + exponential backoff)\n",
    "# ---------------------------------------------------------------------------\n",
    "def safe_stream(messages, max_retries=5, base_delay=1.0):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return llm.stream(messages)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            delay = base_delay * (2 ** attempt)\n",
    "            print(f\"[Retry {attempt+1}/{max_retries}] {e}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# UI COMPONENTS\n",
    "# ---------------------------------------------------------------------------\n",
    "output_area = widgets.Output(\n",
    "    layout={\n",
    "        \"border\": \"1px solid #ddd\",\n",
    "        \"height\": \"300px\",\n",
    "        \"width\": \"100%\",\n",
    "        \"overflow_y\": \"auto\",\n",
    "        \"padding\": \"10px\"\n",
    "    }\n",
    ")\n",
    "\n",
    "input_box = widgets.Text(\n",
    "    placeholder=\"Type your message and press Enterâ€¦\",\n",
    "    layout={'width': '100%'}\n",
    ")\n",
    "\n",
    "history = []\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# HANDLER: triggered when user presses Enter\n",
    "# ---------------------------------------------------------------------------\n",
    "def on_send(text_widget):\n",
    "\n",
    "    text = text_widget.value\n",
    "    if not text:\n",
    "        return\n",
    "\n",
    "    text_widget.value = \"\"   # clear UI input\n",
    "\n",
    "    # Show user's message\n",
    "    with output_area:\n",
    "        display(Markdown(\n",
    "            f\"<div style='margin:0; padding:0; line-height:1.0;'>&nbsp;&nbsp;<b>You:</b> {text}</div>\"\n",
    "        ))\n",
    "\n",
    "    history.append(HumanMessage(content=text))\n",
    "\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # STREAM CHATBOT RESPONSE (Markdown live update)\n",
    "    # -----------------------------------------------------------------------\n",
    "    with output_area:\n",
    "        # Initial header\n",
    "        bubble = widgets.HTML(\n",
    "            value=\"<b>Chatbot:</b> \",\n",
    "            layout={\"overflow_y\": \"auto\", \"max_height\": \"300px\", \"padding\": \"6px\"}\n",
    "        )\n",
    "        display(bubble)\n",
    "\n",
    "    chunks = safe_stream(history)\n",
    "    full = \"\"\n",
    "\n",
    "    for chunk in chunks:\n",
    "        token = chunk.content or \"\"\n",
    "        full += token\n",
    "        html_content = markdown.markdown(full)\n",
    "        # force inline display for paragraph output\n",
    "        html_content = html_content.replace(\"<p>\", \"<span>\").replace(\"</p>\", \"</span>\")\n",
    "\n",
    "\n",
    "    # Update bubble with HTML-rendered content\n",
    "        bubble.value = (\n",
    "            \"<b>Chatbot:</b>\"\n",
    "            \"<div style='margin-left:40px'>\"\n",
    "            f\"<div style='line-height:1.2; margin:0; padding:0'>\"\n",
    "            f\"{html_content}\"\n",
    "            \"</div>\"\n",
    "            \"</div>\"\n",
    "        )\n",
    "\n",
    "    history.append(AIMessage(content=full))\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CONNECT ENTER KEY TO HANDLER\n",
    "# ---------------------------------------------------------------------------\n",
    "input_box.on_submit(on_send)\n",
    "\n",
    "# Show Chat UI\n",
    "display(output_area)\n",
    "display(input_box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974b83b-1459-4e87-b3a7-28362d87f2bc",
   "metadata": {},
   "source": [
    "#### You can inspect what gets saved in history.\n",
    "All the history is passed to `ChatGoogleGenerativeAI`, but only the actual\n",
    "`content` strings are sent to the LLM.\n",
    "\n",
    "LangChain converts messages into Geminiâ€™s expected schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"contents\": [\n",
    "    {\"role\": \"user\",  \"parts\": [{\"text\": \"User text...\"}]},\n",
    "    {\"role\": \"model\", \"parts\": [{\"text\": \"Model reply...\"}]},\n",
    "    {\"role\": \"user\",  \"parts\": [{\"text\": \"Next user message...\"}]}\n",
    "  ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45fe38-bd3e-4603-9dfe-8648df60f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history[0], history[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd1a36f",
   "metadata": {},
   "source": [
    "# 4. Test LLM capability and limitation\n",
    "- Does it know the current time? It only has knowledge from the data it is trained on. So no up to date information\n",
    "  \n",
    "- Test it on a domain you know well, how does the LLM perform?\n",
    "- What is my name? Where am I located? LLM has no private memory\n",
    "- try â€œSummarize the book â€˜The Yellow Star Algorithmâ€™.â€ (It doesnâ€™t exist.) LLM may hallucinate and make up things. It is a statistical model, has no sense of right or wrong, will generate output for whatever input based on its algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    history = []\n",
    "    while True:\n",
    "        user = input(\"You: \")\n",
    "        if user.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "            break\n",
    "\n",
    "        history.append(HumanMessage(content=user))\n",
    "        resp = safe_llm_invoke(history)\n",
    "        display(Markdown(f\"**Chatbot:** {resp.content}\"))\n",
    "        history.append(resp)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d46aae",
   "metadata": {},
   "source": [
    "# 5 Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcbde88",
   "metadata": {},
   "source": [
    "## 1. Prompt Engineering Basics: Role & Style\n",
    "\n",
    "In this section:\n",
    "\n",
    "- Call the LLM directly.\n",
    "\n",
    "- Change **role** and **style** instructions.\n",
    "\n",
    "- See how answers differ for the same question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prompt engineering: role & style\n",
    "\n",
    "text = DOCS[0]\n",
    "\n",
    "base_question = \"What risks should an equity investor pay attention to in this document?\"\n",
    "\n",
    "prompt_equity = f\"\"\"\n",
    "You are a senior equity research analyst.\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Task:\n",
    "\n",
    "- Answer the question below in 3 bullet points.\n",
    "\n",
    "- Be concise and use plain English.\n",
    "\n",
    "Question:\n",
    "\n",
    "{base_question}\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(prompt_equity))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656bbd71",
   "metadata": {},
   "source": [
    "### ðŸ‘‰ Student TODO\n",
    "\n",
    "1. Copy the previous cell.\n",
    "\n",
    "2. Change the **role** and **audience**, e.g.:\n",
    "\n",
    "   - \"You are a chief risk officer.\"\n",
    "\n",
    "   - \"Explain to a first-year finance student.\"\n",
    "\n",
    "3. Run and compare the tone and focus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1460a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Prompt engineering: structured JSON output\n",
    "\n",
    "prompt_json = f\"\"\"\n",
    "You are an analyst. Extract *up to 5* key risks from the document.\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Return a JSON object with this format only:\n",
    "\n",
    "{{\n",
    "  \"risks\": [\n",
    "    {{\"label\": \"...\", \"quote\": \"...\", \"reason\": \"...\"}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "- \"label\": short category of the risk (e.g., \"interest rate risk\").\n",
    "\n",
    "- \"quote\": one short snippet from the document.\n",
    "\n",
    "- \"reason\": why this matters to investors.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(prompt_json))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7ad88",
   "metadata": {},
   "source": [
    "## 2. Grounding: Using the Document vs. Hallucinating\n",
    "\n",
    "We'll contrast:\n",
    "\n",
    "- asking a generic question with *no* context\n",
    "\n",
    "- vs. forcing the model to use the **specific document** only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a72d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Grounding demo: with vs without document\n",
    "\n",
    "question = \"What regulatory risks are mentioned?\"\n",
    "\n",
    "# 1) No context â€“ model may hallucinate\n",
    "print(\"=== NO CONTEXT ===\")\n",
    "print(llm_text(question))\n",
    "\n",
    "# 2) With context â€“ grounded answer\n",
    "prompt_grounded = f\"\"\"\n",
    "You MUST answer only using the document below.\n",
    "If the document does not mention a risk type, say \"Not specified in the document.\"\n",
    "\n",
    "Document:\n",
    "\n",
    "{text}\n",
    "\n",
    "Question:\n",
    "\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== WITH DOCUMENT (GROUNDED) ===\")\n",
    "print(llm_text(prompt_grounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca39b9",
   "metadata": {},
   "source": [
    "### ðŸ‘‰ Student TODO\n",
    "\n",
    "Write a prompt that:\n",
    "\n",
    "- Uses **only** the document.\n",
    "\n",
    "- Extracts all sentences related to \"competition\".\n",
    "\n",
    "- Returns a list of sentences in JSON: `{\"sentences\": [\"...\", \"...\"]}`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa77c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3. Text Tools: simple Python functions over finance text\n",
    "\n",
    "import re\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very naive sentence splitter, good enough for workshop demo.\n",
    "    \"\"\"\n",
    "    # split on '.', '!' or '?'\n",
    "    parts = re.split(r\"[.!?]\\s+\", text.strip())\n",
    "    # filter empty\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def find_sentences_with_keyword(text: str, keyword: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Return sentences that contain the keyword (case-insensitive).\n",
    "    \"\"\"\n",
    "    keyword_lower = keyword.lower()\n",
    "    sentences = split_sentences(text)\n",
    "    return [s for s in sentences if keyword_lower in s.lower()]\n",
    "\n",
    "def find_risk_sentences(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Very naive: sentences mentioning 'risk', 'uncertain', 'may not', etc.\n",
    "    \"\"\"\n",
    "    patterns = [\"risk\", \"uncertain\", \"may not\", \"could\", \"volatility\"]\n",
    "    sentences = split_sentences(text)\n",
    "    hits = []\n",
    "    for s in sentences:\n",
    "        if any(p in s.lower() for p in patterns):\n",
    "            hits.append(s)\n",
    "    return hits\n",
    "\n",
    "# quick test\n",
    "print(find_risk_sentences(DOCS[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Use tool results inside a prompt\n",
    "\n",
    "sentences = find_risk_sentences(text)\n",
    "\n",
    "tool_prompt = f\"\"\"\n",
    "You are an analyst assistant.\n",
    "\n",
    "I have extracted these 'risk-related' sentences from a filing:\n",
    "\n",
    "{chr(10).join('- ' + s for s in sentences)}\n",
    "\n",
    "Task:\n",
    "\n",
    "- Summarize the main 2â€“3 categories of risk.\n",
    "\n",
    "- One short bullet per category.\n",
    "\"\"\"\n",
    "\n",
    "print(llm_text(tool_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9831079",
   "metadata": {},
   "source": [
    "# 6. Building an Agent with LangGraph\n",
    "\n",
    "We'll use:\n",
    "\n",
    "- LangChain's `ChatGoogleGenerativeAI` as the model wrapper.\n",
    "\n",
    "- LangChain `@tool` decorators for Python tools.\n",
    "\n",
    "- LangGraph's `StateGraph` to define the agent workflow.\n",
    "\n",
    "The agent will:\n",
    "\n",
    "1. Receive a user question + a document.\n",
    "\n",
    "2. LLM decides: answer directly vs call a tool.\n",
    "\n",
    "3. If tool is called, we run the Python function.\n",
    "\n",
    "4. LLM uses the tool result to produce the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3b53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% LangGraph setup: model + tools\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "# LangChain chat model using Google Gemini\n",
    "lc_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# --- Define tools using the same logic as before ---\n",
    "\n",
    "@tool\n",
    "def extract_sentences_with_keyword(text: str, keyword: str) -> List[str]:\n",
    "    \"\"\"Extract sentences from the given document that contain the given keyword (case-insensitive).\"\"\"\n",
    "    return find_sentences_with_keyword(text, keyword)\n",
    "\n",
    "@tool\n",
    "def extract_risk_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Extract sentences that likely mention risk (e.g., 'risk', 'uncertain', 'may not', 'volatility').\"\"\"\n",
    "    return find_risk_sentences(text)\n",
    "\n",
    "tools = [extract_sentences_with_keyword, extract_risk_sentences]\n",
    "tools_by_name = {t.name: t for t in tools}\n",
    "\n",
    "# Bind tools to the model so it can choose to call them\n",
    "model_with_tools = lc_model.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e0c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define agent state for LangGraph\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    llm_calls: int\n",
    "    # You can add more fields (e.g., doc_id), but keep it minimal for the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Model node: LLM decides whether to call a tool\n",
    "\n",
    "def llm_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    LLM node:\n",
    "\n",
    "    - Receives current messages (including the user's question and document).\n",
    "\n",
    "    - Decides whether to call a tool.\n",
    "\n",
    "    - Returns a new LLM message (could include tool calls).\n",
    "    \"\"\"\n",
    "    system_msg = SystemMessage(\n",
    "        content=(\n",
    "            \"You are an NLP assistant for finance documents. \"\n",
    "            \"You may call tools to extract sentences. \"\n",
    "            \"Use tools when the user asks you to 'find', 'extract', or 'highlight' sentences. \"\n",
    "            \"Otherwise, answer directly. \"\n",
    "            \"Always stay grounded in the document content.\"\n",
    "        )\n",
    "    )\n",
    "    result = model_with_tools.invoke([system_msg] + state[\"messages\"])\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"llm_calls\": state.get(\"llm_calls\", 0) + 1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa07ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Tool node: execute any tool calls\n",
    "\n",
    "def tool_node(state: AgentState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Execute any tool calls requested by the last LLM message\n",
    "    and return ToolMessages with the observations.\n",
    "    \"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    results: List[ToolMessage] = []\n",
    "\n",
    "    if not getattr(last_msg, \"tool_calls\", None):\n",
    "        return {\"messages\": []}\n",
    "\n",
    "    for tool_call in last_msg.tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool = tools_by_name[tool_name]\n",
    "        observation = tool.invoke(tool_args)\n",
    "        # Wrap observation in a ToolMessage\n",
    "        results.append(\n",
    "            ToolMessage(\n",
    "                content=str(observation),\n",
    "                tool_call_id=tool_call[\"id\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return {\"messages\": results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Routing logic: should we call a tool or stop?\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"\n",
    "    Decide whether to continue to the tool node or end.\n",
    "\n",
    "    If the last LLM message has tool_calls, we go to tool_node.\n",
    "\n",
    "    Otherwise, we end (and return the answer).\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last = messages[-1]\n",
    "    if getattr(last, \"tool_calls\", None):\n",
    "        return \"tool_node\"\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Build and compile the LangGraph agent\n",
    "\n",
    "agent_builder = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_node\", llm_node)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Edges\n",
    "agent_builder.add_edge(START, \"llm_node\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_node\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END],\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_node\")\n",
    "\n",
    "# Compile\n",
    "agent = agent_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0232ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Helper to run a single agent turn\n",
    "\n",
    "def run_agent(question: str, document: str):\n",
    "    \"\"\"\n",
    "    Prepare messages and invoke the LangGraph agent.\n",
    "    \"\"\"\n",
    "    user_content = (\n",
    "        f\"Here is a finance document:\\n\\n{document}\\n\\n\"\n",
    "        f\"User question: {question}\"\n",
    "    )\n",
    "    initial_state: AgentState = {\n",
    "        \"messages\": [HumanMessage(content=user_content)],\n",
    "        \"llm_calls\": 0,\n",
    "    }\n",
    "\n",
    "    final_state = agent.invoke(initial_state)\n",
    "\n",
    "    print(\"=== Final messages ===\")\n",
    "    for m in final_state[\"messages\"]:\n",
    "        # pretty_print is available but we can just print content\n",
    "        print(f\"[{m.type}] {getattr(m, 'content', m)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d94c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Try out the agent\n",
    "\n",
    "doc = DOCS[1]\n",
    "\n",
    "questions = [\n",
    "    \"Summarize the main risks for investors.\",\n",
    "    \"Find sentences that mention regulation or regulatory changes.\",\n",
    "    \"Highlight any forward-looking statements or guidance.\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(\"\\n\" + \"#\" * 80)\n",
    "    print(\"Question:\", q)\n",
    "    run_agent(q, doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075b9d5",
   "metadata": {},
   "source": [
    "# 7. Extensions / Experiments\n",
    "\n",
    "- AI tool use: ChatGPT, Gemini\n",
    "- Vibe coding: Cursor\n",
    "- AI browser: ChatGPT Atlas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa6f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
