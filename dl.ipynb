{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b2244f",
   "metadata": {},
   "source": [
    "# Deep Learning Packages in Python: A Financial Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b11418",
   "metadata": {},
   "source": [
    "## 1. Lecture Overview & Learning Objectives \n",
    "\n",
    "Welcome to this lecture on Deep Learning (DL) packages in Python! In the realm of finance, deep learning is rapidly transforming how we approach complex problems, from predicting market movements and managing risk to understanding sentiment and optimizing trading strategies. Python, with its rich ecosystem of libraries, has become the de facto language for implementing these advanced models.\n",
    "\n",
    "### Why is this relevant for Finance Students?\n",
    "\n",
    "*   **Harnessing Complexity:** Financial markets are inherently non-linear and complex. DL models, with their ability to learn intricate patterns, are well-suited to capture these dynamics where traditional models might fall short.\n",
    "*   **Big Data Analytics:** The financial world generates vast amounts of data (tick data, news feeds, social media, macroeconomic indicators). DL provides powerful tools to process and extract value from this scale of data.\n",
    "*   **Competitive Edge:** Proficiency in these tools is becoming a critical skill for quantitative analysts, portfolio managers, and risk managers. Understanding which tool to use for which task can provide a significant competitive advantage.\n",
    "\n",
    "### Learning Objectives:\n",
    "By the end of this lecture, you should be able to:\n",
    "1.  Identify the major deep learning packages in Python: **TensorFlow/Keras, PyTorch, and JAX**.\n",
    "2.  Understand the core philosophy, strengths, and weaknesses of each package.\n",
    "3.  Implement basic deep learning models using TensorFlow/Keras and PyTorch.\n",
    "4.  Appreciate JAX's unique capabilities for high-performance numerical computing and research.\n",
    "5.  Make informed decisions on choosing the appropriate DL framework for specific financial applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175ddd5",
   "metadata": {},
   "source": [
    "## 2. Introduction to Deep Learning in Python \n",
    "\n",
    "Deep Learning is a subset of machine learning inspired by the structure and function of the human brain's neural networks. It involves training artificial neural networks with many layers (hence \"deep\") on vast amounts of data to learn complex representations and patterns.\n",
    "\n",
    "### Key Concepts:\n",
    "*   **Neural Networks:** Composed of interconnected nodes (neurons) organized in layers.\n",
    "*   **Layers:** Input, Hidden (multiple), Output.\n",
    "*   **Activation Functions:** Introduce non-linearity (e.g., ReLU, Sigmoid, Tanh).\n",
    "*   **Loss Function:** Measures how well the model is performing (e.g., MSE for regression, Cross-Entropy for classification).\n",
    "*   **Optimizer:** Adjusts model weights to minimize the loss (e.g., Adam, SGD).\n",
    "*   **Backpropagation:** Algorithm to efficiently calculate gradients and update weights.\n",
    "\n",
    "### Major Deep Learning Packages in Python:\n",
    "\n",
    "Today, we will focus on the three giants in the Python deep learning ecosystem:\n",
    "\n",
    "1.  **TensorFlow / Keras:** Google's powerful and comprehensive ecosystem. Keras is its user-friendly high-level API.\n",
    "2.  **PyTorch:** Developed by Facebook AI Research (FAIR), known for its flexibility and Pythonic interface.\n",
    "3.  **JAX:** Google's newer library for high-performance numerical computing and machine learning research, often used for custom models and advanced optimizations.\n",
    "\n",
    "Each of these frameworks has its strengths and ideal use cases. Understanding them will empower you to choose the right tool for your financial modeling needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4969c253-92a8-40c0-8e94-7ab2656b716b",
   "metadata": {},
   "source": [
    "## Fundamental Concepts:\n",
    "\n",
    "### 1. Neural Networks\n",
    "\n",
    "**What it is:**\n",
    "A **Neural Network** (NN) is a computational model inspired by the structure and function of the human brain. It's composed of numerous interconnected nodes, often called \"neurons\" or \"perceptrons,\" organized into layers. Each connection between neurons has an associated \"weight,\" representing the strength or importance of that connection.\n",
    "\n",
    "| Feature | Human Brain | Artificial Neural Network |\n",
    "| :--- | :--- | :--- |\n",
    "| **Basic Unit** | Neuron | Node (Perceptron) |\n",
    "| **Signal** | Electrical Impulses | Floating-Point Numbers |\n",
    "| **Connections** | Synapses | Weights |\n",
    "| **Learning** | Adjusting Synaptic Strength (Plasticity) | Adjusting Weights (Optimization) |\n",
    "| **Structure** | Neural Networks & Regions | Layers (Input, Hidden, Output) |\n",
    "| **Speed** | Relatively Slow (milliseconds) but massively parallel | Extremely Fast (nanoseconds) but depends on hardware |\n",
    "| **Energy** | Highly efficient (~20 watts) | Very high power consumption (kilowatts) |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Why it's important:**\n",
    "Neural networks are designed to recognize patterns and make predictions. Their ability to learn complex, non-linear relationships makes them exceptionally powerful for tasks like fraud detection, algorithmic trading, credit scoring, and market sentiment analysis where traditional linear models might fall short.\n",
    "\n",
    "---\n",
    "### 2. Layers\n",
    "\n",
    "Neural networks are structured into distinct layers, each serving a specific purpose in processing the information flow:\n",
    "\n",
    "\n",
    "![Artificial Neural Network Layers](https://upload.wikimedia.org/wikipedia/commons/e/e4/Artificial_neural_network.svg \"Diagram illustrating the input, hidden, and output layers of a neural network\")\n",
    "*Image Source: Svjo / CC BY-SA 3.0*\n",
    "\n",
    "*   **Input Layer:**\n",
    "    *   **What it is:** This is the first layer of the network. Each neuron in the input layer corresponds to a feature in the input data. For example, if you're predicting stock prices, the input layer might represent features like opening price, closing price, volume, and various technical indicators.\n",
    "    *   **Why it's important:** It's the entry point for your data into the network. It receives the raw information the model will learn from.\n",
    "\n",
    "*   **Hidden Layers:**\n",
    "    *   **What it is:** These are the layers between the input and output layers. In \"deep\" learning, there are typically multiple hidden layers. Each neuron in a hidden layer processes information from the previous layer, applies a transformation, and passes the result to the next layer. These layers are \"hidden\" because their outputs are not directly visible as raw input or final output.\n",
    "    *   **Why it's important:** Hidden layers are where the network learns to extract increasingly abstract and complex features from the data. The more hidden layers (and neurons within them), the more complex patterns the network can potentially learn, which is crucial for handling highly non-linear relationships in financial time series or high-dimensional datasets.\n",
    "\n",
    "*   **Output Layer:**\n",
    "    *   **What it is:** This is the final layer of the network. The number of neurons in the output layer depends on the type of problem being solved. For a regression task (e.g., predicting a continuous stock price), it might have one neuron. For a classification task (e.g., predicting if a stock will go up or down, or classifying credit risk into categories), it would have one neuron per class.\n",
    "    *   **Why it's important:** It provides the network's final prediction or decision.\n",
    "\n",
    "---\n",
    "### 3. Activation Functions\n",
    "\n",
    "**What it is:**\n",
    "An **Activation Function** is applied to the output of each neuron in the hidden and output layers. Its primary role is to introduce non-linearity into the network. Without activation functions, a neural network, no matter how many layers it has, would essentially behave like a single linear regression model. Common activation functions include:\n",
    "\n",
    "\n",
    "\n",
    "*   **ReLU (Rectified Linear Unit):** `f(x) = max(0, x)`. Outputs the input directly if it's positive, otherwise outputs zero. It's computationally efficient and widely used.\n",
    "*   **Sigmoid:** `f(x) = 1 / (1 + e^(-x))`. Squashes values between 0 and 1, often used in binary classification output layers to represent probabilities.\n",
    "*   **Tanh (Hyperbolic Tangent):** `f(x) = (e^x - e^(-x)) / (e^x + e^(-x))`. Squashes values between -1 and 1, often preferred over Sigmoid in hidden layers because its output is zero-centered.\n",
    "\n",
    "**Why it's important:**\n",
    "Non-linearity is crucial because most real-world data, especially in finance, is non-linear. Activation functions allow neural networks to learn complex, non-linear relationships between inputs and outputs, enabling them to model intricate market dynamics that linear models cannot capture.\n",
    "\n",
    "---\n",
    "### 4. Loss Function\n",
    "\n",
    "**What it is:**\n",
    "A **Loss Function** (also known as a Cost Function or Objective Function) quantifies the discrepancy between the network's predicted output and the actual target output for a given input. A lower loss value indicates a better-performing model. Examples include:\n",
    "\n",
    "![Cost Function Diagram](https://upload.wikimedia.org/wikipedia/commons/5/5b/Gradient_descent_method.png \"A 2D representation of a cost function with a global minimum\")\n",
    "*Image Source:wikimedia.org*\n",
    "\n",
    "*   **Mean Squared Error (MSE):** Commonly used for regression tasks (e.g., predicting exact stock prices). It calculates the average of the squared differences between predictions and actual values.\n",
    "*   **Cross-Entropy:** Commonly used for classification tasks (e.g., predicting if a stock will rise or fall, or which credit risk category a borrower belongs to). It measures the dissimilarity between predicted probability distributions and true distributions.\n",
    "\n",
    "**Why it's important:**\n",
    "The loss function is the critical metric that guides the learning process. The goal of training a neural network is to minimize this loss, effectively making the model's predictions as close as possible to the true values.\n",
    "\n",
    "---\n",
    "## 5. Optimizer\n",
    "\n",
    "**What it is:**\n",
    "An **Optimizer** is an algorithm or method used to adjust the weights and biases of the neural network in such a way that the loss function is minimized. It uses the gradients calculated during backpropagation to determine how to update the network's parameters. Popular optimizers include:\n",
    "\n",
    "\n",
    "*   **Adam (Adaptive Moment Estimation):** A very popular and generally effective optimizer that combines concepts from other optimizers like RMSprop and AdaGrad, adapting learning rates for each parameter.\n",
    "*   **SGD (Stochastic Gradient Descent):** A fundamental optimizer that updates weights iteratively based on the gradient of the loss function with respect to the weights for a small batch of data.\n",
    "\n",
    "**Why it's important:**\n",
    "The optimizer dictates \"how\" the network learns. By intelligently adjusting weights, it navigates the complex landscape of the loss function to find the optimal set of parameters that yield the best performance. Without an effective optimizer, the network might fail to learn or converge slowly.\n",
    "\n",
    "---\n",
    "### 6. Backpropagation\n",
    "\n",
    "**What it is:**\n",
    "**Backpropagation** is a fundamental algorithm for training neural networks. It works by efficiently calculating the gradients of the loss function with respect to all the weights and biases in the network. This calculation starts from the output layer and propagates backward through the network, hence the name \"backpropagation.\"\n",
    "\n",
    "\n",
    "**Why it's important:**\n",
    "Backpropagation is the engine of learning in deep neural networks. It provides the optimizer with the necessary information (the gradients) to understand how each weight and bias contributes to the overall error. This allows the optimizer to make informed adjustments to the network's parameters, enabling the network to learn from its mistakes and improve its predictive accuracy over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb73da7-0f0a-4ea1-a574-b926ea0e2ab2",
   "metadata": {},
   "source": [
    "##  Hiking Analogy\n",
    "\n",
    "Let's use an analogy to make it crystal clear.\n",
    "\n",
    "Imagine you are a hiker standing on a huge, foggy mountain range. Your goal is to find the absolute lowest point in the valley (this is the **minimum error**).\n",
    "\n",
    "*   **The Mountain Range:** This is your \"loss landscape.\" Every point on the landscape represents a specific set of weights for your neural network, and the altitude at that point is the error (how \"wrong\" the network is with those weights).\n",
    "*   **Your Goal:** Get to the bottom of the deepest valley.\n",
    "\n",
    "Now, let's define the roles of Backpropagation, SGD, and Adam.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Backpropagation: The Compass and Slope Calculator\n",
    "\n",
    "Backpropagation is not an optimizer. It's the **algorithm that tells you the direction and steepness of the slope where you are currently standing.**\n",
    "\n",
    "*   **What it does:** It calculates the **gradient** of the loss function with respect to every single weight in the network.\n",
    "*   **In our analogy:** It's like having a special tool that you can use at your current spot to figure out exactly which way is \"downhill\" and how steep the slope is in every direction. It doesn't move you; it just gives you the critical information: \"To go down, you must take 3 steps east, 1 step north, and 10 steps down.\"\n",
    "\n",
    "**Backpropagation is the engine that computes the gradients. It's the \"how-to\" for finding the slope.**\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. SGD (Stochastic Gradient Descent): The Basic Hiking Strategy\n",
    "\n",
    "SGD is the most basic **optimization algorithm**. It's a simple, direct strategy for using the slope information to actually move.\n",
    "\n",
    "*   **What it does:** It takes the gradient calculated by Backpropagation and takes a step in the opposite direction of the steepest slope. The size of this step is controlled by a parameter called the **learning rate**.\n",
    "*   **In our analogy:** SGD is the hiker's rule: \"Calculate the slope (using Backpropagation), then take a fixed-size step directly downhill.\" If the learning rate is high, you take a big leap. If it's low, you take a tiny, careful shuffle.\n",
    "\n",
    "**SGD is the \"how-to\" for moving, based on the slope.**\n",
    "\n",
    "**Limitations of SGD:** This basic strategy has problems. The terrain (loss landscape) is not a simple smooth bowl. It has:\n",
    "*   **Local Minima:** Small valleys that aren't the true lowest point. SGD can get stuck here.\n",
    "*   **Saddle Points:** Areas that are flat in one direction but steep in another. SGD can get stuck here too.\n",
    "*   **Noisy Terrain:** The path can be very erratic and zig-zag down a valley, making the journey slow.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Adam (Adaptive Moment Estimation): The Advanced, Smart Hiking Strategy\n",
    "\n",
    "Adam is a more sophisticated **optimization algorithm**. It uses the same slope information from Backpropagation but decides *how* to move in a much smarter way.\n",
    "\n",
    "Adam keeps track of two things as it hikes:\n",
    "1.  **Momentum (the first moment):** It averages the past few steps to maintain a direction. If you've been moving consistently east, it will keep you moving east, helping you power through small bumps and ravines (local minima).\n",
    "    *   **Analogy:** It's like a bowling ball rolling downhill. Its momentum helps it push past small obstacles.\n",
    "\n",
    "2.  **Adaptive Learning Rates (the second moment):** It keeps track of how large the gradients have been for each parameter (weight) individually. For weights that have large, noisy gradients, it takes smaller, more careful steps. For weights that have small, consistent gradients, it takes larger, more confident steps.\n",
    "    *   **Analogy:** Imagine you're hiking on a narrow, steep ridge (high gradient) for one parameter, and a wide, gentle slope (low gradient) for another. Adam lets you shuffle carefully along the ridge while taking big strides on the gentle slope, all at the same time.\n",
    "\n",
    "**Adam is the \"how-to\" for moving more efficiently and intelligently, based on the slope.**\n",
    "\n",
    "---\n",
    "\n",
    "#### Putting It All Together\n",
    "\n",
    "Here is the step-by-step process during one training iteration:\n",
    "\n",
    "1.  **Forward Pass:** The network makes a prediction.\n",
    "2.  **Calculate Loss:** You measure how wrong that prediction was (your \"altitude\").\n",
    "3.  **Backpropagation:** You use the loss to calculate the gradient for every weight in the network. **(This is the compass telling you the slope).**\n",
    "4.  **Optimizer (SGD or Adam) Takes Over:**\n",
    "    *   **If using SGD:** The optimizer looks at the gradient and says, \"Okay, let's take a step of size X directly opposite to this direction.\"\n",
    "    *   **If using Adam:** The optimizer looks at the gradient, checks its history of momentum and past gradient sizes, and says, \"Based on my momentum and the terrain here, I'll take this *specifically sized* step in this *general* downhill direction.\"\n",
    "5.  **Update Weights:** The optimizer updates the weights. The hiker has moved to a new position.\n",
    "6.  **Repeat:** This entire cycle is repeated thousands or millions of times until the hiker reaches the bottom of the valley."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c32f4b",
   "metadata": {},
   "source": [
    "## 3. TensorFlow & Keras: The Production Powerhouses \n",
    "\n",
    "### What is TensorFlow?\n",
    "TensorFlow is an open-source machine learning framework developed by Google. It's designed for numerical computation using data flow graphs, where nodes represent mathematical operations and edge represent multi-dimensional data arrays (tensors). It's highly scalable and designed for production environments, capable of running on various platforms from mobile to large-scale distributed systems.\n",
    "\n",
    "### What is Keras?\n",
    "Keras is a high-level API for building and training deep learning models. It runs on top of TensorFlow (and used to support others like Theano, CNTK). Keras was designed for fast experimentation, allowing users to go from idea to result with the fewest possible steps. It has become the standard high-level API for TensorFlow, making deep learning much more accessible.\n",
    "\n",
    "### Why use TensorFlow/Keras in Finance?\n",
    "\n",
    "*   **Robustness & Scalability:** Ideal for deploying complex models in real-time trading systems or large-scale risk simulations.\n",
    "*   **Comprehensive Ecosystem:** Tools like TensorBoard for visualization, TensorFlow Serving for model deployment, and TensorFlow Lite for edge devices.\n",
    "*   **Large Community & Resources:** Extensive documentation, tutorials, and a massive user base mean plenty of support and pre-trained models.\n",
    "*   **Ease of Use (Keras):** Quickly prototype and iterate on models for tasks like market prediction, sentiment analysis from financial news, or credit scoring.\n",
    "\n",
    "### Pros and Cons of TensorFlow/Keras\n",
    "\n",
    "| Aspect          | Pros                                                                                             | Cons                                                                                              |\n",
    "| :-------------- | :----------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------ |\n",
    "| **Ease of Use** | Keras API makes rapid prototyping very easy, clear and intuitive.                                | Lower-level TensorFlow can be complex; debugging can be less straightforward than PyTorch.       |\n",
    "| **Flexibility** | Supports both high-level (Keras) and low-level control. Functional API for complex architectures. | Can feel overly opinionated or restrictive for highly experimental research compared to PyTorch. |\n",
    "| **Performance** | Excellent performance on GPUs/TPUs. Static computation graph allows for strong optimizations.     | Initial overhead due to graph compilation.                                                        |\n",
    "| **Deployment**  | Industry-leading tools for production deployment (TF Serving, TF Lite, TF.js).                   |                                                                                                   |\n",
    "| **Community**   | Massive community, extensive documentation, and widespread industry adoption.                    |                                                                                                   |\n",
    "| **Finance Use** | Market prediction, algorithmic trading, fraud detection, credit risk modeling, time series analysis. |                                                                                                   |\n",
    "\n",
    "### How to use TensorFlow/Keras: A Simple Example (Stock Price Prediction)\n",
    "\n",
    "Let's build a simple Multi-Layer Perceptron (MLP) to predict the next day's stock price movement (up/down) based on previous day's features. We'll use a dummy dataset for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2be74e4-7e32-4e7a-a98c-d0809d2d54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbcd4dd-82fb-436d-9c87-9938adba782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "No GPU devices found. TensorFlow will run on CPU.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# 1. List all available physical GPU devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # 2. Configure GPU memory growth\n",
    "        # This prevents TensorFlow from pre-allocating all memory on the GPU.\n",
    "        # It will only allocate as much GPU memory as needed, and it grows dynamically.\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "        # You can also set a specific GPU to use if you have multiple\n",
    "        # For example, to use the first GPU:\n",
    "        # tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "\n",
    "        # Print confirmation\n",
    "        print(f\"GPUs available: {len(gpus)}. Memory growth is enabled for all GPUs.\")\n",
    "        print(\"Details:\", [gpu.name for gpu in gpus])\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "        print(\"TensorFlow will likely fall back to CPU or default GPU settings.\")\n",
    "else:\n",
    "    print(\"No GPU devices found. TensorFlow will run on CPU.\")\n",
    "\n",
    "\n",
    "\n",
    "# If you want to explicitly place operations on CPU or a specific GPU, you can use:\n",
    "# with tf.device('/CPU:0'):\n",
    "#     # Operations here will run on CPU\n",
    "#     cpu_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "#     print(\"CPU Tensor:\", cpu_tensor)\n",
    "\n",
    "# if gpus:\n",
    "#     with tf.device('/GPU:0'): # For the first GPU\n",
    "#         # Operations here will run on GPU:0\n",
    "#         gpu_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "#         print(\"GPU Tensor:\", gpu_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6434e6-d130-47b4-ab98-58ac9348ceef",
   "metadata": {},
   "source": [
    "### How Dropout Works\n",
    "\n",
    "Dropout is a regularization technique used in deep learning to prevent overfitting in neural networks. Overfitting occurs when a model performs exceptionally well on the training data but struggles to generalize to new, unseen data.\n",
    "\n",
    "\n",
    "1.  **During Training:**\n",
    "    *   In each training iteration, dropout randomly deactivates (or \"drops out\") a fraction of neurons in a layer, along with their connections.\n",
    "    *   This means that for a given training step, a temporary \"thinned\" network is created, as the dropped neurons do not contribute to the forward pass or backpropagation.\n",
    "    *   The \"dropout probability\" (often a hyperparameter between 0.2 and 0.5, with 0.2 being a good baseline) determines the chance of a neuron being dropped. This random deactivation is performed independently for each training example and each layer.\n",
    "    *   By randomly disabling neurons, the network is prevented from becoming overly reliant on specific neurons or co-adaptations between neurons. This forces the remaining neurons to learn more robust and generalized features.\n",
    "\n",
    "2.  **During Inference (Testing):**\n",
    "    *   In contrast to training, all neurons are active during testing or inference.\n",
    "    *   To account for the different network structure during training (where some neurons were dropped), the outgoing weights of the neurons are scaled down by the dropout rate (the probability *p* with which a unit was retained during training). This ensures that the expected output of a neuron remains the same as during training.\n",
    "\n",
    "### Benefits of Dropout\n",
    "\n",
    "*   **Prevents Overfitting:** This is the primary benefit. By randomly disabling neurons, the network cannot rely too heavily on specific connections, making it less likely to memorize the training data and improving its ability to generalize to new data.\n",
    "*   **Ensemble Effect:** Dropout can be viewed as training an ensemble of many smaller, \"thinned\" neural networks in parallel during each iteration. This ensemble effect leads to a more robust model and better generalization.\n",
    "*   **Enhanced Data Representation:** It introduces noise during training, which can help the network learn more effective data representations.\n",
    "*   **Computationally Efficient:** Dropout is relatively simple to implement and adds minimal computational overhead compared to its benefits in reducing overfitting.\n",
    "*   **Works well with large networks:** It is particularly effective in deep neural networks with many layers where overfitting is a common challenge.\n",
    "\n",
    "### Drawbacks of Dropout\n",
    "\n",
    "*   **Longer Training Times:** Due to its stochastic nature and the effective training of multiple sub-networks, dropout can increase the number of epochs required for the model to converge, leading to longer overall training durations.\n",
    "*   **Hyperparameter Tuning:** The dropout rate is a hyperparameter that requires careful tuning for optimal performance.\n",
    "*   **Optimization Complexity:** The exact reasons why dropout works are sometimes considered unclear, which can make optimization challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03f807b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1000, 10)\n",
      "y shape: (1000,)\n",
      "Target distribution: [865 135]\n",
      "X_train shape: (800, 10), y_train shape: (800,)\n",
      "X_test shape: (200, 10), y_test shape: (200,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │             \u001b[38;5;34m704\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,817</span> (11.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,817\u001b[0m (11.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,817</span> (11.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,817\u001b[0m (11.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the Keras model...\n",
      "Training complete.\n",
      "\n",
      "Evaluating the model...\n",
      "Test Loss: 0.1394\n",
      "Test Accuracy: 0.9150\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       173\n",
      "           1       0.67      0.74      0.70        27\n",
      "\n",
      "    accuracy                           0.92       200\n",
      "   macro avg       0.81      0.84      0.83       200\n",
      "weighted avg       0.92      0.92      0.92       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Data Generation (Dummy Financial Data)\n",
    "# Let's simulate some financial features and a target variable (stock movement: 0 = down/flat, 1 = up)\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "\n",
    "X = np.random.rand(num_samples, num_features) * 100  # e.g., technical indicators, volume, news sentiment scores\n",
    "# Create a 'noisy' relationship for the target variable\n",
    "y = (X[:, 0] * 0.5 + X[:, 1] * 0.3 - X[:, 2] * 0.2 + np.random.randn(num_samples) * 5 > 50).astype(int)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Target distribution: {np.bincount(y)}\")\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# 3. Model Definition (Keras Sequential API)\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(num_features,)),  # Input layer\n",
    "    keras.layers.Dense(64, activation='relu'),   # Hidden layer 1\n",
    "    keras.layers.Dropout(0.3),                   # Dropout for regularization\n",
    "    keras.layers.Dense(32, activation='relu'),   # Hidden layer 2\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # Output layer (sigmoid for binary classification)\n",
    "])\n",
    "\n",
    "# 4. Model Compilation\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 5. Model Training\n",
    "print(\"\\nTraining the Keras model...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,          # Number of passes over the training data\n",
    "    batch_size=32,      # Number of samples per gradient update\n",
    "    validation_split=0.1, # Use 10% of training data for validation during training\n",
    "    verbose=0           # Suppress output for cleaner presentation\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "# To see training progress:\n",
    "# pd.DataFrame(history.history).plot(figsize=(10, 7))\n",
    "# import matplotlib.pyplot as plt; plt.show()\n",
    "\n",
    "# 6. Model Evaluation\n",
    "print(\"\\nEvaluating the model...\")\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee6834-fac5-45c0-9ed8-d933d345942a",
   "metadata": {},
   "source": [
    "### How to use TensorFlow/Keras: Another Example (LTSM sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01bcab56-0484-44c6-a42f-79917856d345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "No GPU devices found. TensorFlow will run on CPU.\n",
      "--------------------------------------------------\n",
      "Loading IMDB dataset (top 10000 words)...\n",
      "Train samples: 25000, Test samples: 25000\n",
      "Padding sequences to maxlen=250...\n",
      "Sequences padded.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "--------------------------------------------------\n",
      "Building the LSTM model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Compiling the model...\n",
      "Training the model (this may take a few minutes)...\n",
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 124ms/step - accuracy: 0.7537 - loss: 0.4877 - val_accuracy: 0.8376 - val_loss: 0.4420\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 125ms/step - accuracy: 0.8876 - loss: 0.2897 - val_accuracy: 0.8734 - val_loss: 0.3194\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 123ms/step - accuracy: 0.9216 - loss: 0.2123 - val_accuracy: 0.8648 - val_loss: 0.3607\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 125ms/step - accuracy: 0.9486 - loss: 0.1445 - val_accuracy: 0.8672 - val_loss: 0.3926\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 125ms/step - accuracy: 0.9648 - loss: 0.1018 - val_accuracy: 0.8522 - val_loss: 0.4353\n",
      "Model training finished.\n",
      "--------------------------------------------------\n",
      "Evaluating the model on the test set...\n",
      "Test Loss: 0.4638\n",
      "Test Accuracy: 0.8460\n",
      "--------------------------------------------------\n",
      "Demonstrating sentiment prediction for raw sentences...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "Sentence: \"This movie was absolutely fantastic! I loved every single moment of it. Highly recommend.\"\n",
      "Predicted Probability (Positive): 0.9553\n",
      "Predicted Sentiment: Positive\n",
      "--------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Sentence: \"Terrible film, a complete waste of time and money. I wouldn't watch it again even for free.\"\n",
      "Predicted Probability (Positive): 0.0013\n",
      "Predicted Sentiment: Negative\n",
      "--------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Sentence: \"It was okay, not great, not bad. Just an average movie experience.\"\n",
      "Predicted Probability (Positive): 0.2979\n",
      "Predicted Sentiment: Negative\n",
      "--------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Sentence: \"The acting was superb, but the plot was a bit weak and confusing.\"\n",
      "Predicted Probability (Positive): 0.0853\n",
      "Predicted Sentiment: Negative\n",
      "--------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Sentence: \"An engaging story with brilliant performances, a must-see for everyone.\"\n",
      "Predicted Probability (Positive): 0.4232\n",
      "Predicted Sentiment: Negative\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "\n",
    "# --- Configuration for GPU (if available) ---\n",
    "# This part is optional but good practice if you have a GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth to True to avoid allocating all GPU memory at once\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPUs available: {len(gpus)}. Memory growth is enabled.\")\n",
    "        print(\"Details:\", [gpu.name for gpu in gpus])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found. TensorFlow will run on CPU.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# --- 1. Load and Preprocess Data ---\n",
    "\n",
    "# Parameters for data loading and preprocessing\n",
    "vocab_size = 10000  # Only consider the top `vocab_size` words\n",
    "maxlen = 250        # Max length of a review (sentences longer than this will be truncated)\n",
    "embedding_dim = 128 # Dimension of the word embeddings\n",
    "\n",
    "print(f\"Loading IMDB dataset (top {vocab_size} words)...\")\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "print(f\"Train samples: {len(x_train)}, Test samples: {len(x_test)}\")\n",
    "\n",
    "# Pad sequences to ensure uniform length for all reviews\n",
    "# 'post' padding adds zeros at the end\n",
    "# 'pre' padding adds zeros at the beginning\n",
    "print(f\"Padding sequences to maxlen={maxlen}...\")\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "print(\"Sequences padded.\")\n",
    "\n",
    "# Retrieve the word index mapping (word to integer)\n",
    "word_index = imdb.get_word_index()\n",
    "# Keras's IMDB dataset reserves 0, 1, 2 for padding, start-of-sequence, and unknown.\n",
    "# We need to adjust the index for actual words when mapping back.\n",
    "word_to_id = {key: (value + 3) for key, value in word_index.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2  # Unknown word\n",
    "word_to_id[\"<UNUSED>\"] = 3 # Unused (often maps to actual words if not adjusted)\n",
    "\n",
    "# Create a reverse mapping (id to word)\n",
    "id_to_word = {value: key for key, value in word_to_id.items()}\n",
    "\n",
    "# Example: Convert a preprocessed review back to words (for verification)\n",
    "# print(\"\\nExample preprocessed review (first 10 words):\")\n",
    "# print([id_to_word.get(i, \"?\") for i in x_train[0][:10]])\n",
    "# print(\"Corresponding label:\", y_train[0])\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 2. Build the LSTM Model ---\n",
    "print(\"Building the LSTM model...\")\n",
    "model = Sequential([\n",
    "    # Embedding layer: Converts integer-encoded words into dense vectors\n",
    "    Embedding(vocab_size, embedding_dim, input_length=maxlen),\n",
    "    \n",
    "    # LSTM layer: Processes sequences. 'units' is the dimensionality of the output space.\n",
    "    # It learns long-term dependencies in the sequence.\n",
    "    LSTM(embedding_dim), # Using the same dimension as embedding for simplicity\n",
    "    \n",
    "    # Dropout layer: Helps prevent overfitting by randomly setting a fraction of input units to 0.\n",
    "    Dropout(0.5), # 50% of neurons will be dropped during training\n",
    "    \n",
    "    # Dense output layer: For binary classification (positive/negative sentiment)\n",
    "    # 1 unit and 'sigmoid' activation for probabilities between 0 and 1.\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 3. Compile and Train the Model ---\n",
    "print(\"Compiling the model...\")\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy', # Appropriate for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Training the model (this may take a few minutes)...\")\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=5,           # Number of times to iterate over the entire dataset\n",
    "                    batch_size=64,      # Number of samples per gradient update\n",
    "                    validation_split=0.2 # Use 20% of training data for validation\n",
    "                   )\n",
    "print(\"Model training finished.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 4. Evaluate the Model ---\n",
    "print(\"Evaluating the model on the test set...\")\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 5. Predict Sentiment for Raw Sentences ---\n",
    "print(\"Demonstrating sentiment prediction for raw sentences...\")\n",
    "\n",
    "def preprocess_text(text, word_to_id_map, maxlen_val, vocab_size_val):\n",
    "    \"\"\"\n",
    "    Tokenizes, converts words to IDs, and pads a raw text sentence.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    # Convert words to IDs, defaulting to <UNK> if not in vocab\n",
    "    # Also, ensure we don't exceed the vocab_size by mapping out-of-vocab to <UNK>\n",
    "    encoded_review = [\n",
    "        word_to_id_map.get(word, word_to_id_map[\"<UNK>\"])\n",
    "        for word in words\n",
    "        if word_to_id_map.get(word, word_to_id_map[\"<UNK>\"]) < vocab_size_val\n",
    "    ]\n",
    "    # Add the start-of-sequence token\n",
    "    encoded_review = [word_to_id_map[\"<START>\"]] + encoded_review\n",
    "    \n",
    "    # Pad the sequence\n",
    "    padded_review = pad_sequences([encoded_review], maxlen=maxlen_val)\n",
    "    return padded_review\n",
    "\n",
    "def predict_sentiment(model, raw_sentence, word_to_id_map, maxlen_val, vocab_size_val):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a raw sentence using the trained model.\n",
    "    \"\"\"\n",
    "    preprocessed_input = preprocess_text(raw_sentence, word_to_id_map, maxlen_val, vocab_size_val)\n",
    "    prediction = model.predict(preprocessed_input)[0][0]\n",
    "    \n",
    "    sentiment = \"Positive\" if prediction >= 0.5 else \"Negative\"\n",
    "    \n",
    "    print(f\"Sentence: \\\"{raw_sentence}\\\"\")\n",
    "    print(f\"Predicted Probability (Positive): {prediction:.4f}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Sample sentences\n",
    "sample_sentence1 = \"This movie was absolutely fantastic! I loved every single moment of it. Highly recommend.\"\n",
    "sample_sentence2 = \"Terrible film, a complete waste of time and money. I wouldn't watch it again even for free.\"\n",
    "sample_sentence3 = \"It was okay, not great, not bad. Just an average movie experience.\"\n",
    "sample_sentence4 = \"The acting was superb, but the plot was a bit weak and confusing.\"\n",
    "sample_sentence5 = \"An engaging story with brilliant performances, a must-see for everyone.\"\n",
    "\n",
    "predict_sentiment(model, sample_sentence1, word_to_id, maxlen, vocab_size)\n",
    "predict_sentiment(model, sample_sentence2, word_to_id, maxlen, vocab_size)\n",
    "predict_sentiment(model, sample_sentence3, word_to_id, maxlen, vocab_size)\n",
    "predict_sentiment(model, sample_sentence4, word_to_id, maxlen, vocab_size)\n",
    "predict_sentiment(model, sample_sentence5, word_to_id, maxlen, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935d90eb-e275-476d-9940-7f410511f660",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "In deep learning, an **epoch** refers to one complete pass through the entire training dataset.\n",
    "\n",
    "Here's a breakdown of what that means:\n",
    "\n",
    "1.  **Full Dataset Pass:** During one epoch, every single training example from your dataset is fed forward through the neural network, and the network's weights are updated via backpropagation based on the error it makes on those examples.\n",
    "\n",
    "2.  **Learning Cycles:** Neural networks typically require multiple epochs to learn the underlying patterns in the data effectively. A single pass might not be enough for the model to sufficiently adjust its internal parameters (weights and biases) to achieve good performance.\n",
    "\n",
    "3.  **Relationship with Batch Size and Iterations:**\n",
    "    *   **Batch Size:** The training dataset is usually too large to process all at once. Instead, it's divided into smaller subsets called \"batches\" (or \"mini-batches\"). The model processes one batch at a time, calculates the loss, and updates its weights.\n",
    "    *   **Iteration/Step:** An \"iteration\" (or \"step\") refers to processing a single batch.\n",
    "    *   **Number of Iterations per Epoch:** If you have `N` total training samples and a `batch_size` of `B`, then one epoch will consist of `N / B` iterations (or steps).\n",
    "\n",
    "    **Formula:** `Number of Iterations per Epoch = Total Training Samples / Batch Size`\n",
    "\n",
    "**Example:**\n",
    "*   If you have a training dataset of 10,000 images.\n",
    "*   And your `batch_size` is 100.\n",
    "*   Then one epoch will involve `10,000 / 100 = 100` iterations.\n",
    "*   During each iteration, 100 images are processed, and the model's weights are updated. After all 100 iterations are completed, one epoch is finished, meaning the model has seen all 10,000 images once.\n",
    "\n",
    "**Why Multiple Epochs are Necessary:**\n",
    "*   **Gradual Learning:** The model learns gradually. Each epoch allows the model to refine its understanding of the data, adjusting its parameters incrementally to minimize the loss function.\n",
    "*   **Convergence:** Training for multiple epochs allows the model's performance (e.g., accuracy) to converge to a better solution.\n",
    "*   **Underfitting vs. Overfitting:**\n",
    "    *   **Too few epochs** can lead to **underfitting**, where the model hasn't learned enough from the data and performs poorly on both training and test sets.\n",
    "    *   **Too many epochs** can lead to **overfitting**, where the model learns the training data too well, memorizing noise and specific examples, and thus performs poorly on new, unseen data. Techniques like dropout, early stopping, and regularization help mitigate overfitting even with many epochs.\n",
    "\n",
    "In summary, an epoch is a fundamental unit in the training process of a neural network, representing a full cycle of seeing and learning from the entire training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff7a2b",
   "metadata": {},
   "source": [
    "### Discussion: TensorFlow/Keras in Finance\n",
    "\n",
    "*   **Sentiment Analysis:** Using LSTMs or Transformers in Keras for analyzing financial news, earnings call transcripts, or social media for market sentiment.\n",
    "*   **Algorithmic Trading:** Developing reinforcement learning agents with TensorFlow (e.g., using `tf-agents`) to learn optimal trading strategies.\n",
    "*   **Credit Risk Modeling:** Building deep neural networks for credit default prediction, processing structured and unstructured data (e.g., loan applications, customer behavior data).\n",
    "*   **Fraud Detection:** Identifying anomalous transactions or patterns using autoencoders or other unsupervised/semi-supervised DL models.\n",
    "\n",
    "Keras simplifies the process, allowing finance professionals to focus on feature engineering and model architecture rather than low-level implementation details. TensorFlow then provides the robust backend for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a38fea1",
   "metadata": {},
   "source": [
    "## 4. PyTorch: The Research Favorite (45-50 minutes)\n",
    "\n",
    "### What is PyTorch?\n",
    "PyTorch is an open-source machine learning library primarily developed by Facebook's AI Research lab (FAIR). It's renowned for its flexibility, dynamic computation graph, and Pythonic interface, which makes it a favorite among researchers and developers who need fine-grained control over their models and training processes.\n",
    "\n",
    "### Why use PyTorch in Finance?\n",
    "\n",
    "*   **Flexibility & Debugging:** The dynamic graph allows for easier debugging (like regular Python code) and more experimental model architectures, which is crucial when exploring novel financial hypotheses.\n",
    "*   **Pythonic Design:** Integrates seamlessly with the rest of the Python data science stack (NumPy, Pandas, Scikit-learn), making it very intuitive for Python developers.\n",
    "*   **Research & Innovation:** Many cutting-edge research papers in finance (e.g., in areas like quantitative finance, NLP for market analysis) are often implemented first in PyTorch.\n",
    "*   **Growing Ecosystem:** While historically more research-focused, PyTorch's ecosystem for deployment (TorchScript, PyTorch Mobile, PyTorch Lightning for training abstraction) has matured significantly.\n",
    "\n",
    "### Pros and Cons of PyTorch\n",
    "\n",
    "| Aspect          | Pros                                                                                             | Cons                                                                                              |\n",
    "| :-------------- | :----------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------ |\n",
    "| **Ease of Use** | Highly Pythonic, intuitive API. Dynamic graph makes debugging and control straightforward.       | Requires more manual control over the training loop compared to Keras's `model.fit()`.           |\n",
    "| **Flexibility** | Extremely flexible, ideal for custom layers, loss functions, and research-oriented architectures. | With great power comes great responsibility; more boilerplate code for standard tasks.           |\n",
    "| **Performance** | Excellent performance on GPUs. JIT compilation (TorchScript) for optimization.                   | May require more careful optimization for deployment compared to TensorFlow's static graph.      |\n",
    "| **Deployment**  | `TorchScript` enables production deployment, but the ecosystem is slightly less mature than TF.  |                                                                                                   |\n",
    "| **Community**   | Strong and rapidly growing community, especially in academia and research.                       | Historically smaller than TensorFlow, but catching up quickly.                                   |\n",
    "| **Finance Use** | Developing novel trading strategies, complex risk models, advanced NLP for financial text, generative models for synthetic data. |                                                                                                   |\n",
    "\n",
    "### How to use PyTorch: A Simple Example (Stock Price Prediction)\n",
    "\n",
    "Let's re-implement the same stock price prediction problem using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57984bf-f939-48c5-9716-92b5ca5f0e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c73a89a-794d-4452-b75a-df1beb4655a1",
   "metadata": {},
   "source": [
    "A PyTorch tensor is the fundamental data structure in PyTorch, serving as the primary way to store and manipulate numerical data. It is essentially a multi-dimensional array, similar to NumPy arrays, but with several key advantages that make it suitable for deep learning:\n",
    "\n",
    "Here's a breakdown of what a PyTorch tensor is and its characteristics:\n",
    "\n",
    "1.  **Multi-dimensional Array:** At its core, a PyTorch tensor is an array that can have zero, one, or more dimensions (e.g., a scalar, a vector, a matrix, or higher-order tensors). This allows it to represent various types of data, from single numbers to images, audio, or entire batches of data.\n",
    "\n",
    "2.  **GPU Acceleration:** One of the most significant advantages of PyTorch tensors is their ability to leverage Graphics Processing Units (GPUs) for computations. This allows for massive parallelization and significantly speeds up the training and inference of deep learning models, which often involve large-scale matrix operations.\n",
    "\n",
    "3.  **Automatic Differentiation (Autograd):** PyTorch tensors are at the heart of PyTorch's automatic differentiation engine, called Autograd. When you perform operations on tensors, PyTorch automatically builds a computational graph. This graph tracks all the operations, allowing PyTorch to automatically compute gradients (derivatives) of the output with respect to the input tensors. This feature is crucial for training neural networks using backpropagation.\n",
    "\n",
    "4.  **Data Types:** Tensors can hold data of various types, such as floating-point numbers (e.g., `torch.float32`, `torch.float64`), integers (e.g., `torch.int32`, `torch.int64`), and booleans. The data type affects the precision and memory usage of the tensor.\n",
    "\n",
    "5.  **Device Agnostic:** Tensors can reside on either the CPU or the GPU. You can easily move tensors between devices using methods like `.to('cuda')` or `.to('cpu')`.\n",
    "\n",
    "6.  **Immutable Shape (but mutable data):** Once a tensor is created, its shape (number of dimensions and size of each dimension) is generally fixed. However, the actual numerical values stored within the tensor are mutable and can be changed.\n",
    "\n",
    "7.  **Interoperability with NumPy:** PyTorch tensors can be easily converted to and from NumPy arrays, facilitating integration with the broader Python scientific computing ecosystem.\n",
    "\n",
    "**In summary:** A PyTorch tensor is the core data container for all computations in PyTorch, providing a flexible, GPU-accelerated, and automatically differentiable multi-dimensional array structure essential for building and training deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e239d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([800, 10]), y_train_tensor shape: torch.Size([800, 1])\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=10, out_features=64, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Training the PyTorch model...\n",
      "Epoch [5/20], Loss: 0.2587\n",
      "Epoch [10/20], Loss: 0.1537\n",
      "Epoch [15/20], Loss: 0.1265\n",
      "Epoch [20/20], Loss: 0.1103\n",
      "Training complete.\n",
      "\n",
      "Evaluating the model...\n",
      "Test Accuracy: 0.9450\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.98      0.97       173\n",
      "         1.0       0.83      0.74      0.78        27\n",
      "\n",
      "    accuracy                           0.94       200\n",
      "   macro avg       0.90      0.86      0.88       200\n",
      "weighted avg       0.94      0.94      0.94       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Data Generation (Dummy Financial Data) - Same as before\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "\n",
    "X_np = np.random.rand(num_samples, num_features) * 100\n",
    "y_np = (X_np[:, 0] * 0.5 + X_np[:, 1] * 0.3 - X_np[:, 2] * 0.2 + np.random.randn(num_samples) * 5 > 50).astype(int)\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled_np = scaler.fit_transform(X_np)\n",
    "\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_scaled_np, y_np, test_size=0.2, random_state=42, stratify=y_np)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_np, dtype=torch.float32).unsqueeze(1) # Add a dimension for binary output\n",
    "X_test_tensor = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_np, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"X_train_tensor shape: {X_train_tensor.shape}, y_train_tensor shape: {y_train_tensor.shape}\")\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 3. Model Definition (PyTorch nn.Module)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "input_size = num_features\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "output_size = 1\n",
    "\n",
    "model_pt = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_pt.to(device)\n",
    "\n",
    "print(model_pt)\n",
    "\n",
    "# 4. Loss Function and Optimizer\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model_pt.parameters(), lr=0.001)\n",
    "\n",
    "# 5. Model Training Loop (Manual)\n",
    "epochs = 20\n",
    "print(\"\\nTraining the PyTorch model...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model_pt.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "        outputs = model_pt(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()       # Backpropagation\n",
    "        optimizer.step()      # Update weights\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 6. Model Evaluation\n",
    "print(\"\\nEvaluating the model...\")\n",
    "model_pt.eval() # Set model to evaluation mode\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculations\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model_pt(inputs)\n",
    "        predicted = (outputs > 0.5).squeeze().cpu().numpy()\n",
    "        all_preds.extend(predicted)\n",
    "        all_labels.extend(labels.squeeze().cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb56a6b8-567d-47bd-b86a-2e6ad2264643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\binshi\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\binshi\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\binshi\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\binshi\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\binshi\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\binshi\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\binshi\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\binshi\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\binshi\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
      "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/110.9 MB 2.9 MB/s eta 0:00:39\n",
      "   ---------------------------------------- 1.3/110.9 MB 3.3 MB/s eta 0:00:34\n",
      "    --------------------------------------- 1.8/110.9 MB 3.2 MB/s eta 0:00:34\n",
      "    --------------------------------------- 2.6/110.9 MB 3.3 MB/s eta 0:00:33\n",
      "    --------------------------------------- 2.6/110.9 MB 3.3 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 3.1/110.9 MB 2.7 MB/s eta 0:00:41\n",
      "   - -------------------------------------- 3.9/110.9 MB 2.7 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 4.5/110.9 MB 2.7 MB/s eta 0:00:39\n",
      "   - -------------------------------------- 5.0/110.9 MB 2.8 MB/s eta 0:00:39\n",
      "   -- ------------------------------------- 5.8/110.9 MB 2.8 MB/s eta 0:00:38\n",
      "   -- ------------------------------------- 6.3/110.9 MB 2.9 MB/s eta 0:00:37\n",
      "   -- ------------------------------------- 7.6/110.9 MB 3.1 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 8.4/110.9 MB 3.2 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 9.4/110.9 MB 3.3 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 10.7/110.9 MB 3.5 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 12.1/110.9 MB 3.7 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 13.4/110.9 MB 3.8 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 14.4/110.9 MB 3.9 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 15.2/110.9 MB 3.9 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 16.3/110.9 MB 4.0 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 17.3/110.9 MB 4.0 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 18.4/110.9 MB 4.1 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 19.1/110.9 MB 4.1 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 19.9/110.9 MB 4.1 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 21.0/110.9 MB 4.1 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 21.2/110.9 MB 4.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 22.0/110.9 MB 4.0 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 22.8/110.9 MB 4.0 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 23.1/110.9 MB 4.0 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 23.9/110.9 MB 3.9 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 24.6/110.9 MB 3.9 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 24.9/110.9 MB 3.8 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 25.7/110.9 MB 3.8 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 26.5/110.9 MB 3.8 MB/s eta 0:00:23\n",
      "   --------- ------------------------------ 27.0/110.9 MB 3.7 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 28.0/110.9 MB 3.7 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 28.8/110.9 MB 3.8 MB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 29.9/110.9 MB 3.8 MB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 30.7/110.9 MB 3.8 MB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 31.2/110.9 MB 3.8 MB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 32.0/110.9 MB 3.8 MB/s eta 0:00:21\n",
      "   ------------ --------------------------- 33.3/110.9 MB 3.8 MB/s eta 0:00:21\n",
      "   ------------ --------------------------- 34.3/110.9 MB 3.8 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 35.4/110.9 MB 3.9 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 36.2/110.9 MB 3.9 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 37.0/110.9 MB 3.9 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 38.3/110.9 MB 3.9 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 39.1/110.9 MB 3.9 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 40.4/110.9 MB 4.0 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 41.4/110.9 MB 4.0 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 42.2/110.9 MB 4.0 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 43.0/110.9 MB 4.0 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 43.3/110.9 MB 3.9 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 44.3/110.9 MB 3.9 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 45.1/110.9 MB 3.9 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 46.4/110.9 MB 4.0 MB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 47.4/110.9 MB 4.0 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 48.5/110.9 MB 4.0 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 49.5/110.9 MB 4.0 MB/s eta 0:00:16\n",
      "   ------------------ --------------------- 50.6/110.9 MB 4.0 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 51.6/110.9 MB 4.1 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 53.0/110.9 MB 4.1 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 53.7/110.9 MB 4.1 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 54.8/110.9 MB 4.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 55.8/110.9 MB 4.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 56.9/110.9 MB 4.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 57.7/110.9 MB 4.1 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 58.7/110.9 MB 4.1 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 59.5/110.9 MB 4.1 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 60.6/110.9 MB 4.1 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 61.3/110.9 MB 4.1 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 62.4/110.9 MB 4.2 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 63.2/110.9 MB 4.2 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 64.2/110.9 MB 4.2 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 65.0/110.9 MB 4.2 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 65.5/110.9 MB 4.2 MB/s eta 0:00:11\n",
      "   ----------------------- ---------------- 66.1/110.9 MB 4.1 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 66.8/110.9 MB 4.1 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 67.9/110.9 MB 4.1 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 68.9/110.9 MB 4.1 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 69.7/110.9 MB 4.1 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 70.5/110.9 MB 4.1 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 71.6/110.9 MB 4.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 72.6/110.9 MB 4.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 73.4/110.9 MB 4.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 74.4/110.9 MB 4.2 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 75.5/110.9 MB 4.2 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 76.3/110.9 MB 4.2 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 77.3/110.9 MB 4.2 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 78.4/110.9 MB 4.2 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 79.2/110.9 MB 4.2 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 80.0/110.9 MB 4.2 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 81.0/110.9 MB 4.2 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 81.8/110.9 MB 4.2 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 83.1/110.9 MB 4.2 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 83.9/110.9 MB 4.2 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 84.9/110.9 MB 4.2 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 85.7/110.9 MB 4.2 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 86.8/110.9 MB 4.2 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 87.3/110.9 MB 4.2 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 87.8/110.9 MB 4.2 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 88.6/110.9 MB 4.2 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 89.4/110.9 MB 4.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 90.2/110.9 MB 4.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 91.2/110.9 MB 4.2 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 92.0/110.9 MB 4.2 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 92.8/110.9 MB 4.2 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 93.8/110.9 MB 4.2 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 94.6/110.9 MB 4.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 95.4/110.9 MB 4.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 96.5/110.9 MB 4.2 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 97.5/110.9 MB 4.2 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 98.0/110.9 MB 4.2 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 98.8/110.9 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 99.6/110.9 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 100.7/110.9 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 101.7/110.9 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 102.5/110.9 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 103.5/110.9 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 104.6/110.9 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 105.6/110.9 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 106.4/110.9 MB 4.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 107.2/110.9 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 107.7/110.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  108.3/110.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.3/110.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  110.4/110.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  110.9/110.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  110.9/110.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  110.9/110.9 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 110.9/110.9 MB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26a915-533f-4942-b1dd-5404afa9f3b3",
   "metadata": {},
   "source": [
    "##  BERT and FinBERT\n",
    "BERT and FinBERT are both powerful language models, but they serve different purposes due to their training data and specialization.\n",
    "\n",
    "### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "**What is BERT?**\n",
    "BERT is an open-source machine learning framework for Natural Language Processing (NLP) developed by Google in 2018. It revolutionized the field of NLP by introducing a novel approach to pre-training language representations.\n",
    "\n",
    "**Key Characteristics of BERT:**\n",
    "*   **Transformer Architecture:** BERT is built upon the Transformer architecture, specifically using only the encoder part. This architecture employs a self-attention mechanism that allows it to consider the entire context of a word in a sentence simultaneously, rather than processing text sequentially (left-to-right or right-to-left). This \"bidirectional\" understanding is a key differentiator.\n",
    "*   **Pre-training:** BERT undergoes pre-training on massive amounts of unlabeled text data, such as Wikipedia and Google's BooksCorpus (over 3 billion words). This unsupervised pre-training allows it to learn deep contextual representations of words and complex language patterns.\n",
    "*   **Two Training Strategies:** During pre-training, BERT uses two main tasks:\n",
    "    *   **Masked Language Modeling (MLM):** Randomly masks a percentage of words in a sentence and then tries to predict the masked words based on their context.\n",
    "    *   **Next Sentence Prediction (NSP):** Predicts whether two sentences follow each other in the original text.\n",
    "*   **Fine-tuning:** After pre-training, BERT can be fine-tuned with smaller, labeled datasets for specific NLP tasks like sentiment analysis, question answering, named entity recognition, and text classification, achieving state-of-the-art performance.\n",
    "\n",
    "### FinBERT (Financial Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "**What is FinBERT?**\n",
    "FinBERT is a specialized variant of the BERT model, specifically tailored for financial sentiment analysis and other NLP tasks within the financial domain. It is built by taking a pre-trained general BERT model and further training (fine-tuning) it on a large corpus of financial text.\n",
    "\n",
    "**Key Characteristics and Why it's Important:**\n",
    "*   **Domain-Specific Specialization:** While general BERT models are powerful, financial language has unique terminology, jargon, and context that can make general models less accurate for sentiment analysis. FinBERT addresses this by focusing on finance.\n",
    "*   **Training Data:** FinBERT is further trained on extensive financial corpora, such as the Reuters TRC2 dataset and the Financial PhraseBank. This additional training allows it to better understand and interpret financial jargon and the nuances of financial sentiment.\n",
    "*   **Improved Accuracy in Finance:** For tasks like classifying financial text as positive, negative, or neutral, FinBERT significantly outperforms general BERT models because it has learned domain-specific contextual information. For example, it can understand that \"The company's profits are up 10%\" is positive, while \"The stock price is down 5%\" is negative.\n",
    "*   **Use Cases:** FinBERT is highly valuable for analyzing the sentiment of financial news articles, earnings reports, market updates, regulatory alerts, and social media posts related to financial markets, assisting in investment strategies and risk management.\n",
    "*   **Output:** FinBERT typically provides softmax outputs for three labels: positive, negative, or neutral, indicating the probability of each sentiment.\n",
    "\n",
    "In essence, FinBERT takes the strong language understanding capabilities of BERT and refines them with domain-specific knowledge, making it a powerful tool for financial text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce2e28f-fb41-4b9b-9d4d-cabc270b4e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FinBERT tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f794d32bea104d98905719fc23d2c0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binshi\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\binshi\\.cache\\huggingface\\hub\\models--ProsusAI--finbert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7d75ef74384f30b9023f465ffac809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180c9b1528ea493ab31ba40451727e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad193c3dd6c40628e67704284d7a6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92d46ded5434cf09a1d0c720984d021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT model loaded and moved to: cpu\n",
      "--------------------------------------------------\n",
      "Analyzing sentiment for sample sentences:\n",
      "--------------------------------------------------\n",
      "Sentence: \"The company announced strong quarterly earnings, exceeding analyst expectations.\"\n",
      "  Predicted Sentiment: POSITIVE\n",
      "  Probabilities: P(Positive)=0.9531, P(Negative)=0.0234, P(Neutral)=0.0235\n",
      "--------------------\n",
      "Sentence: \"Despite record sales, the stock price plummeted due to market uncertainty.\"\n",
      "  Predicted Sentiment: NEGATIVE\n",
      "  Probabilities: P(Positive)=0.0087, P(Negative)=0.9722, P(Neutral)=0.0191\n",
      "--------------------\n",
      "Sentence: \"Analysts maintain a 'neutral' rating on the company's shares.\"\n",
      "  Predicted Sentiment: NEUTRAL\n",
      "  Probabilities: P(Positive)=0.0349, P(Negative)=0.4024, P(Neutral)=0.5627\n",
      "--------------------\n",
      "Sentence: \"Inflation concerns are growing as commodity prices continue to rise.\"\n",
      "  Predicted Sentiment: POSITIVE\n",
      "  Probabilities: P(Positive)=0.8338, P(Negative)=0.1179, P(Neutral)=0.0483\n",
      "--------------------\n",
      "Sentence: \"A new strategic partnership is expected to boost revenue significantly.\"\n",
      "  Predicted Sentiment: POSITIVE\n",
      "  Probabilities: P(Positive)=0.9520, P(Negative)=0.0171, P(Neutral)=0.0309\n",
      "--------------------\n",
      "Sentence: \"The board approved a new share buyback program, delighting investors.\"\n",
      "  Predicted Sentiment: POSITIVE\n",
      "  Probabilities: P(Positive)=0.9005, P(Negative)=0.0141, P(Neutral)=0.0854\n",
      "--------------------\n",
      "Sentence: \"Company X filed for bankruptcy today.\"\n",
      "  Predicted Sentiment: NEGATIVE\n",
      "  Probabilities: P(Positive)=0.0168, P(Negative)=0.5964, P(Neutral)=0.3868\n",
      "--------------------\n",
      "Sentence: \"Market shows no significant movement today.\"\n",
      "  Predicted Sentiment: NEUTRAL\n",
      "  Probabilities: P(Positive)=0.0444, P(Negative)=0.2103, P(Neutral)=0.7452\n",
      "--------------------\n",
      "FinBERT sentiment analysis complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e66d89ec7a3467d80ebadca15678adc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load FinBERT Model and Tokenizer ---\n",
    "# We'll use the 'ProsusAI/finbert' model from Hugging Face\n",
    "print(\"Loading FinBERT tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# Ensure the model is in evaluation mode (important for consistent predictions)\n",
    "model.eval()\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"FinBERT model loaded and moved to: {device}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 2. Prepare Sample Financial Sentences ---\n",
    "# These are raw input sentences, just like you'd get from online data.\n",
    "sample_sentences = [\n",
    "    \"The company announced strong quarterly earnings, exceeding analyst expectations.\", # Positive\n",
    "    \"Despite record sales, the stock price plummeted due to market uncertainty.\",     # Negative/Mixed\n",
    "    \"Analysts maintain a 'neutral' rating on the company's shares.\",                  # Neutral\n",
    "    \"Inflation concerns are growing as commodity prices continue to rise.\",           # Negative\n",
    "    \"A new strategic partnership is expected to boost revenue significantly.\",        # Positive\n",
    "    \"The board approved a new share buyback program, delighting investors.\",          # Positive\n",
    "    \"Company X filed for bankruptcy today.\",                                          # Negative\n",
    "    \"Market shows no significant movement today.\",                                    # Neutral\n",
    "]\n",
    "\n",
    "# --- 3. Process and Predict Sentiment for Each Sentence ---\n",
    "print(\"Analyzing sentiment for sample sentences:\")\n",
    "\n",
    "# Define the sentiment labels that FinBERT typically outputs\n",
    "labels = ['positive', 'negative', 'neutral']\n",
    "\n",
    "results = []\n",
    "for sentence in sample_sentences:\n",
    "    # Tokenize the sentence\n",
    "    # `return_tensors=\"pt\"` ensures PyTorch tensors are returned\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Perform inference (forward pass)\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference to save memory and speed up\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the logits (raw prediction scores)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = torch.softmax(logits, dim=1).cpu().numpy()[0] # Move to CPU and convert to numpy\n",
    "\n",
    "    # Get the predicted sentiment (index of the highest probability)\n",
    "    predicted_class_id = np.argmax(probabilities)\n",
    "    predicted_sentiment = labels[predicted_class_id]\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"sentence\": sentence,\n",
    "        \"sentiment\": predicted_sentiment,\n",
    "        \"probabilities\": {label: prob for label, prob in zip(labels, probabilities)}\n",
    "    })\n",
    "\n",
    "# --- 4. Display Results ---\n",
    "print(\"-\" * 50)\n",
    "for res in results:\n",
    "    print(f\"Sentence: \\\"{res['sentence']}\\\"\")\n",
    "    print(f\"  Predicted Sentiment: {res['sentiment'].upper()}\")\n",
    "    print(f\"  Probabilities: P(Positive)={res['probabilities']['positive']:.4f}, \"\n",
    "          f\"P(Negative)={res['probabilities']['negative']:.4f}, \"\n",
    "          f\"P(Neutral)={res['probabilities']['neutral']:.4f}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "print(\"FinBERT sentiment analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d280a3",
   "metadata": {},
   "source": [
    "### Discussion: PyTorch in Finance\n",
    "\n",
    "*   **Quantitative Research:** Developing new models for option pricing (e.g., using neural networks to approximate Black-Scholes or calibrate implied volatility surfaces).\n",
    "*   **Generative Models:** Using GANs (Generative Adversarial Networks) or VAEs (Variational Autoencoders) to generate synthetic financial data for backtesting or privacy-preserving data sharing.\n",
    "*   **Reinforcement Learning for Trading:** Building custom RL environments and agents with PyTorch for complex trading simulations, leveraging its flexibility for complex state and action spaces.\n",
    "*   **Natural Language Processing:** State-of-the-art NLP models (e.g., Transformers from Hugging Face, often built on PyTorch) for processing earnings calls, analyst reports, and news for alpha generation.\n",
    "\n",
    "PyTorch offers granular control, which is highly valued when you need to deviate from standard architectures or delve deep into debugging, making it a powerful tool for financial innovation and research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8321c24",
   "metadata": {},
   "source": [
    "## 5. JAX: The High-Performance Numerics and Research Tool (20-25 minutes)\n",
    "\n",
    "### What is JAX?\n",
    "JAX is a high-performance numerical computing library from Google that combines three key features:\n",
    "1.  **Automatic Differentiation:** Capable of differentiating native Python and NumPy functions.\n",
    "2.  **JIT Compilation (XLA):** Compiles Python and NumPy code into optimized, high-performance routines for CPUs, GPUs, and TPUs using XLA (Accelerated Linear Algebra).\n",
    "3.  **Composability:** Its transformations (`grad`, `jit`, `vmap`, `pmap`) can be arbitrarily composed.\n",
    "\n",
    "JAX's paradigm is functional programming – functions are pure, and state is handled explicitly, which leads to highly reproducible and performant code. It's not a full-fledged deep learning framework like TensorFlow or PyTorch, but rather a powerful toolkit upon which DL frameworks like Flax and Haiku are built.\n",
    "\n",
    "### Why use JAX in Finance?\n",
    "\n",
    "*   **High-Performance Numerical Computing:** Crucial for computationally intensive tasks in quantitative finance, such as Monte Carlo simulations, complex optimization problems, or large-scale calibration of financial models.\n",
    "*   **Automatic Differentiation of Arbitrary Functions:** Extremely useful for calculating sensitivities (Greeks in options pricing), risk measures, or gradients for custom optimization algorithms without manual derivation.\n",
    "*   **Functional Programming:** Encourages writing more robust, testable, and parallelizable code, which is important for financial models where correctness is paramount.\n",
    "*   **Research & Custom Models:** For researchers developing novel financial models or optimization techniques that go beyond standard neural network architectures.\n",
    "\n",
    "### Pros and Cons of JAX\n",
    "\n",
    "| Aspect          | Pros                                                                                             | Cons                                                                                              |\n",
    "| :-------------- | :----------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------ |\n",
    "| **Ease of Use** | Simple API for core transformations (`grad`, `jit`). Feels like NumPy.                          | Learning curve for functional programming paradigm, explicit state management. Less 'plug-and-play' for standard DL. |\n",
    "| **Flexibility** | Extremely flexible for custom numerical algorithms, AD, and high-performance scientific computing. | Not a full-fledged DL framework; requires building more from scratch or using frameworks like Flax/Haiku. |\n",
    "| **Performance** | Best-in-class performance on accelerators (GPUs, TPUs) due to XLA compilation.                 | Initial compilation time for JITted functions.                                                    |\n",
    "| **Deployment**  | Primarily a research/development tool; deployment typically involves integrating compiled JAX functions into other systems. |\n",
    "| **Community**   | Smaller but highly active community, primarily in research and high-performance computing.      | Less industry adoption than TF/PyTorch for general DL tasks.                                      |\n",
    "| **Finance Use** | Option pricing (Greeks), risk management (sensitivities), complex optimization, Monte Carlo simulations, Bayesian inference. |                                                                                                   |\n",
    "\n",
    "### How to use JAX: A Simple Example (Option Pricing Greeks)\n",
    "\n",
    "Let's demonstrate JAX's automatic differentiation by calculating the Black-Scholes option price and its Greek (Delta) with minimal effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0dfd8-68f8-48a3-aeaa-fb6482da06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce42597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black-Scholes Call Price: 8.0214\n",
      "Black-Scholes Delta (d(Call)/dS): 0.5422\n",
      "Black-Scholes Rho (d(Call)/dr): 46.2015\n",
      "\n",
      "Prices for multiple options:\n",
      "[ 2.3494284 10.450576  20.774443 ]\n",
      "Deltas for multiple options:\n",
      "[0.30940998 0.6368306  0.79325354]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax.scipy.stats import norm\n",
    "\n",
    "# 1. Define the Black-Scholes Call Option Price function\n",
    "# S: Stock price\n",
    "# K: Strike price\n",
    "# T: Time to expiration (years)\n",
    "# r: Risk-free rate\n",
    "# sigma: Volatility\n",
    "\n",
    "@jit # JIT compile for performance\n",
    "def black_scholes_call(S, K, T, r, sigma):\n",
    "    d1 = (jnp.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * jnp.sqrt(T))\n",
    "    d2 = d1 - sigma * jnp.sqrt(T)\n",
    "    call_price = S * norm.cdf(d1) - K * jnp.exp(-r * T) * norm.cdf(d2)\n",
    "    return call_price\n",
    "\n",
    "# 2. Define parameters\n",
    "S_val = 100.0  # Stock price\n",
    "K_val = 105.0  # Strike price\n",
    "T_val = 1.0    # Time to expiration (1 year)\n",
    "r_val = 0.05   # Risk-free rate (5%)\n",
    "sigma_val = 0.20 # Volatility (20%)\n",
    "\n",
    "# 3. Calculate the call option price\n",
    "call_price = black_scholes_call(S_val, K_val, T_val, r_val, sigma_val)\n",
    "print(f\"Black-Scholes Call Price: {call_price:.4f}\")\n",
    "\n",
    "# 4. Calculate Delta (gradient with respect to Stock Price S)\n",
    "# `grad` returns a function that computes the gradient of `black_scholes_call`\n",
    "# with respect to its first argument (S by default, index 0).\n",
    "delta_func = grad(black_scholes_call)\n",
    "delta_val = delta_func(S_val, K_val, T_val, r_val, sigma_val)\n",
    "\n",
    "print(f\"Black-Scholes Delta (d(Call)/dS): {delta_val:.4f}\")\n",
    "\n",
    "# 5. Calculate Rho (gradient with respect to Risk-free rate r)\n",
    "# To compute the gradient with respect to a different argument, specify `argnums`.\n",
    "# r is the 4th argument (index 3)\n",
    "rho_func = grad(black_scholes_call, argnums=3)\n",
    "rho_val = rho_func(S_val, K_val, T_val, r_val, sigma_val)\n",
    "\n",
    "print(f\"Black-Scholes Rho (d(Call)/dr): {rho_val:.4f}\")\n",
    "\n",
    "# Example of vmap (vectorization) - processing multiple options efficiently\n",
    "S_multiple = jnp.array([90.0, 100.0, 110.0])\n",
    "K_multiple = jnp.array([100.0, 100.0, 100.0])\n",
    "T_multiple = jnp.array([0.5, 1.0, 1.5])\n",
    "\n",
    "# We want to map over S, K, T, keeping r and sigma constant\n",
    "vmap_bs_call = vmap(black_scholes_call, in_axes=(0, 0, 0, None, None))\n",
    "prices_multiple = vmap_bs_call(S_multiple, K_multiple, T_multiple, r_val, sigma_val)\n",
    "print(f\"\\nPrices for multiple options:\\n{prices_multiple}\")\n",
    "\n",
    "# And we can vmap the delta calculation too!\n",
    "vmap_delta_func = vmap(grad(black_scholes_call), in_axes=(0, 0, 0, None, None))\n",
    "deltas_multiple = vmap_delta_func(S_multiple, K_multiple, T_multiple, r_val, sigma_val)\n",
    "print(f\"Deltas for multiple options:\\n{deltas_multiple}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac67f4",
   "metadata": {},
   "source": [
    "### Discussion: JAX in Finance\n",
    "\n",
    "*   **Risk Management:** Calculating accurate and fast sensitivities (Greeks for options, Duration/Convexity for bonds) for large portfolios.\n",
    "*   **Calibration of Models:** Optimizing parameters of complex financial models (e.g., stochastic volatility models) by using JAX for efficient gradient-based optimization.\n",
    "*   **Quantitative Trading Strategies:** Implementing complex custom indicators or optimization routines that require fast computations and derivatives.\n",
    "*   **Bayesian Inference:** For advanced statistical modeling in finance, JAX is a powerful backend for probabilistic programming libraries that need automatic differentiation (e.g., NumPyro).\n",
    "\n",
    "JAX empowers quants and researchers to build highly efficient and custom numerical algorithms, addressing the core mathematical challenges in finance with unparalleled performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d32352",
   "metadata": {},
   "source": [
    "## 6. Comparison and Conclusion (10-15 minutes)\n",
    "\n",
    "Let's summarize the key characteristics of these powerful deep learning packages:\n",
    "\n",
    "| Feature             | TensorFlow/Keras                                   | PyTorch                                            | JAX                                                |\n",
    "| :------------------ | :------------------------------------------------- | :------------------------------------------------- | :------------------------------------------------- |\n",
    "| **Primary Focus**   | Production, Scalability, General-purpose DL        | Research, Flexibility, Pythonic Interface          | High-performance Numerics, AD, Research, Functional |\n",
    "| **Graph Type**      | Static (compiled)                                  | Dynamic (define-by-run)                            | Functional transformations, JIT compilation         |\n",
    "| **Ease of Use**     | Keras: Very high; TF low-level: moderate-low       | Moderate-high (Pythonic, but more manual loop)     | Moderate (NumPy-like, but functional paradigm)      |\n",
    "| **Debugging**       | Can be challenging (static graph errors)           | Excellent (standard Python debugger)               | Good (functional, but JIT can obscure)             |\n",
    "| **Deployment**      | Excellent (TF Serving, TF Lite, TF.js)             | Good (TorchScript, PyTorch Mobile)                 | Less direct (functions can be exported)            |\n",
    "| **Community/Ecosystem** | Massive, mature, industry-standard               | Large, rapidly growing, strong in academia         | Smaller, highly technical, growing                 |\n",
    "| **Best For**        | Large-scale deployments, robust applications, rapid prototyping (Keras).       | Cutting-edge research, custom architectures, complex modeling, flexible experimentation. | Highly optimized numerical tasks, complex optimization, advanced quantitative models.       |\n",
    "\n",
    "### When to choose which framework in Finance:\n",
    "\n",
    "*   **TensorFlow/Keras:**\n",
    "    *   You need to deploy models into production systems with high reliability and scalability (e.g., real-time fraud detection, automated trading).\n",
    "    *   You prefer a high-level API for rapid prototyping and have standard deep learning tasks (e.g., classifying market sentiment, predicting stock movements with standard LSTMs).\n",
    "    *   Your team is already familiar with the TensorFlow ecosystem.\n",
    "\n",
    "*   **PyTorch:**\n",
    "    *   You are performing cutting-edge financial research, experimenting with novel model architectures, or require deep customization (e.g., new generative models for financial data, complex reinforcement learning agents).\n",
    "    *   You value a more Pythonic interface and easier debugging capabilities.\n",
    "    *   You're building complex NLP models for financial text analysis, often leveraging the Hugging Face ecosystem.\n",
    "\n",
    "*   **JAX:**\n",
    "    *   You need extreme performance for numerical computations, such as Monte Carlo simulations, complex optimization, or high-dimensional calibration problems.\n",
    "    *   You require automatic differentiation for custom functions, especially for calculating sensitivities (Greeks) or gradients of bespoke financial models.\n",
    "    *   You are comfortable with a functional programming style and building components from a lower level, or working with libraries built on JAX (e.g., Flax, Haiku, NumPyro).\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "The deep learning landscape is dynamic, with frameworks constantly evolving and learning from each other. TensorFlow has embraced Keras as its high-level API and incorporated dynamic execution. PyTorch has improved its production deployment capabilities with TorchScript. JAX, while distinct, is influencing the design of other libraries.\n",
    "\n",
    "As Master of Finance students, your goal isn't necessarily to master all of them, but to understand their strengths and weaknesses so you can pick the *right tool for the job*. The ability to articulate *why* you chose a particular framework for a specific financial problem demonstrates a deeper understanding of both quantitative methods and practical implementation.\n",
    "\n",
    "I encourage you to experiment with these libraries. Start with Keras for ease, then explore PyTorch for flexibility, and consider JAX when you face computationally intensive numerical challenges or need precise gradient control.\n",
    "\n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a9ea14-9c7d-4dc6-8e5c-fca12560a291",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Fully Connected Network (FCN) - (also known as Multi-Layer Perceptron / MLP)\n",
    "\n",
    "A **Fully Connected Network** is a type of artificial neural network where neurons are organized into layers, and every neuron in one layer is connected to every neuron in the subsequent layer. There are no connections between neurons within the same layer or backward connections.\n",
    "\n",
    "**Key Characteristics:**\n",
    "*   **Architecture:** Consists of an input layer, one or more hidden layers (dense layers), and an output layer.\n",
    "*   **Data Processing:** Each input is treated independently. Data flows in one direction, from the input layer through the hidden layers to the output layer (feedforward).\n",
    "*   **Input Type:** Best suited for **structured, independent data** where the order of features does not matter, or features are processed simultaneously.\n",
    "*   **Memory:** Has **no inherent memory** of past inputs. Each prediction is based solely on the current input.\n",
    "*   **Parameter Sharing:** Weights are generally *not* shared across different parts of the input or across different instances of the network's operation, except within the same layer (e.g., all connections from `L-1` to `L` share the same weight matrix).\n",
    "*   **Output:** Typically outputs a fixed-size vector or a single value for classification or regression.\n",
    "\n",
    "**Pros:**\n",
    "*   **Simplicity:** Conceptually straightforward and easier to implement for basic tasks.\n",
    "*   **Versatility:** Can learn highly complex, non-linear relationships within the data.\n",
    "*   **Efficiency:** For fixed-size, independent inputs, FCNs can be very efficient to train and deploy.\n",
    "*   **Universal Approximator:** With enough hidden layers and neurons, they can approximate any continuous function.\n",
    "\n",
    "**Cons:**\n",
    "*   **Inefficient for Sequential Data:** Cannot naturally handle sequences or capture temporal dependencies. Treating sequence elements independently loses crucial information.\n",
    "*   **Fixed Input Size:** Typically requires a fixed-size input vector, which can be problematic for variable-length sequences.\n",
    "*   **Parameter Overload:** Can have a very large number of parameters if the input dimension is high, leading to overfitting with limited data.\n",
    "\n",
    "**Common Use Cases (in Finance):**\n",
    "*   **Credit Scoring:** Predicting loan default based on a fixed set of financial and demographic features.\n",
    "*   **Fraud Detection:** Identifying fraudulent transactions based on a snapshot of transaction details.\n",
    "*   **Equity Research (Static):** Classifying stocks based on fundamental metrics and technical indicators at a specific point in time.\n",
    "*   **Cross-Sectional Alpha Factors:** Predicting stock returns based on current company characteristics relative to others.\n",
    "\n",
    "---\n",
    "\n",
    "### Recurrent Neural Network (RNN)\n",
    "\n",
    "A **Recurrent Neural Network** is a type of neural network designed to recognize patterns in sequential data. Unlike FCNs, RNNs have internal memory, allowing them to use information from previous inputs in the sequence to influence the processing of the current input.\n",
    "\n",
    "**Key Characteristics:**\n",
    "*   **Architecture:** Features connections that loop back on themselves or connect to the next time step, creating a \"memory\" of past information.\n",
    "*   **Data Processing:** Processes data sequentially, one element at a time, while maintaining an internal \"hidden state\" that captures information from previous steps in the sequence.\n",
    "*   **Input Type:** Specifically designed for **sequential, time-dependent data** where the order of elements is crucial.\n",
    "*   **Memory:** Possesses an **internal state (memory)** that is updated at each step, allowing it to remember information over arbitrary lengths of sequences.\n",
    "*   **Parameter Sharing:** Critically, the same set of weights is used across all time steps (or positions) in the sequence, which is highly efficient for learning temporal patterns.\n",
    "*   **Output:** Can produce an output at each time step, or a single output after processing the entire sequence. Can handle variable-length input and output sequences.\n",
    "\n",
    "**Pros:**\n",
    "*   **Excellent for Sequential Data:** Naturally handles data where order and context are important.\n",
    "*   **Variable-Length Inputs/Outputs:** Can process and generate sequences of varying lengths.\n",
    "*   **Parameter Efficiency:** Shares parameters across time steps, reducing the total number of parameters compared to an FCN trying to model sequences of fixed length.\n",
    "\n",
    "**Cons:**\n",
    "*   **Vanishing/Exploding Gradients:** Traditional RNNs struggle with long-term dependencies due to vanishing or exploding gradients during training. (This is largely addressed by advanced RNN architectures like LSTMs and GRUs).\n",
    "*   **Slower Training:** Sequential processing can be slower than parallel processing in FCNs, especially for very long sequences.\n",
    "*   **Complexity:** Can be more complex to design and debug compared to simpler FCNs.\n",
    "\n",
    "**Common Use Cases (in Finance):**\n",
    "*   **Time Series Prediction:** Forecasting stock prices, exchange rates, or economic indicators.\n",
    "*   **Algorithmic Trading:** Developing strategies that adapt to market dynamics over time.\n",
    "*   **Sentiment Analysis of News Feeds:** Understanding the evolving sentiment from continuous streams of financial news.\n",
    "*   **Credit Default Prediction (Dynamic):** Predicting default considering a borrower's payment history over time.\n",
    "*   **Bond Rating Trends:** Analyzing changes in a company's financial health over quarters/years to predict rating adjustments.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison and Contrast\n",
    "\n",
    "| Feature            | Fully Connected Network (FCN)                               | Recurrent Neural Network (RNN)                                     |\n",
    "| :----------------- | :---------------------------------------------------------- | :----------------------------------------------------------------- |\n",
    "| **Primary Use**    | Independent, structured, tabular data                       | Sequential, time-dependent data (text, time series, audio)         |\n",
    "| **Architecture**   | Feedforward; no loops                                       | Contains loops; connects to previous states                        |\n",
    "| **Memory**         | No inherent memory of past inputs                           | Maintains an internal hidden state (memory) over time              |\n",
    "| **Input/Output**   | Fixed-size input/output                                     | Can handle variable-length sequences                               |\n",
    "| **Parameter Sharing** | Weights are unique per connection (within a layer)      | Same weights are shared across different time steps                |\n",
    "| **Order Sensitivity** | Order of features typically doesn't matter (if input is flattened) | Highly sensitive to the order of elements in a sequence            |\n",
    "| **Training Issues** | Standard backpropagation                                    | Vanishing/exploding gradients (in vanilla RNNs); addressed by LSTMs/GRUs |\n",
    "| **Complexity**     | Relatively simpler (for basic tasks)                        | More complex, especially for long sequences (LSTMs/GRUs needed)    |\n",
    "| **Financial Use**  | Credit scoring, fraud detection (snapshot), cross-sectional alpha | Stock price forecasting, sentiment analysis of news, dynamic risk modeling |\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "*   **FCNs** excel at processing **static, independent data points** where all relevant information is present in a single input vector. They are powerful for learning complex non-linear relationships but lack a sense of continuity or temporal order.\n",
    "*   **RNNs** are specifically designed for **sequential data**, enabling them to capture temporal dependencies and context over time. They \"remember\" past information, making them indispensable for tasks like time series analysis and natural language processing in finance.\n",
    "\n",
    "The choice between an FCN and an RNN (or one of its variants like LSTM/GRU) primarily depends on the **nature of your data** and whether **temporal dependencies are crucial** for the task at hand. Often, more complex financial applications might combine both, using RNNs to extract features from time series data, and then feeding those features into FCNs for final prediction or classification."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
